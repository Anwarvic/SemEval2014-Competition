{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# <font color='green'>Analyse des sentiments</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='red'>Lecture des données</font>\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importer les bibliothèques\n",
    "\n",
    "Reportez-vous aux pages Web pour les bibliothèques individuelles\n",
    "* [pandas] (http://pandas.pydata.org/), pour charger et gérer les données\n",
    "* [matplotlib] (http://matplotlib.org/), pour la visualisation\n",
    "* [numpy] (http://www.numpy.org/) pour peindre la représentation et la manipulation\n",
    "* [re] (https://docs.python.org/3/library/re.html) pour l'expression régulière\n",
    "* [nltk] (http://www.nltk.org/) pour le prétraitement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "from copy import copy\n",
    "import collections\n",
    "import scipy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import model_selection\n",
    "from sklearn.metrics import make_scorer, roc_auc_score\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lecture de l'ensemble de données\n",
    "Certaines des données \"uploaded_cleansed_B\" sont produites à partir de \"uploaded_cleansed_A\". La différence est:\n",
    "- \"uploaded_cleansed_A\" a trois colonnes que nous n'utiliserons pas.\n",
    "- \"uploaded_cleansed_A\" a des tweets répété."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9665, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>264183816548130816</td>\n",
       "      <td>15140428</td>\n",
       "      <td>positive</td>\n",
       "      <td>Gas by my house hit $3.39!!!! I'm going to Cha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>263405084770172928</td>\n",
       "      <td>591166521</td>\n",
       "      <td>negative</td>\n",
       "      <td>Not Available</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>262163168678248449</td>\n",
       "      <td>35266263</td>\n",
       "      <td>negative</td>\n",
       "      <td>Not Available</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>264249301910310912</td>\n",
       "      <td>18516728</td>\n",
       "      <td>negative</td>\n",
       "      <td>Iranian general says Israel's Iron Dome can't ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>262682041215234048</td>\n",
       "      <td>254373818</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Not Available</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    0          1         2  \\\n",
       "0  264183816548130816   15140428  positive   \n",
       "1  263405084770172928  591166521  negative   \n",
       "2  262163168678248449   35266263  negative   \n",
       "3  264249301910310912   18516728  negative   \n",
       "4  262682041215234048  254373818   neutral   \n",
       "\n",
       "                                                   3  \n",
       "0  Gas by my house hit $3.39!!!! I'm going to Cha...  \n",
       "1                                      Not Available  \n",
       "2                                      Not Available  \n",
       "3  Iranian general says Israel's Iron Dome can't ...  \n",
       "4                                      Not Available  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('/data/train/downloaded_cleansed_B.tsv', sep= '\\t', header=None)\n",
    "print (df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notez que certains tweets sont \"Non disponible\". Nous les rejetterons car cela n'aidera pas dans l'analyse des sentiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Supprimer tous les tweets \"NOT AVAILABLE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>264183816548130816</td>\n",
       "      <td>15140428</td>\n",
       "      <td>positive</td>\n",
       "      <td>Gas by my house hit $3.39!!!! I'm going to Cha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>264249301910310912</td>\n",
       "      <td>18516728</td>\n",
       "      <td>negative</td>\n",
       "      <td>Iranian general says Israel's Iron Dome can't ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>264105751826538497</td>\n",
       "      <td>147088367</td>\n",
       "      <td>positive</td>\n",
       "      <td>with J Davlar 11th. Main rivals are team Polan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>264094586689953794</td>\n",
       "      <td>332474633</td>\n",
       "      <td>negative</td>\n",
       "      <td>Talking about ACT's &amp;amp;&amp;amp; SAT's, deciding...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>254941790757601280</td>\n",
       "      <td>557103111</td>\n",
       "      <td>negative</td>\n",
       "      <td>They may have a SuperBowl in Dallas, but Dalla...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    0          1         2  \\\n",
       "0  264183816548130816   15140428  positive   \n",
       "3  264249301910310912   18516728  negative   \n",
       "6  264105751826538497  147088367  positive   \n",
       "7  264094586689953794  332474633  negative   \n",
       "9  254941790757601280  557103111  negative   \n",
       "\n",
       "                                                   3  \n",
       "0  Gas by my house hit $3.39!!!! I'm going to Cha...  \n",
       "3  Iranian general says Israel's Iron Dome can't ...  \n",
       "6  with J Davlar 11th. Main rivals are team Polan...  \n",
       "7  Talking about ACT's &amp;&amp; SAT's, deciding...  \n",
       "9  They may have a SuperBowl in Dallas, but Dalla...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[df[3] != \"Not Available\"]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7205, 4)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dessiner les données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAncAAAHsCAYAAACwg4t/AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nDTHcSZIkNeT/B2aH2bcVHf4qAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f379d1244a8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# calculate the total number of occurrences of each class\n",
    "y = [len(df[df[2] == i]) for i in ['positive', 'negative', 'neutral']]\n",
    "# X axis\n",
    "objects = ['positive', 'negative', 'neutral']\n",
    "x_pos = range(len(objects))\n",
    "\n",
    "# Draw Diagram\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.bar(x_pos, y, alpha=0.5)\n",
    "plt.xticks(x_pos, objects)\n",
    "plt.ylabel('Occurences').set_size(20)\n",
    "plt.xlabel('Classes').set_size(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interprétation\n",
    "À partir du graphique ci-dessus, nous pouvons clairement noter que la classe «négative» a le moins d'échantillons dans les données par rapport à «positif» et «neutre». Par conséquent, les données semblent déséquilibrées."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7205,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_tweets = list(df[3])\n",
    "labels = df[2]\n",
    "mapper = {'positive': 1, 'negative': -1, 'neutral': 0}\n",
    "labels = labels.map(mapper)\n",
    "labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment140 Score\n",
    "\n",
    "Avant de faire un pré-traitement sur les tweets, nous allons d'abord utiliser le score du Sentiment140 corpus. Ce corpus a le score des mots les plus courants (formels, informels) utilisés dans twitter. Le score est un nombre compris entre [-4.999: 4.999].\n",
    "\n",
    "Le score sera divisé en trois parties:\n",
    "- unigram score  --> 'unigram140_score'\n",
    "- bigram score   --> 'bigram140_score'\n",
    "- pair score     --> 'pair140_score'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7205"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def Sentiment140_dictionary(filename):\n",
    "    sentiment140 = {}\n",
    "    with open(filename) as fin:\n",
    "        line = fin.readline()[:-1]\n",
    "        while line:\n",
    "            line = line.split('\\t')\n",
    "            sentiment140[line[0]] = float(line[1])\n",
    "            line = fin.readline()[:-1]\n",
    "    return sentiment140\n",
    "\n",
    "\n",
    "def unigram140Polarity(tweet, d):\n",
    "    score=0.0\n",
    "    reps = 0\n",
    "    for w in tweet.split(' '):\n",
    "        if w in d.keys():\n",
    "            reps += 1\n",
    "            score+=d[w]\n",
    "    return score, reps\n",
    "\n",
    "unigram140_d = Sentiment140_dictionary('/data/resources/Sentiment140/unigrams-pmilexicon.txt')\n",
    "hashtag_words = [word for word in unigram140_d.keys() if word[0]=='#']\n",
    "unigram140Score, unigram140Reps = [], []\n",
    "for tweet in raw_tweets:\n",
    "    score, reps = unigram140Polarity(tweet.lower(), unigram140_d)\n",
    "    unigram140Score.append(score)\n",
    "    unigram140Reps.append(reps)\n",
    "\n",
    "len(unigram140Score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7205"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def find_bigrams(input_list):\n",
    "    bigram_list = []\n",
    "    for i in range(len(input_list)-1):\n",
    "        bigram_list.append(input_list[i] + \" \" + input_list[i+1])\n",
    "    return bigram_list\n",
    "\n",
    "\n",
    "def bigram140Polarity(tweet, d):\n",
    "    score=0.0\n",
    "    reps = 0\n",
    "    tweet = find_bigrams(tweet.split(' '))\n",
    "    for w in tweet:\n",
    "        if w in d.keys():\n",
    "            reps += 1\n",
    "            score+=d[w]\n",
    "    return score, reps\n",
    "\n",
    "\n",
    "bigram140_d = Sentiment140_dictionary('/data/resources/Sentiment140/bigrams-pmilexicon.txt')\n",
    "bigram140Score, bigram140Reps = [], []\n",
    "for tweet in raw_tweets:\n",
    "    score, reps = bigram140Polarity(tweet.lower(), bigram140_d)\n",
    "    bigram140Score.append(score)\n",
    "    bigram140Reps.append(reps)\n",
    "\n",
    "len(bigram140Score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SemEval2015 English lexicon \n",
    "\n",
    "Ce sont les toutes premières et dernières entrées de 'SemEval2015-English-Twitter-Lexicon.txt':\n",
    "- 0.984\tloves\n",
    "- 0.984\t#inspirational\n",
    "- 0.969\tamazing\n",
    "- 0.969\t#peaceful\n",
    "- 0.953\t#greatness\n",
    "- ...\n",
    "- -0.969\tabuse\n",
    "- -0.969\t#failure\n",
    "- -0.982\tkill\n",
    "- -0.984\tbitches\n",
    "- -0.984\t#disappointment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of EnglishLexicon entries 1516\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7205"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def loadSemEval(filename):\n",
    "    f = open(filename,'r')\n",
    "    lexicon = {}\n",
    "    line = f.readline()\n",
    "    while line:\n",
    "        l = line[:-1].split('\\t')\n",
    "        lexicon[l[1]] = float(l[0])\n",
    "        line = f.readline()\n",
    "    return lexicon\n",
    "\n",
    "\n",
    "def SemEvalLexiconPolarity(tweet, EnglishLexicon):\n",
    "    score=0.0\n",
    "    reps = 0\n",
    "    for w in tweet.split(' '):\n",
    "        if w in EnglishLexicon.keys():\n",
    "            reps += 1\n",
    "            score += EnglishLexicon[w]\n",
    "    return score, reps\n",
    "\n",
    "EnglishLexicon = loadSemEval('/data/resources/SemEval2015-English-Twitter-Lexicon.txt')\n",
    "hashtag_words.extend([word for word in EnglishLexicon.keys() if word[0]=='#'])\n",
    "hashtag_words = set(hashtag_words)\n",
    "SemEvalScore, SemEvalReps = [], []\n",
    "for tweet in raw_tweets:\n",
    "    score, reps = SemEvalLexiconPolarity(tweet.lower(), EnglishLexicon)\n",
    "    SemEvalScore.append(score)\n",
    "    SemEvalReps.append(reps)\n",
    "\n",
    "print (\"Number of EnglishLexicon entries %d\" % len(EnglishLexicon.keys()))\n",
    "len(SemEvalScore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# <font color='red'>Pre-entraîner les tweets</font>\n",
    "https://nlp.stanford.edu/IR-book/html/htmledition/determining-the-vocabulary-of-terms-1.html\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Supprimer les slangs des tweets\n",
    "Par (slangs) argot, nous entendons des mots comme:\n",
    "- i've --> I have\n",
    "- 12be --> want to be\n",
    "- *4u  --> kiss for you"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loadSlangs(filename):\n",
    "    \"\"\"\n",
    "    This function reads the file that contains the slangs, and put them in a dictionary such that\n",
    "    the key is the \"slang\" and the value is the acronym.\n",
    "    slangs['i've'] = 'i have'\n",
    "    slang['12be'] = 'want to be'\n",
    "    ...\n",
    "    CAUTION: the keys and values are lower-case letters\n",
    "    \"\"\"\n",
    "    slangs={}\n",
    "    fi=open(filename,'r')\n",
    "    line=fi.readline()\n",
    "    while line:\n",
    "        l=line.split(r',%,')\n",
    "        if len(l) == 2:\n",
    "            slangs[l[0].lower()]=l[1][:-1].lower()  #HERE\n",
    "        line=fi.readline()\n",
    "    fi.close()\n",
    "    return slangs\n",
    "\n",
    "\n",
    "def replaceSlangs(tweet,slangs):\n",
    "    \"\"\"\n",
    "    This function is used to replace the slang in the original tweets and replace them with the acronym.\n",
    "    And it's also returns the the tweet in lower-case letters\n",
    "    \"\"\"\n",
    "    result=''\n",
    "    tweet = tweet.lower()\n",
    "    words=tweet.split()\n",
    "    for w in words:\n",
    "        if w in slangs.keys():\n",
    "            result=result+slangs[w]+\" \"\n",
    "        else:\n",
    "            result=result+w+\" \"\n",
    "    return result.strip()\n",
    "\n",
    "slangs = loadSlangs('/data/resources/internetSlangs.txt')\n",
    "raw_tweets = [replaceSlangs(tweet, slangs) for tweet in raw_tweets]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remplacer les mots apostrophe\n",
    "\n",
    "Par cela, nous entendons changer des mots comme 'can't', 'cant' en 'can not'. Ces mots sont dans un fichier appelé 'apostrophe_words.txt' qui existait dans le répertoire 'resources'.\n",
    "\n",
    "Nous devons faire cela pour gérer le problème de la négation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_apostrophe_words(filename):\n",
    "    \"\"\"\n",
    "    This function reads the file that contains all words that have apostrophe, and put them in a dictionary \n",
    "    such that the key is the \"word containing apostrophe\" and the value is the \"the word without apostrophe\".\n",
    "    slangs['i've'] = 'i have'\n",
    "    slang['I'm] = 'I am'\n",
    "    ...\n",
    "    CAUTION: the keys and values are lower-case letters\n",
    "    \"\"\"\n",
    "    apo={}\n",
    "    fi=open(filename,'r')\n",
    "    line=fi.readline()\n",
    "    while line:\n",
    "        l=line.split(r',%,')\n",
    "        if len(l) == 2:\n",
    "            apo[l[0].lower()]=l[1][:-1].lower()\n",
    "        line=fi.readline()\n",
    "    fi.close()\n",
    "    return apo\n",
    "\n",
    "\n",
    "def replace_apostrophe(tweet,apos):\n",
    "    result=''\n",
    "    words=tweet.split()\n",
    "    for w in words:\n",
    "        if w in apos.keys():\n",
    "            result=result+apos[w]+\" \"\n",
    "        else:\n",
    "            result=result+w+\" \"\n",
    "    return result.strip()\n",
    "\n",
    "apos = load_apostrophe_words('/data/resources/apostrophe_words.txt')\n",
    "raw_tweets = [replace_apostrophe(tweet, apos) for tweet in raw_tweets]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Appliquer des techniques de prétraitement standard\n",
    "\n",
    "* https://nlp.stanford.edu/IR-book/html/htmledition/tokenization-1.html\n",
    "* https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utiliser NRC emoticon lexicon\n",
    "\n",
    "Nous remplacerons l'émoticône par sa signification associée."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TT = TweetTokenizer()\n",
    "\n",
    "def emoticondictionary(filename):\n",
    "    \"\"\"\n",
    "    Reads the emoticon file and represents it as dictionary where the emoticon is the key, \n",
    "    and its indication as a value\n",
    "    \"\"\"\n",
    "    emo_scores = {'Positive': 'positive', 'Extremely-Positive': 'positive', \n",
    "                  'Negative': 'negative','Extremely-Negative': 'negative',\n",
    "                  'Neutral': 'neutral'}\n",
    "    emo_score_list = {}\n",
    "    fi = open(filename,\"r\")\n",
    "    l = fi.readline()\n",
    "    while l:\n",
    "        #replace the \"Non-break space\" with the ordinary space \" \"\n",
    "        l = l.replace(\"\\xa0\",\" \") #HERE\n",
    "        li = l.split(\" \")\n",
    "        l2 = li[:-1] #removes the polarity of the emoticon ('negative', 'positive')\n",
    "        l2.append(li[len(li) - 1].split(\"\\t\")[0]) #gets the last emoticon attached to the polarity by '\\t'\n",
    "        sentiment=li[len(li) - 1].split(\"\\t\")[1][:-1] #gets only the polarity, and removes '\\n'\n",
    "        score=emo_scores[sentiment]\n",
    "        l2.append(score)\n",
    "        for i in range(0,len(l2)-1):\n",
    "            emo_score_list[l2[i]]=l2[len(l2)-1]\n",
    "        l=fi.readline()\n",
    "    return emo_score_list\n",
    "\n",
    "dict = emoticondictionary('/data/resources/emoticon.txt')\n",
    "\n",
    "\n",
    "# substititue emoticon with its associated sentiment\n",
    "def subsEmoticon(tweet,d):\n",
    "    l = TT.tokenize(tweet)\n",
    "    tweet = [d[i] if i in d.keys() else i for i in l]\n",
    "    return tweet\n",
    "\n",
    "\n",
    "raw_tweets = [subsEmoticon(tweet, dict) for tweet in raw_tweets]\n",
    "# print(\":D X3 :|\")\n",
    "# subsEmoticon(\":D X3 :|\", dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gérer la négation\n",
    "\n",
    "Suite au travail de Pang et al (2002), nous définissons un contexte nié comme un segment d'un tweet qui commence par un mot de négation (par exemple, no, never) et se termine par l'un des signes de ponctuation: ',', ' . ',': ','; ','! ','? '.\n",
    "\n",
    "Après avoir manipulé la négation, un tweet comme  'I don't like vegan food' serait 'I do not like_not vegan_not food_not.'.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "negation_words = set(['barely', 'hardly', 'lack', 'never', 'neither', 'no', 'nobody', \\\n",
    "                      'not', 'nothing', 'none', 'nowhere', 'shortage', 'scarcely', 'few', \\\n",
    "                      'low', 'merely', 'nope', 'seldom', 'rarely', 'without', 'zero'])\n",
    "punctuations = [',', '.', ':', ';', '!', '?']\n",
    "#other punctuations\n",
    "others = ['(', ')', '[', ']', '{', '}', '*', '+', '-', '%', '^', '&', '<', '>']\n",
    "\n",
    "def handle_negation(tweet):\n",
    "    output = []\n",
    "    negate = False\n",
    "    for word in tweet:\n",
    "        if word in punctuations and negate:\n",
    "            negate = False\n",
    "        if negate and not word in negation_words and word not in others:\n",
    "            output.append(word+\"_not\")\n",
    "        else:\n",
    "            output.append(word)\n",
    "        if word in negation_words and not negate:\n",
    "            negate = True\n",
    "        elif word in negation_words and negate:\n",
    "            negate = False\n",
    "    return output\n",
    "\n",
    "raw_tweets = [handle_negation(tweet) for tweet in raw_tweets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['iranian',\n",
       " 'general',\n",
       " 'says',\n",
       " \"israel's\",\n",
       " 'iron',\n",
       " 'dome',\n",
       " 'can',\n",
       " 'not',\n",
       " 'deal_not',\n",
       " 'with_not',\n",
       " 'their_not',\n",
       " 'missiles_not',\n",
       " '(',\n",
       " 'keep_not',\n",
       " 'talking_not',\n",
       " 'like_not',\n",
       " 'that_not',\n",
       " 'and_not',\n",
       " 'we_not',\n",
       " 'may_not',\n",
       " 'end_not',\n",
       " 'up_not',\n",
       " 'finding_not',\n",
       " 'out_not',\n",
       " ')']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_tweets[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### lemmatizer les mots \n",
    "La lemmatisation ressemble à la conversion du mot 'networks' en 'network'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mmer = WordNetLemmatizer()\n",
    "# Lemmatize the tweets\n",
    "def lemma(tweet):\n",
    "    return ' '.join([mmer.lemmatize(word) for word in tweet])\n",
    "\n",
    "lemmatized_tweets = [lemma(tweet) for tweet in raw_tweets]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### supprimer les hashtags non importants\n",
    "Nous avons extrait les hashtags importants de deux corpus différents (Sentiment140 and SemEval2015 English lexicon) qui sont les seuls mots qui ont des scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_hashtags(tweet):\n",
    "    output = \"\"\n",
    "    for word in tweet.split():\n",
    "        if word[0] == '#' and word not in hashtag_words:\n",
    "            continue\n",
    "        else:\n",
    "            output += word + ' '\n",
    "    return output.strip()\n",
    "\n",
    "clean_tweets = [remove_hashtags(tweet) for tweet in lemmatized_tweets]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On traite ici différents problèmes:\n",
    "- supprime les caractères de ponctuation comme,. :; etc.\n",
    "- supprime les numéros du tweet.\n",
    "- supprime les espaces supplémentaires dans le tweet.\n",
    "- supprime l'occurrence de deux ou plusieurs caractères dans un mot, par exemple. loooong -> loong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def preprocess(tweet):\n",
    "    # delete symbols and URIs and tags\n",
    "    tweet =  ' '.join(re.sub(\"(@[A-Za-z0-9_]+)|([^0-9A-Za-z_ \\t])|(\\w+:\\/\\/\\S+)\", '', tweet).split()) #here _\n",
    "    # Convert '@username' to 'at_user'\n",
    "    # tweet = re.sub('@[^\\s]+','at_user',tweet)\n",
    "    # remove hashtags\n",
    "    # tweet = re.sub(r'#\\s', '', tweet)\n",
    "    # remove numbers\n",
    "    tweet = re.sub('[0-9]', '', tweet)\n",
    "    # remove additional spaces\n",
    "    tweet = re.sub('[\\s]+', ' ', tweet)\n",
    "    # replace the occurrence of 2 or more characters in a word, eg. loooong -> loong\n",
    "    tweet = re.sub(r'(.)\\1{2,}', r'\\1\\1', tweet)\n",
    "    return tweet\n",
    "\n",
    "\n",
    "preprocessed_tweets = [preprocess(tweet) for tweet in clean_tweets]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Supprimer stopwords\n",
    "https://nlp.stanford.edu/IR-book/html/htmledition/dropping-common-terms-stop-words-1.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "359\n",
      "\n",
      "Compare tweets before / after\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>3</th>\n",
       "      <th>final_tweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Gas by my house hit $3.39!!!! I'm going to Cha...</td>\n",
       "      <td>gas house hit going chapel hill sat positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Iranian general says Israel's Iron Dome can't ...</td>\n",
       "      <td>iranian general say israels iron dome deal_not...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>with J Davlar 11th. Main rivals are team Polan...</td>\n",
       "      <td>davlar th main rival team poland hopefully mak...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Talking about ACT's &amp;amp;&amp;amp; SAT's, deciding...</td>\n",
       "      <td>talking acts sats deciding want go college app...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>They may have a SuperBowl in Dallas, but Dalla...</td>\n",
       "      <td>may superbowl dallas dallas winning_not superb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Im bringing the monster load of candy tomorrow...</td>\n",
       "      <td>instant message bringing monster load candy to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Apple software, retail chiefs out in overhaul:...</td>\n",
       "      <td>apple software retail chief overhaul san franc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>@oluoch @victor_otti @kunjand I just watched i...</td>\n",
       "      <td>watched sridevis comeback remember sun morning...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>#Livewire Nadal confirmed for Mexican Open in ...</td>\n",
       "      <td>nadal confirmed mexican open february rafael n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>@MsSheLahY I didnt want to just pop up... but ...</td>\n",
       "      <td>didnt want pop yep chapel hill next wednesday ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    3  \\\n",
       "0   Gas by my house hit $3.39!!!! I'm going to Cha...   \n",
       "3   Iranian general says Israel's Iron Dome can't ...   \n",
       "6   with J Davlar 11th. Main rivals are team Polan...   \n",
       "7   Talking about ACT's &amp;&amp; SAT's, deciding...   \n",
       "9   They may have a SuperBowl in Dallas, but Dalla...   \n",
       "10  Im bringing the monster load of candy tomorrow...   \n",
       "11  Apple software, retail chiefs out in overhaul:...   \n",
       "12  @oluoch @victor_otti @kunjand I just watched i...   \n",
       "14  #Livewire Nadal confirmed for Mexican Open in ...   \n",
       "15  @MsSheLahY I didnt want to just pop up... but ...   \n",
       "\n",
       "                                         final_tweets  \n",
       "0        gas house hit going chapel hill sat positive  \n",
       "3   iranian general say israels iron dome deal_not...  \n",
       "6   davlar th main rival team poland hopefully mak...  \n",
       "7   talking acts sats deciding want go college app...  \n",
       "9   may superbowl dallas dallas winning_not superb...  \n",
       "10  instant message bringing monster load candy to...  \n",
       "11  apple software retail chief overhaul san franc...  \n",
       "12  watched sridevis comeback remember sun morning...  \n",
       "14  nadal confirmed mexican open february rafael n...  \n",
       "15  didnt want pop yep chapel hill next wednesday ...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend([word+'_not' for word in stop_words]) #negation\n",
    "stop_words = set(stop_words)\n",
    "stop_words.update('j', 'im')\n",
    "print (len(stop_words))\n",
    "\n",
    "# remove stopwords\n",
    "def rem_stop(tweet):\n",
    "    words = tweet.split()\n",
    "    tweet = ' '.join([word for word in words if word not in stop_words])\n",
    "    return tweet\n",
    "\n",
    "final_tweets = [rem_stop(tweet) for tweet in preprocessed_tweets]\n",
    "\n",
    "print(\"\\nCompare tweets before / after\")\n",
    "df['final_tweets'] = final_tweets\n",
    "df[[3, 'final_tweets']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#DELETE UNCESSARY VARIABLES\n",
    "del raw_tweets, lemmatized_tweets, preprocessed_tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# <font color='red'>Création de Features</font>\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utilisation de MPQA Lexicon\n",
    "\n",
    "Ce sont les toutes premières et dernières entrées du fichier 'mpqa.txt'\n",
    "- abandoned priorpolarity=negative\n",
    "- abandonment priorpolarity=negative\n",
    "- abandon priorpolarity=negative\n",
    "- abase priorpolarity=negative\n",
    "- abasement priorpolarity=negative\n",
    "- ...\n",
    "- zealot priorpolarity=negative\n",
    "- zealous priorpolarity=negative\n",
    "- zealously priorpolarity=negative\n",
    "- zenith priorpolarity=positive\n",
    "- zest priorpolarity=positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of MPQA words: 13772\n",
      "['watched', 'sridevis', 'positive', 'remember', 'sun', 'morning', 'nta', 'positive']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['neutral', 'positive', 'negative']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def MPQAdictionary(filename):\n",
    "    \"\"\"\n",
    "    reads mpqa file which contains the polarity of some of the english words. e.g. 'love': 'positive'\n",
    "    \"\"\"\n",
    "    MPQA_scores = {'priorpolarity=positive\\n': 'positive','priorpolarity=negative\\n': 'negative',\n",
    "                  'priorpolarity=neutral\\n': 'neutral', 'priorpolarity=both\\n': 'neutral'}\n",
    "    MPQA_score_list = {}\n",
    "    fi = open(filename,\"r\")\n",
    "    line = fi.readline()\n",
    "    while line: \n",
    "        li = line.split(\" \")\n",
    "        l2 = li[:-1] # the word as a list\n",
    "        sentiment=li[1] #the word's polarity\n",
    "        score=MPQA_scores[sentiment]\n",
    "        l2.append(score)\n",
    "        for i in range(0,len(l2)-1):\n",
    "            MPQA_score_list[l2[i]]=l2[-1]\n",
    "            # negation\n",
    "            if l2[-1] == 'positive':\n",
    "                MPQA_score_list[l2[i]+'_not']='positive' \n",
    "            else:\n",
    "                MPQA_score_list[l2[i]+'_not']='negative' \n",
    "        line=fi.readline()\n",
    "    return MPQA_score_list\n",
    "\n",
    "\n",
    "def subsMPQA(tweet,d):\n",
    "    l = TT.tokenize(tweet)\n",
    "    #print(l)\n",
    "    tweet = [d[i] if i in d.keys() else i for i in l]\n",
    "    return tweet\n",
    "\n",
    "dictionary = MPQAdictionary('/data/resources/mpqa/mpqa.txt')\n",
    "print (\"Number of MPQA words: %d\" % len(dictionary.keys()))\n",
    "raw_tweets_MPQA = [subsMPQA(tweet,dictionary) for tweet in final_tweets]\n",
    "\n",
    "print (subsMPQA(final_tweets[7], dictionary))\n",
    "# watched sridevis comeback remember sun morning nta positive\n",
    "subsMPQA(\"surprise happy abandoned\", dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utilisation de WordSat Corpus\n",
    "Ce sont les toutes premières et dernières entrées du fichier 'WordSat_pos.txt':\n",
    "- ABIDE\n",
    "- ABIDED\n",
    "- ABIDES\n",
    "- ABIDING\n",
    "- ABILITY\n",
    "- ...\n",
    "- ZENITHS\n",
    "- ZEST\n",
    "- ZESTFULLY\n",
    "- ZESTFULNESS\n",
    "- ZESTS\n",
    "\n",
    "Ce sont les toutes premières et dernières entrées du fichier 'WordSat_neg.txt':\n",
    "- ABANDON\n",
    "- ABASE\n",
    "- ABASED\n",
    "- ABASES\n",
    "- ABATE\n",
    "- ...\n",
    "- YUKKY\n",
    "- ZEALOT\n",
    "- ZEALOTS\n",
    "- ZEALOUS\n",
    "- ZEALOUSLY\n",
    "- ZILCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of positive words 13841\n",
      "Number of negative words 13841\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['positive', 'firas', 'positive']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from copy import copy\n",
    "\n",
    "ENGLISH_WSD_LOCATION = os.path.join('/data/resources/WSD')\n",
    "POS_WORDS_FILE = os.path.join(ENGLISH_WSD_LOCATION, 'WordSat_pos.txt')\n",
    "NEG_WORDS_FILE = os.path.join(ENGLISH_WSD_LOCATION, 'WordSat_neg.txt')\n",
    "\n",
    "pos_words = []\n",
    "neg_words = []\n",
    "\n",
    "for pos_word in open(POS_WORDS_FILE, 'r').readlines():\n",
    "    pos_word = pos_word.split(' ')[0]\n",
    "    if \"_\" not in pos_word:\n",
    "        pos_words.append(pos_word.lower().strip('*'))\n",
    "\n",
    "for neg_word in open(NEG_WORDS_FILE, 'r').readlines():\n",
    "    neg_word = neg_word.split(' ')[0]\n",
    "    if \"_\" not in neg_word:\n",
    "        neg_words.append(neg_word.lower().strip('*'))\n",
    "\n",
    "#negation\n",
    "expanded_pos = copy(pos_words)\n",
    "expanded_pos.extend([word+\"_not\" for word in neg_words])\n",
    "expanded_neg = copy(neg_words)\n",
    "expanded_neg.extend([word+\"_not\" for word in pos_words])\n",
    "\n",
    "#change its type into a set\n",
    "expanded_pos = set(expanded_pos)\n",
    "expanded_neg = set(expanded_neg)\n",
    "\n",
    "#delete unnecessary objects\n",
    "del pos_words, neg_words\n",
    "del ENGLISH_WSD_LOCATION, POS_WORDS_FILE, NEG_WORDS_FILE\n",
    "print (\"Number of positive words %d\" % len(expanded_pos))\n",
    "print (\"Number of negative words %d\" % len(expanded_neg))\n",
    "\n",
    "def subs_pos(tweet, pos_words):\n",
    "    return ['positive' if i in pos_words else i for i in tweet]\n",
    "\n",
    "def subs_neg(tweet, neg_words):\n",
    "    return ['negative' if i in neg_words else i for i in tweet]\n",
    "\n",
    "raw_tweets_wsd = [subs_pos(tweet, expanded_pos) for tweet in raw_tweets_MPQA]\n",
    "raw_tweets_wsd = [subs_neg(tweet, expanded_neg) for tweet in raw_tweets_wsd]\n",
    "\n",
    "subs_pos(\"enjoy firas extraordinarily\".split(' '), expanded_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utilisation de Bing Liu Lexicon\n",
    "Ce sont les toutes premières et dernières entrées du fichier 'positive-words.txt':\n",
    "- a+\n",
    "- abound\n",
    "- abounds\n",
    "- abundance\n",
    "- abundant\n",
    "- ...\n",
    "- youthful\n",
    "- zeal\n",
    "- zenith\n",
    "- zest\n",
    "- zippy\n",
    "\n",
    "Ce sont les toutes premières et dernières entrées du fichier 'negative-words.txt':\n",
    "- 2-faced\n",
    "- 2-faces\n",
    "- abnormal\n",
    "- abolish\n",
    "- abominable\n",
    "- ...\n",
    "- zaps\n",
    "- zealot\n",
    "- zealous\n",
    "- zealously\n",
    "- zombie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of positive words 14676\n",
      "Number of negative words 14676\n"
     ]
    }
   ],
   "source": [
    "ENGLISH_OPINION_LEXICON_LOCATION = os.path.join('/data/resources/opinion-lexicon-English')\n",
    "POS_WORDS_FILE = os.path.join(ENGLISH_OPINION_LEXICON_LOCATION, 'positive-words.txt')\n",
    "NEG_WORDS_FILE = os.path.join(ENGLISH_OPINION_LEXICON_LOCATION, 'negative-words.txt')\n",
    "\n",
    "for pos_word in open(POS_WORDS_FILE, 'r').readlines()[35:]:\n",
    "    word = pos_word.rstrip()\n",
    "    expanded_pos.add(word)\n",
    "    expanded_neg.add(word+\"_not\")  #negation\n",
    "\n",
    "for neg_word in open(NEG_WORDS_FILE, 'r').readlines()[35:]:\n",
    "    word = pos_word.rstrip()\n",
    "    expanded_neg.add(word)\n",
    "    expanded_pos.add(word+\"_not\")  #negation\n",
    "\n",
    "\n",
    "#delete unnecessary objects\n",
    "del ENGLISH_OPINION_LEXICON_LOCATION, POS_WORDS_FILE, NEG_WORDS_FILE\n",
    "\n",
    "print (\"Number of positive words %d\" % len(expanded_pos))\n",
    "print (\"Number of negative words %d\" % len(expanded_neg))\n",
    "\n",
    "\n",
    "raw_tweets_bing = [subs_pos(tweet, expanded_pos) for tweet in raw_tweets_wsd]\n",
    "raw_tweets_bing = [subs_neg(tweet, expanded_neg) for tweet in raw_tweets_bing]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Afinn](https://pypi.python.org/pypi/afinn)\n",
    "\n",
    "Ce sont les toutes premières et dernières entrées de 'afinn.txt':\n",
    "- abandon\t-2\n",
    "- abandoned\t-2\n",
    "- abandons\t-2\n",
    "- abducted\t-2\n",
    "- abduction\t-2\n",
    "- ...\n",
    "- yucky\t-2\n",
    "- yummy\t3\n",
    "- zealot\t-2\n",
    "- zealots\t-2\n",
    "- zealous\t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Afinn entries 4922\n"
     ]
    }
   ],
   "source": [
    "def loadAfinn(filename):\n",
    "    f=open(filename,'r')\n",
    "    afinn={}\n",
    "    line=f.readline()\n",
    "    while line:\n",
    "        if \" \" in line:   #exclude entries like 'cool stuff    3'\n",
    "            pass\n",
    "        else:\n",
    "            l=line[:-1].split('\\t') #line[:-1] removes the '\\r\\n' character\n",
    "            afinn[l[0]]=float(l[1])    # normalization -------> \n",
    "            afinn[l[0]+\"_not\"] = -float(l[1])  # negation\n",
    "        line=f.readline()\n",
    "\n",
    "    return afinn\n",
    "\n",
    "afinn = loadAfinn('/data/resources/afinn.txt')\n",
    "# print (afinn)\n",
    "print (\"Number of Afinn entries %d\" % len(afinn.keys()))\n",
    "\n",
    "def afinnPolarity(tweet, afinn):\n",
    "    score=0.0\n",
    "    reps = 0\n",
    "    for w in tweet:\n",
    "        if w in afinn.keys():\n",
    "            reps += 1\n",
    "            score+=afinn[w]\n",
    "    return score, reps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SentiWordNet\n",
    "\n",
    "Voici les cinq premières lignes du fichier csv 'sentiWordnetBig.csv':\n",
    "\n",
    "|POS|ID|PosSCore|NegScore|SynsetTerms|\n",
    "|-|-------|-----|-----|-------------------|\n",
    "|a|1740|0.125|0|able#1|\n",
    "|a|2098|0|0.75|unable#1|\n",
    "|a|2312|0|0|dorsal#2 abaxial#1|\n",
    "|a|2527|0|0|ventral#2 adaxial#1|\n",
    "|a|2730|0|0|acroscopic#1|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening the SentiWordnet file ...\n",
      "Loading...\n",
      "Number of sentiWordnet entries 294612\n"
     ]
    }
   ],
   "source": [
    "def loadSentiWordnet(filename): \n",
    "    output={}\n",
    "    print (\"Opening the SentiWordnet file ...\")\n",
    "    fi=open(filename,\"r\")\n",
    "    line=fi.readline() # ignore the header\n",
    "    line=fi.readline()\n",
    "    print (\"Loading...\")\n",
    "\n",
    "    while line:\n",
    "        l=line.split('\\t')\n",
    "        try:\n",
    "            sentence=l[4]\n",
    "            new = [word for word in sentence.split() if (word[-2] == \"#\" and word[-1].isdigit())]\n",
    "            pos=abs(float(l[2]))\n",
    "            neg=abs(float(l[3]))\n",
    "            neu=float(pos-neg)\n",
    "        except:\n",
    "            line=fi.readline()\n",
    "            continue\n",
    "\n",
    "        for w in new:\n",
    "            output[(w[:-2])]=neu\n",
    "            output[(w[:-2])+'_not'] = -neu   #negation\n",
    "        line=fi.readline()\n",
    "        \n",
    "    fi.close()\n",
    "    return output\n",
    "\n",
    "\n",
    "sentiWordnet = loadSentiWordnet('/data/resources/sentiWordnetBig.csv')\n",
    "print (\"Number of sentiWordnet entries %d\" % len(sentiWordnet.keys()))\n",
    "\n",
    "\n",
    "\n",
    "def WordnetPolarity(tweet, sentiWordnet):\n",
    "    score=0.0\n",
    "    reps = 0\n",
    "    for w in tweet:\n",
    "        if w in sentiWordnet.keys():\n",
    "            reps += 1\n",
    "            score+=sentiWordnet[w]\n",
    "    return score, reps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SenticNet API\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from senticnet.senticnet import Senticnet\n",
    "\n",
    "def SenticnetPolarity(tweet):\n",
    "    score=0.0\n",
    "    reps = 0\n",
    "    for w in tweet:\n",
    "        try:\n",
    "            score += float(Senticnet().polarity_intense(w))\n",
    "            reps += 1\n",
    "        except KeyError:\n",
    "            pass\n",
    "    return score, reps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calcul de polarité"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7205, 7)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bing_mpqa_score</th>\n",
       "      <th>afinn_score</th>\n",
       "      <th>wordnet_score</th>\n",
       "      <th>sem_eval_score</th>\n",
       "      <th>Senticnet_score</th>\n",
       "      <th>final_score</th>\n",
       "      <th>final_tweets</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0058</td>\n",
       "      <td>0.005651</td>\n",
       "      <td>-0.003480</td>\n",
       "      <td>0.008469</td>\n",
       "      <td>0.003552</td>\n",
       "      <td>0.013699</td>\n",
       "      <td>gas house hit going chapel hill sat positive</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0116</td>\n",
       "      <td>0.011303</td>\n",
       "      <td>0.003480</td>\n",
       "      <td>-0.003356</td>\n",
       "      <td>0.015833</td>\n",
       "      <td>0.027176</td>\n",
       "      <td>iranian general say israels iron dome deal_not...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0058</td>\n",
       "      <td>0.005651</td>\n",
       "      <td>-0.017398</td>\n",
       "      <td>-0.002014</td>\n",
       "      <td>0.003386</td>\n",
       "      <td>0.014754</td>\n",
       "      <td>davlar th main rival team poland hopefully mak...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.010439</td>\n",
       "      <td>0.004899</td>\n",
       "      <td>-0.016451</td>\n",
       "      <td>-0.064887</td>\n",
       "      <td>talking acts sats deciding want go college app...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.0058</td>\n",
       "      <td>-0.005651</td>\n",
       "      <td>-0.008699</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.011883</td>\n",
       "      <td>-0.041962</td>\n",
       "      <td>may superbowl dallas dallas winning_not superb...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-0.0058</td>\n",
       "      <td>-0.005651</td>\n",
       "      <td>-0.017398</td>\n",
       "      <td>0.016953</td>\n",
       "      <td>0.004136</td>\n",
       "      <td>-0.021504</td>\n",
       "      <td>instant message bringing monster load candy to...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.005219</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.008929</td>\n",
       "      <td>-0.014154</td>\n",
       "      <td>apple software retail chief overhaul san franc...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.0116</td>\n",
       "      <td>0.011303</td>\n",
       "      <td>0.005219</td>\n",
       "      <td>0.006484</td>\n",
       "      <td>0.014127</td>\n",
       "      <td>0.059681</td>\n",
       "      <td>watched sridevis comeback remember sun morning...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.0116</td>\n",
       "      <td>0.011303</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009598</td>\n",
       "      <td>0.011439</td>\n",
       "      <td>0.028416</td>\n",
       "      <td>nadal confirmed mexican open february rafael n...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.0116</td>\n",
       "      <td>0.011303</td>\n",
       "      <td>-0.001740</td>\n",
       "      <td>0.018295</td>\n",
       "      <td>-0.002178</td>\n",
       "      <td>0.075618</td>\n",
       "      <td>didnt want pop yep chapel hill next wednesday ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    bing_mpqa_score  afinn_score  wordnet_score  sem_eval_score  \\\n",
       "0            0.0058     0.005651      -0.003480        0.008469   \n",
       "3            0.0116     0.011303       0.003480       -0.003356   \n",
       "6            0.0058     0.005651      -0.017398       -0.002014   \n",
       "7            0.0000     0.000000      -0.010439        0.004899   \n",
       "9           -0.0058    -0.005651      -0.008699        0.000000   \n",
       "10          -0.0058    -0.005651      -0.017398        0.016953   \n",
       "11           0.0000     0.000000      -0.005219        0.000000   \n",
       "12           0.0116     0.011303       0.005219        0.006484   \n",
       "14           0.0116     0.011303       0.000000        0.009598   \n",
       "15           0.0116     0.011303      -0.001740        0.018295   \n",
       "\n",
       "    Senticnet_score  final_score  \\\n",
       "0          0.003552     0.013699   \n",
       "3          0.015833     0.027176   \n",
       "6          0.003386     0.014754   \n",
       "7         -0.016451    -0.064887   \n",
       "9         -0.011883    -0.041962   \n",
       "10         0.004136    -0.021504   \n",
       "11        -0.008929    -0.014154   \n",
       "12         0.014127     0.059681   \n",
       "14         0.011439     0.028416   \n",
       "15        -0.002178     0.075618   \n",
       "\n",
       "                                         final_tweets         2  \n",
       "0        gas house hit going chapel hill sat positive  positive  \n",
       "3   iranian general say israels iron dome deal_not...  negative  \n",
       "6   davlar th main rival team poland hopefully mak...  positive  \n",
       "7   talking acts sats deciding want go college app...  negative  \n",
       "9   may superbowl dallas dallas winning_not superb...  negative  \n",
       "10  instant message bringing monster load candy to...   neutral  \n",
       "11  apple software retail chief overhaul san franc...   neutral  \n",
       "12  watched sridevis comeback remember sun morning...  positive  \n",
       "14  nadal confirmed mexican open february rafael n...   neutral  \n",
       "15  didnt want pop yep chapel hill next wednesday ...  positive  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BingMpqaScore = []\n",
    "AfinnScore, AfinnReps = [], []\n",
    "WordnetScore, WordnetReps = [], []\n",
    "SenticnetScore, SenticnetReps = [], []\n",
    "length = len(raw_tweets_bing)\n",
    "\n",
    "for tw in raw_tweets_bing:\n",
    "    Bing_MPQA = 0\n",
    "    for i in tw:\n",
    "        if (i == 'positive'):\n",
    "            Bing_MPQA +=  1\n",
    "        if (i == 'negative'):\n",
    "            Bing_MPQA -= 1\n",
    "    BingMpqaScore.append(Bing_MPQA)\n",
    "    tmp = afinnPolarity(tw, afinn)\n",
    "    AfinnScore.append(tmp[0])\n",
    "    AfinnReps.append(tmp[1])\n",
    "    tmp = WordnetPolarity(tw, sentiWordnet)\n",
    "    WordnetScore.append(tmp[0])\n",
    "    WordnetReps.append(tmp[1])\n",
    "    tmp = SenticnetPolarity(tw)\n",
    "    SenticnetScore.append(tmp[0])\n",
    "    SenticnetReps.append(tmp[1])\n",
    "\n",
    "    \n",
    "#reshape\n",
    "BingMpqaScore = np.array(BingMpqaScore).reshape(length, 1)\n",
    "AfinnScore = np.array(AfinnScore).reshape(length, 1)\n",
    "AfinnReps = np.array(AfinnReps).reshape(length, 1)\n",
    "WordnetScore = np.array(WordnetScore).reshape(length, 1)\n",
    "WordnetReps = np.array(WordnetReps).reshape(length, 1)\n",
    "SemEvalScore = np.array(SemEvalScore).reshape(length, 1)\n",
    "SemEvalReps = np.array(SemEvalReps).reshape(length, 1)\n",
    "SenticnetScore = np.array(SenticnetScore).reshape(length, 1)\n",
    "SenticnetReps = np.array(SenticnetReps).reshape(length, 1)\n",
    "unigram140Score = np.array(unigram140Score).reshape(length, 1)\n",
    "unigram140Reps = np.array(unigram140Reps).reshape(length, 1)\n",
    "bigram140Score = np.array(bigram140Score).reshape(length, 1)\n",
    "bigram140Reps = np.array(bigram140Reps).reshape(length, 1)\n",
    "\n",
    "#Normalization\n",
    "BingMpqaScore = BingMpqaScore/np.linalg.norm(BingMpqaScore)\n",
    "AfinnScore = AfinnScore/np.linalg.norm(AfinnScore)\n",
    "AfinnReps = AfinnReps/np.linalg.norm(AfinnReps)\n",
    "WordnetScore = WordnetScore/np.linalg.norm(WordnetScore)\n",
    "WordnetReps = WordnetReps/np.linalg.norm(WordnetReps)\n",
    "SemEvalScore = SemEvalScore/np.linalg.norm(SemEvalScore)\n",
    "SemEvalReps = SemEvalReps/np.linalg.norm(SemEvalReps)\n",
    "SenticnetScore = SenticnetScore/np.linalg.norm(SenticnetScore)\n",
    "SenticnetReps = SenticnetReps/np.linalg.norm(SenticnetReps)\n",
    "unigram140Score = unigram140Score/np.linalg.norm(unigram140Score)\n",
    "unigram140Reps = unigram140Reps/np.linalg.norm(unigram140Reps)\n",
    "bigram140Score = bigram140Score/np.linalg.norm(bigram140Score)\n",
    "bigram140Reps = bigram140Reps/np.linalg.norm(bigram140Reps)\n",
    "\n",
    "\n",
    "\n",
    "#final_score_tweets (my score list)\n",
    "df['bing_mpqa_score'] = BingMpqaScore\n",
    "df['afinn_score'] = AfinnScore\n",
    "df['wordnet_score'] = WordnetScore\n",
    "df['sem_eval_score'] = SemEvalScore\n",
    "df['Senticnet_score'] = SenticnetScore\n",
    "all_scores = np.hstack( (BingMpqaScore, AfinnScore, WordnetScore, SemEvalScore, SenticnetScore, \\\n",
    "                                                                         unigram140Score, bigram140Score) )\n",
    "sum_score = np.sum(all_scores, axis=1).reshape(length, 1)\n",
    "print (all_scores.shape)\n",
    "df['final_score'] = sum_score\n",
    "\n",
    "df[['bing_mpqa_score','afinn_score', 'wordnet_score', 'sem_eval_score', 'Senticnet_score', \\\n",
    "                                'final_score', 'final_tweets' ,2]].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## <font color='red'>Entraîner le modèle</font>\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construire un modèle NN avec des cellules LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Dense, Input, Dropout, LSTM, Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.initializers import glorot_uniform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tout d'abord, nous allons lire le pre-trained  [Global Vectors for Word Representation Embedding](https://nlp.stanford.edu/projects/glove/) réalisés par le personnel de stanford Jeffrey Pennington, Richard Socher, Christopher D. Manning. Plus précisément, le 'glove.6B.50d' qui contient 400 000 mots chaque mot a un vecteur 50 dimensions. Le fichier glove.6B.50d.txt se trouve dans le répertoire 'resources /'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_glove_vecs(glove_file):\n",
    "    with open(glove_file, 'r') as f:\n",
    "        words = set()\n",
    "        word_to_vec_map = {}\n",
    "        for line in f:\n",
    "            line = line.strip().split()\n",
    "            curr_word = line[0]\n",
    "            words.add(curr_word)\n",
    "            word_to_vec_map[curr_word] = np.array(line[1:], dtype=np.float64)\n",
    "        \n",
    "        i = 1\n",
    "        words_to_index = {}\n",
    "        index_to_words = {}\n",
    "        for w in sorted(words):\n",
    "            words_to_index[w] = i\n",
    "            index_to_words[i] = w\n",
    "            i = i + 1\n",
    "    return words_to_index, index_to_words, word_to_vec_map\n",
    "\n",
    "\n",
    "word_to_index, index_to_word, word_to_vec_map = read_glove_vecs('/data/resources/glove.6B.50d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400000, 400000, 400000, 50)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_to_index.keys()), len(index_to_word.keys()), len(word_to_vec_map.keys()), len(word_to_vec_map['love'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(4047, 133),\n",
       " (2306, 107),\n",
       " (686, 25),\n",
       " (1772, 23),\n",
       " (3589, 23),\n",
       " (625, 22),\n",
       " (3676, 22),\n",
       " (6152, 22),\n",
       " (69, 21),\n",
       " (1492, 21)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#the lengths of the tweets in the train dataset\n",
    "sorted([(idx, len(tweet)) for idx, tweet in enumerate(raw_tweets_bing)], key=lambda x: x[1], reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pretrained_embedding_layer():\n",
    "    \"\"\"\n",
    "    Creates a Keras Embedding() layer and loads in pre-trained GloVe 50-dimensional vectors.\n",
    "    And it returns the pretrained embedding_layer layer Keras instance\n",
    "    \"\"\"\n",
    "    \n",
    "    vocab_len = len(word_to_index) + 1          # adding 1 to fit Keras embedding (requirement) (= 400,001)\n",
    "    emb_dim = 50                                # define dimensionality of your GloVe word vectors (= 50)\n",
    "    \n",
    "    # Initialize the embedding matrix as a numpy array of zeros of shape\n",
    "    emb_matrix = np.zeros((vocab_len, emb_dim))\n",
    "    \n",
    "    # Set each row \"index\" of the embedding matrix to be the word vector representation\n",
    "    # of the \"index\"th word of the vocabulary\n",
    "    for word, index in word_to_index.items():\n",
    "        emb_matrix[index, :] = word_to_vec_map[word]\n",
    "\n",
    "    # Define Keras embedding layer with the correct output/input sizes, make it non-trainable.\n",
    "    embedding_layer = Embedding(input_dim=vocab_len, output_dim=emb_dim, trainable=False)\n",
    "\n",
    "    # Build the embedding layer, it is required before setting the weights of the embedding layer.\n",
    "    # Do not modify the \"None\".\n",
    "    embedding_layer.build((None,))\n",
    "    \n",
    "    # Set the weights of the embedding layer to the embedding matrix. Your layer is now pretrained.\n",
    "    embedding_layer.set_weights([emb_matrix])\n",
    "    \n",
    "    return embedding_layer\n",
    "\n",
    "embedding_layer = pretrained_embedding_layer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_model(max_len):\n",
    "    \"\"\"\n",
    "    Creating the model's graph.\n",
    "    \n",
    "    Arguments:\n",
    "    input_shape -- shape of the input, usually (max_len,)\n",
    "    word_to_vec_map -- dictionary mapping every word in a vocabulary into its 50-dimensional vector representation\n",
    "    word_to_index -- dictionary mapping from words to their indices in the vocabulary (400,001 words)\n",
    "\n",
    "    Returns:\n",
    "    model -- a model instance in Keras\n",
    "    \"\"\"\n",
    "    input_shape = (max_len,)\n",
    "    # Define sentence_indices as the input of the graph,\n",
    "    sentence_indices = Input(shape=input_shape, dtype='int32')\n",
    "    \n",
    "    # Create the embedding layer pretrained with GloVe Vectors\n",
    "    embedding_layer = pretrained_embedding_layer()\n",
    "    \n",
    "    # Propagate sentence_indices through your embedding layer, you get back the embeddings\n",
    "    embeddings = embedding_layer(sentence_indices)\n",
    "    \n",
    "    # Using an LSTM layer with 50-dimensional hidden state (returned output is a batch)\n",
    "    X = LSTM(50, return_sequences=True)(embeddings)\n",
    "    # Add dropout with a probability of 0.5\n",
    "    X = Dropout(0.5)(X)\n",
    "    # Using an LSTM layer with 25-dimensional hidden state (returned output is a batch)\n",
    "    X = LSTM(25, return_sequences=True)(X)\n",
    "    # Add dropout with a probability of 0.5\n",
    "    X = Dropout(0.5)(X)\n",
    "    # Using another LSTM layer with 10-dimensional hidden state (returned output is a single hidden state)\n",
    "    X = LSTM(10)(X)\n",
    "    # Add dropout with a probability of 0.5\n",
    "    X = Dropout(0.5)(X)\n",
    "    # Adding a Dense layer with softmax activation to get back a batch of 3-dimensional vectors.\n",
    "    X = Dense(3, activation='softmax')(X)\n",
    "    # Add a softmax function as activation\n",
    "    X = Activation('softmax')(X)\n",
    "    \n",
    "    # Create Model instance which converts sentence_indices into X.\n",
    "    model = Model(inputs=sentence_indices, outputs=X)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "embedding_2 (Embedding)      (None, 50, 50)            20000050  \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 50, 50)            20200     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 50, 50)            0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 50, 25)            7600      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 50, 25)            0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 10)                1440      \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 3)                 33        \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 3)                 0         \n",
      "=================================================================\n",
      "Total params: 20,029,323\n",
      "Trainable params: 29,273\n",
      "Non-trainable params: 20,000,050\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_model(50) #choose '50' as input length\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sentences_to_indices(training_tweets):\n",
    "    \"\"\"\n",
    "    Converts an array of sentences (strings) into an array of indices corresponding to words in the sentences.\n",
    "    The output shape should be such that it can be given to `Embedding()`. And it returns array of indices \n",
    "    corresponding to words in the sentences.\n",
    "    \"\"\"\n",
    "    \n",
    "    m = len(training_tweets)                      # number of training examples (should be 7205)\n",
    "        \n",
    "    # Initialize 'indices' as a numpy matrix of zeros\n",
    "    indices = np.zeros((m, 50)) #'50' is the input length\n",
    "        \n",
    "    for i in range(m):\n",
    "        for j, w in enumerate(training_tweets[i]):\n",
    "            # Set the (i,j)th entry of 'indices' to the index of the correct word.\n",
    "            try:\n",
    "                indices[i, j] = word_to_index[w]\n",
    "            # If the word 'w' doesn't exist in our dictionary, set its index into 0\n",
    "            except KeyError:\n",
    "                if j < 50:\n",
    "                    indices[i, j] = 0\n",
    "                break\n",
    "    \n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_to_one_hot(Y, C):\n",
    "    Y = np.eye(C)[Y.values.reshape(-1)]\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7205, 50)\n",
      "(7205, 3)\n"
     ]
    }
   ],
   "source": [
    "X_train_indices = sentences_to_indices(raw_tweets_bing)\n",
    "Y_train_oh = convert_to_one_hot(labels, C = 3)\n",
    "print (X_train_indices.shape)\n",
    "print (Y_train_oh.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "7205/7205 [==============================] - 34s - loss: 1.0478 - acc: 0.4718    \n",
      "Epoch 2/10\n",
      "7205/7205 [==============================] - 33s - loss: 1.0399 - acc: 0.4636    \n",
      "Epoch 3/10\n",
      "7205/7205 [==============================] - 33s - loss: 1.0268 - acc: 0.4787    \n",
      "Epoch 4/10\n",
      "7205/7205 [==============================] - 33s - loss: 1.0095 - acc: 0.5149    \n",
      "Epoch 5/10\n",
      "7205/7205 [==============================] - 33s - loss: 1.0080 - acc: 0.5294    \n",
      "Epoch 6/10\n",
      "7205/7205 [==============================] - 33s - loss: 1.0095 - acc: 0.5251    \n",
      "Epoch 7/10\n",
      "7205/7205 [==============================] - 33s - loss: 1.0156 - acc: 0.5092    \n",
      "Epoch 8/10\n",
      "7205/7205 [==============================] - 33s - loss: 1.0237 - acc: 0.4873    \n",
      "Epoch 9/10\n",
      "7205/7205 [==============================] - 33s - loss: 1.0212 - acc: 0.4910    \n",
      "Epoch 10/10\n",
      "7205/7205 [==============================] - 33s - loss: 1.0204 - acc: 0.4930    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f374502f7f0>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_indices, Y_train_oh, epochs = 10, batch_size = 32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sentence_to_avg(train_indices, weights):\n",
    "    \"\"\"\n",
    "    This function returns the average vector encoding information about the sentence as \n",
    "    numpy-array of shape (50,)\n",
    "    \"\"\"\n",
    "    # Initializing the average word vector.\n",
    "    length = len(train_indices)\n",
    "    avg = np.zeros((length, 50))\n",
    "    \n",
    "    # Averaging the word vectors.\n",
    "    for i in range(length):\n",
    "        for idx in train_indices[i]:\n",
    "            avg[i, :] += weights[int(idx), :]\n",
    "        avg[i, :] /= len(train_indices[i])\n",
    "    \n",
    "    return avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7205, 50)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_embedding_features = sentence_to_avg(X_train_indices, model.get_weights()[0])\n",
    "word_embedding_features = scipy.sparse.csr_matrix(word_embedding_features)\n",
    "word_embedding_features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Créer le feature vector\n",
    "* See [Bag of Words](https://en.wikipedia.org/wiki/Bag-of-words_model) for more details (Supprimer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7205, 134865)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vectorizer = CountVectorizer(analyzer='word', preprocessor=None, stop_words=None, tokenizer=None, ngram_range=(1,3))\n",
    "count_features = count_vectorizer.fit_transform(final_tweets)\n",
    "count_features = scipy.sparse.csr_matrix(count_features)\n",
    "count_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(7205, 5)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#reducing the CountVector\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "svd = TruncatedSVD(n_components=5, n_iter=7, random_state=42)\n",
    "count_features = svd.fit_transform(count_features)\n",
    "count_features = scipy.sparse.csr_matrix(count_features)\n",
    "print (type(count_features))\n",
    "count_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7205, 134865)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(analyzer='word', preprocessor=None, stop_words=None, tokenizer=None, ngram_range=(1,3))\n",
    "tfidf_features = tfidf_vectorizer.fit_transform(final_tweets)\n",
    "del final_tweets\n",
    "tfidf_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7205, 7) (7205, 1) (7205, 1) (7205, 1) (7205, 1)\n",
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "(7205, 13)\n"
     ]
    }
   ],
   "source": [
    "print (all_scores.shape, sum_score.shape, AfinnReps.shape, WordnetReps.shape, SemEvalReps.shape)\n",
    "final_total = scipy.sparse.csr_matrix(np.hstack( (all_scores, sum_score, AfinnReps, WordnetReps, SemEvalReps, unigram140Reps, bigram140Reps) ))\n",
    "print (type(final_total))\n",
    "print (final_total.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7205, 134933)\n"
     ]
    }
   ],
   "source": [
    "features = scipy.sparse.hstack([word_embedding_features, count_features, tfidf_features, final_total])\n",
    "print (features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# supprimer les objets inutiles\n",
    "del all_scores, sum_score, BingMpqaScore, AfinnScore, WordnetScore, SemEvalScore, unigram140Score, bigram140Score\n",
    "del AfinnReps, WordnetReps, SemEvalReps, unigram140Reps, bigram140Reps\n",
    "del count_features, tfidf_features, final_total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import SVM\n",
    "\n",
    "http://scikit-learn.org/stable/modules/svm.html\n",
    "\n",
    "For a mathematical overview,\n",
    "https://docs.opencv.org/2.4/doc/tutorials/ml/introduction_to_svm/introduction_to_svm.html\n",
    "\n",
    "#### Get the optimal regulation parameter using handout method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.663011797363\n",
      "20\n",
      "{'C': 1.8473875001435647, 'gamma': 0.21612102798126989}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x7f3744d1fcc0>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAEKCAYAAAARnO4WAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3744c48e10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Takes around 30 minutes\n",
    "rand_list = {\"C\": scipy.stats.uniform(0, 5),\n",
    "             \"gamma\": scipy.stats.uniform(0.1, 1)}\n",
    "\n",
    "rand_search = RandomizedSearchCV(SVC(kernel='linear'), param_distributions = rand_list,\\\n",
    "                                 n_iter = 20, n_jobs = 4, scoring = 'f1_micro')\n",
    "rand_search.fit(features, labels)\n",
    "\n",
    "print(rand_search.best_score_)\n",
    "print (len(rand_search.cv_results_['param_C'].data))\n",
    "print (rand_search.best_params_)\n",
    "\n",
    "rand_search.cv_results_['param_gamma'].data\n",
    "plt.plot(sorted(rand_search.cv_results_['mean_test_score']), rand_search.cv_results_['param_C'].data, \"-\")\n",
    "plt.xlabel(\"Mean Test Score\")\n",
    "plt.ylabel(\"C value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.8473875001435647, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape=None, degree=3, gamma=0.21612102798126989,\n",
       "  kernel='linear', max_iter=-1, probability=False, random_state=None,\n",
       "  shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "KERNEL = 'linear'\n",
    "classifier = SVC(kernel=KERNEL, C=rand_search.best_params_['C'], gamma=rand_search.best_params_['gamma'])\n",
    "classifier.fit(features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "def plot_learning_curve(estimator, title, training_curve=False, X, y, train_sizes=np.linspace(.1, 1.0, 5)):\n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    if not training_curve:\n",
    "        plt.ylim((0.4, 0.7))\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    train_sizes, train_scores, test_scores = learning_curve( estimator, X, y, cv=None, n_jobs=1, train_sizes=train_sizes)\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    plt.grid()\n",
    "    \n",
    "    if training_curve:\n",
    "        plt.fill_between(train_sizes, train_scores_mean-train_scores_std, \\\n",
    "                         train_scores_mean+train_scores_std, alpha=0.1, color=\"r\")\n",
    "        plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\", label=\"Training score\")\n",
    "    \n",
    "    plt.fill_between(train_sizes, test_scores_mean-test_scores_std,test_scores_mean+test_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\", label=\"Cross-validation score\")\n",
    "\n",
    "    plt.legend(loc=\"best\")\n",
    "    return plt\n",
    "\n",
    "plot_learning_curve(classifier, \"SVC Learning Curve\", features, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prédiction sur les données d'apprentissage "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9942\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "nb_predict_train = classifier.predict(features)\n",
    "#check accuracy\n",
    "print(\"Accuracy: {:0.4f}\".format(metrics.accuracy_score(labels, nb_predict_train)))\n",
    "del features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2673    1   18]\n",
      " [   2 1006   16]\n",
      " [   4    1 3484]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       1.00      0.99      1.00      2692\n",
      "         -1       1.00      0.98      0.99      1024\n",
      "          0       0.99      1.00      0.99      3489\n",
      "\n",
      "avg / total       0.99      0.99      0.99      7205\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#print confusion matrix\n",
    "print(\"{}\".format(metrics.confusion_matrix(labels, nb_predict_train, \n",
    "                                           labels=[1,-1, 0])))\n",
    "\n",
    "print(\"{}\".format(metrics.classification_report(labels, nb_predict_train, \n",
    "                                                labels=[1, -1, 0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prédire en utilisant le modèle\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importer test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8902, 4)\n"
     ]
    }
   ],
   "source": [
    "t_df = pd.read_csv('/data/test/actual/test_B_labeled.tsv', sep='\\t', header=None)\n",
    "print(t_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7584, 4)\n",
      "(7584,)\n"
     ]
    }
   ],
   "source": [
    "t_df = t_df[t_df[3] != 'Not Available']\n",
    "actual_labels = t_df[2]\n",
    "actual_labels = actual_labels.map(mapper)\n",
    "print(t_df.shape)\n",
    "print(actual_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>282031301962395648</td>\n",
       "      <td>T14111200</td>\n",
       "      <td>neutral</td>\n",
       "      <td>dec 21st 2012 will be know not as the end of t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11975</td>\n",
       "      <td>SM112166</td>\n",
       "      <td>negative</td>\n",
       "      <td>Yar he quite clever but aft many guesses lor. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>136592</td>\n",
       "      <td>LJ112295</td>\n",
       "      <td>negative</td>\n",
       "      <td>Yeah we have Thin Lizzy here I HATE the inform...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>253421252956545024</td>\n",
       "      <td>T13114433</td>\n",
       "      <td>neutral</td>\n",
       "      <td>MT @LccSy #Syria, Deir Ezzor: Ali Bashar al-th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>220880422320603137</td>\n",
       "      <td>T14114138</td>\n",
       "      <td>negative</td>\n",
       "      <td>@MacMiller hate my life, because i can't see y...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    0          1         2  \\\n",
       "2  282031301962395648  T14111200   neutral   \n",
       "3               11975   SM112166  negative   \n",
       "4              136592   LJ112295  negative   \n",
       "5  253421252956545024  T13114433   neutral   \n",
       "6  220880422320603137  T14114138  negative   \n",
       "\n",
       "                                                   3  \n",
       "2  dec 21st 2012 will be know not as the end of t...  \n",
       "3  Yar he quite clever but aft many guesses lor. ...  \n",
       "4  Yeah we have Thin Lizzy here I HATE the inform...  \n",
       "5  MT @LccSy #Syria, Deir Ezzor: Ali Bashar al-th...  \n",
       "6  @MacMiller hate my life, because i can't see y...  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAnMAAAHoCAYAAAAi+WkTAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nnQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f37431f3908>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y = [len(t_df[t_df[2] == i]) for i in ['positive', 'negative', 'neutral']]\n",
    "x = ['positive', 'negative', 'neutral']\n",
    "x_pos = range(len(x))\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.bar(x_pos, y, alpha=0.5)\n",
    "plt.xticks(x_pos, x)\n",
    "plt.ylabel('# Occurences').set_size(15)\n",
    "plt.xlabel('Classes').set_size(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pré-traiter les tweets de l'ensemble de données de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_tweets_test = t_df[3]\n",
    "\n",
    "t_unigram140Score, t_unigram140Reps = [], []\n",
    "for tweet in raw_tweets_test:\n",
    "    score, reps = unigram140Polarity(tweet.lower(), unigram140_d)\n",
    "    t_unigram140Score.append(score)\n",
    "    t_unigram140Reps.append(reps)\n",
    "\n",
    "t_bigram140Score, t_bigram140Reps = [], []\n",
    "for tweet in raw_tweets_test:\n",
    "    score, reps = bigram140Polarity(tweet.lower(), bigram140_d)\n",
    "    t_bigram140Score.append(score)\n",
    "    t_bigram140Reps.append(reps)\n",
    "\n",
    "t_SemEvalScore, t_SemEvalReps = [], []\n",
    "for tweet in raw_tweets_test:\n",
    "    score, reps = SemEvalLexiconPolarity(tweet.lower(), EnglishLexicon)\n",
    "    t_SemEvalScore.append(score)\n",
    "    t_SemEvalReps.append(reps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_tweets_test = [replaceSlangs(tweet, slangs) for tweet in raw_tweets_test]\n",
    "raw_tweets_test = [replace_apostrophe(tweet, apos) for tweet in raw_tweets_test]\n",
    "raw_tweets_test = [subsEmoticon(tweet, dict) for tweet in raw_tweets_test]\n",
    "raw_tweets_test = [handle_negation(tweet) for tweet in raw_tweets_test] #negation\n",
    "lemmatized_tweets_test = [lemma(tweet) for tweet in raw_tweets_test]\n",
    "clean_tweets_test = [remove_hashtags(tweet) for tweet in lemmatized_tweets_test]\n",
    "preprocessed_tweets_test = [preprocess(tweet) for tweet in clean_tweets_test]\n",
    "final_tweets_test = [rem_stop(tweet) for tweet in preprocessed_tweets_test]\n",
    "t_df[3] = final_tweets_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done scoring test set ..\n",
      "Done reshaping test set ..\n",
      "Done normalizing test set ..\n",
      "(7584, 7)\n"
     ]
    }
   ],
   "source": [
    "t_raw_tweets_MPQA = [subsMPQA(tweet,dictionary) for tweet in final_tweets_test]\n",
    "t_raw_tweets_wsd = [subs_pos(tweet, expanded_pos) for tweet in t_raw_tweets_MPQA]\n",
    "t_raw_tweets_wsd = [subs_neg(tweet, expanded_neg) for tweet in t_raw_tweets_wsd]\n",
    "t_raw_tweets_bing = [subs_pos(tweet, expanded_pos) for tweet in t_raw_tweets_wsd]\n",
    "t_raw_tweets_bing = [subs_neg(tweet, expanded_neg) for tweet in t_raw_tweets_bing]\n",
    "\n",
    "t_BingMpqaScore = []\n",
    "t_AfinnScore, t_AfinnReps = [], []\n",
    "t_WordnetScore, t_WordnetReps = [], []\n",
    "t_SenticnetScore, t_SenticnetReps = [], []\n",
    "t_length = len(t_raw_tweets_bing)\n",
    "\n",
    "for tw in t_raw_tweets_bing:\n",
    "    Bing_MPQA = 0\n",
    "    for i in tw:\n",
    "        if (i == 'positive'):\n",
    "            Bing_MPQA +=  1\n",
    "        if (i == 'negative'):\n",
    "            Bing_MPQA -= 1\n",
    "    t_BingMpqaScore.append(Bing_MPQA)\n",
    "    tmp = afinnPolarity(tw, afinn)\n",
    "    t_AfinnScore.append(tmp[0])\n",
    "    t_AfinnReps.append(tmp[1])\n",
    "    tmp = WordnetPolarity(tw, sentiWordnet)\n",
    "    t_WordnetScore.append(tmp[0])\n",
    "    t_WordnetReps.append(tmp[1])\n",
    "    tmp = SenticnetPolarity(tw)\n",
    "    t_SenticnetScore.append(tmp[0])\n",
    "    t_SenticnetReps.append(tmp[1])\n",
    "print(\"Done scoring test set ..\")\n",
    "    \n",
    "#reshape\n",
    "t_BingMpqaScore = np.array(t_BingMpqaScore).reshape(t_length, 1)\n",
    "t_AfinnScore = np.array(t_AfinnScore).reshape(t_length, 1)\n",
    "t_AfinnReps = np.array(t_AfinnReps).reshape(t_length, 1)\n",
    "t_WordnetScore = np.array(t_WordnetScore).reshape(t_length, 1)\n",
    "t_WordnetReps = np.array(t_WordnetReps).reshape(t_length, 1)\n",
    "t_SemEvalScore = np.array(t_SemEvalScore).reshape(t_length, 1)\n",
    "t_SemEvalReps = np.array(t_SemEvalReps).reshape(t_length, 1)\n",
    "t_SenticnetScore = np.array(t_SenticnetScore).reshape(t_length, 1)\n",
    "t_SenticnetReps = np.array(t_SenticnetReps).reshape(t_length, 1)\n",
    "t_unigram140Score = np.array(t_unigram140Score).reshape(t_length, 1)\n",
    "t_unigram140Reps = np.array(t_unigram140Reps).reshape(t_length, 1)\n",
    "t_bigram140Score = np.array(t_bigram140Score).reshape(t_length, 1)\n",
    "t_bigram140Reps = np.array(t_bigram140Reps).reshape(t_length, 1)\n",
    "print(\"Done reshaping test set ..\")\n",
    "\n",
    "#Normalization\n",
    "t_BingMpqaScore = t_BingMpqaScore/np.linalg.norm(t_BingMpqaScore)\n",
    "t_AfinnScore = t_AfinnScore/np.linalg.norm(t_AfinnScore)\n",
    "t_AfinnReps = t_AfinnReps/np.linalg.norm(t_AfinnReps)\n",
    "t_WordnetScore = t_WordnetScore/np.linalg.norm(t_WordnetScore)\n",
    "t_WordnetReps = t_WordnetReps/np.linalg.norm(t_WordnetReps)\n",
    "t_SemEvalScore = t_SemEvalScore/np.linalg.norm(t_SemEvalScore)\n",
    "t_SemEvalReps = t_SemEvalReps/np.linalg.norm(t_SemEvalReps)\n",
    "t_SenticnetScore = t_SenticnetScore/np.linalg.norm(t_SenticnetScore)\n",
    "t_SenticnetReps = t_SenticnetReps/np.linalg.norm(t_SenticnetReps)\n",
    "t_unigram140Score = t_unigram140Score/np.linalg.norm(t_unigram140Score)\n",
    "t_unigram140Reps = t_unigram140Reps/np.linalg.norm(t_unigram140Reps)\n",
    "t_bigram140Score = t_bigram140Score/np.linalg.norm(t_bigram140Score)\n",
    "t_bigram140Reps = t_bigram140Reps/np.linalg.norm(t_bigram140Reps)\n",
    "print(\"Done normalizing test set ..\")\n",
    "\n",
    "t_all_scores = np.hstack( (t_BingMpqaScore, t_AfinnScore, t_WordnetScore, t_SemEvalScore, t_SenticnetScore, \\\n",
    "                                               t_unigram140Score, t_bigram140Score) )\n",
    "t_sum_score = np.sum(t_all_scores, axis=1).reshape(t_length, 1)\n",
    "print (t_all_scores.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7584, 50)\n",
      "(7584, 3)\n"
     ]
    }
   ],
   "source": [
    "X_test_indices = sentences_to_indices(t_raw_tweets_bing)\n",
    "Y_test_oh = convert_to_one_hot(actual_labels, C = 3)\n",
    "print (X_test_indices.shape)\n",
    "print (Y_test_oh.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2634, 734),\n",
       " (634, 174),\n",
       " (7066, 38),\n",
       " (1052, 36),\n",
       " (2247, 33),\n",
       " (633, 32),\n",
       " (3466, 32),\n",
       " (313, 31),\n",
       " (6554, 31),\n",
       " (982, 30)]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#the lengths of the tweets in the train dataset\n",
    "sorted([(i, len(tweet)) for i, tweet in enumerate(t_raw_tweets_bing)], key=lambda x: x[1], reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7584, 50)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_word_embedding_features = sentence_to_avg(X_test_indices, model.get_weights()[0])\n",
    "t_word_embedding_features = scipy.sparse.csr_matrix(t_word_embedding_features)\n",
    "t_word_embedding_features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Créer le features vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7584, 5)\n",
      "(7584, 134865)\n"
     ]
    }
   ],
   "source": [
    "test_count_features = count_vectorizer.transform(final_tweets_test)\n",
    "test_count_features = svd.transform(test_count_features)\n",
    "test_count_features = scipy.sparse.csr_matrix(test_count_features)\n",
    "print (test_count_features.shape)\n",
    "\n",
    "\n",
    "test_tfidf_features = tfidf_vectorizer.transform(final_tweets_test)\n",
    "test_tfidf_features = scipy.sparse.csr_matrix(test_tfidf_features)\n",
    "print (test_tfidf_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7584, 13)\n",
      "(7584, 134933)\n"
     ]
    }
   ],
   "source": [
    "t_final_total = scipy.sparse.csr_matrix(np.hstack( (t_all_scores, t_sum_score, t_AfinnReps, t_WordnetReps, t_SemEvalReps, t_unigram140Reps, t_bigram140Reps) ))\n",
    "print (t_final_total.shape)\n",
    "test_features = scipy.sparse.hstack([t_word_embedding_features, test_count_features, test_tfidf_features, t_final_total])\n",
    "print (test_features.shape)\n",
    "\n",
    "\n",
    "# Delete\n",
    "del raw_tweets_test, lemmatized_tweets_test, preprocessed_tweets_test\n",
    "del t_raw_tweets_MPQA, t_raw_tweets_bing\n",
    "del t_word_embedding_features\n",
    "del t_all_scores, t_sum_score, t_BingMpqaScore, t_AfinnScore, t_WordnetScore, t_SemEvalScore, t_unigram140Score, t_bigram140Score\n",
    "del t_AfinnReps, t_WordnetReps, t_SemEvalReps, t_unigram140Reps, t_bigram140Reps\n",
    "del test_count_features, test_tfidf_features, t_final_total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prédire les étiquettes en utilisant le modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predicted_labels = classifier.predict(test_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Évaluer le modèle\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 67.66%\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy: {:0.2f}%'.format(metrics.accuracy_score(actual_labels, predicted_labels) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -1       0.64      0.52      0.57      1296\n",
      "          0       0.65      0.79      0.71      3448\n",
      "          1       0.73      0.61      0.67      2840\n",
      "\n",
      "avg / total       0.68      0.68      0.67      7584\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Rapport de classification\n",
    "print('{}'.format(metrics.classification_report(actual_labels, predicted_labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voir [Confusion Matrix](https://fr.wikipedia.org/wiki/Matrice_de_confusion) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1734  145  961]\n",
      " [ 135  673  488]\n",
      " [ 494  230 2724]]\n",
      "\n",
      "\u001b[31m\" macro f1 score \"\u001b[0m\n",
      "0.6518791442500445\n",
      "\n",
      "\u001b[31m\" micro f1 score \"\u001b[0m\n",
      "0.6765559071729957\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Confusion Matrix\n",
    "print('{}\\n'.format(metrics.confusion_matrix(actual_labels, predicted_labels, labels=[1,-1,0])))\n",
    "#-------------------- F1-score --------------------\n",
    "print(\"\\x1b[31m\\\" macro f1 score \\\"\\x1b[0m\")\n",
    "print('{}\\n'.format(metrics.f1_score(actual_labels, predicted_labels, average='macro')))\n",
    "print(\"\\x1b[31m\\\" micro f1 score \\\"\\x1b[0m\")\n",
    "print('{}\\n'.format(metrics.f1_score(actual_labels, predicted_labels, average='micro')))\n",
    "\n",
    "#-------------------- precision-score --------------------\n",
    "print(\"\\x1b[31m\\\" macro precision score \\\"\\x1b[0m\")\n",
    "print('{}\\n'.format(metrics.precision_score(actual_labels, predicted_labels, average='macro')))\n",
    "print(\"\\x1b[31m\\\" micro precision score \\\"\\x1b[0m\")\n",
    "print('{}\\n'.format(metrics.precision_score(actual_labels, predicted_labels, average='micro')))\n",
    "\n",
    "#-------------------- recall-score --------------------\n",
    "print(\"\\x1b[31m\\\" macro recall score \\\"\\x1b[0m\")\n",
    "print('{}\\n'.format(metrics.recall_score(actual_labels, predicted_labels, average='macro')))\n",
    "print(\"\\x1b[31m\\\" micro recall score \\\"\\x1b[0m\")\n",
    "print('{}\\n'.format(metrics.recall_score(actual_labels, predicted_labels, average='micro')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparaison avec les 5 meilleures équipes du subtask B\n",
    "\n",
    "Nous comparons notre score avec les autres équipes de l'atelier. Les résultats sont tirés du document joint:\n",
    "[Final report SemEval 2014 Subtask 9](http://www.aclweb.org/anthology/S14-2009)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Team|Accuracy (Macro Averaged)| Accuracy (Micro Averaged)|\n",
    "|----|-------------------------|--------------------------|\n",
    "|TeamX|65.63%|69.99%|\n",
    "|coooolll|63.23%|70.51%|\n",
    "|RTRGO|63.08%|70.15%|\n",
    "|NRC-Canada|67.62%|71.37%|\n",
    "|TUGAS|63.89%|68.84%|\n",
    "|**_ME_**|_63.67%_|_67.44%_|\n",
    "| | |***classement : 10 / 50***|\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# <font color='red'> Analyse des modèles de classification:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Model|F1 Macro Averaging|F1 Micro Averaging|Accuracy|\n",
    "|-----|---------------|----------------|--|\n",
    "|NB (without Lexicons)|51.17%|60.28%|60.28%|\n",
    "|NB (with Lexicons)|46.00%|59.53%|59.53%|\n",
    "|LR (without Lexicons)|55.42%|64.39%|64.39%|\n",
    "|LR (with Lexicons)|40.83%|52.26%|52.26%|\n",
    "|SVM (without Lexicons)|61.92%|66.16%|66.17%|\n",
    "|SVM (with Afinn Lexicon)|59.81%|65.50%|65.51%|\n",
    "|SVM (with Lexicon)|65.53%|68.06%|68.06%|\n",
    "|SVM with 1 Layer of LSTM|65.37%|67.87%|67.88%|\n",
    "|SVM with three Layer of LSTM|65.18%|67.65|67.66%|\n",
    "\n",
    "\n",
    "Notes:\n",
    "- I have used CountVector and tfidfVector as bag of words.\n",
    "- In NB, I didn't use the TruncatedSVD model which summarizes the Count Vector. And that's because NB model doesn't accept negative numbers as input.\n",
    "- I used the whole Count Vector with NB and LR. I believe it's better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
