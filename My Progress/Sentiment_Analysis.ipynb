{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# <font color='green'>sentiment Analysis</font> ![title](./resources/img/sent_twitter.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='red'>Reading Data</font>\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import liberies\n",
    "\n",
    "Refer to the web pages for individual libraries\n",
    "* [Pandas](http://pandas.pydata.org/), to load and manage data\n",
    "* [Matplotlib](http://matplotlib.org/), for visualization\n",
    "* [numpy](http://www.numpy.org/) for painting representation and manipulation\n",
    "* [re](https://docs.python.org/3/library/re.html) for regular expression\n",
    "* [nltk](http://www.nltk.org/) for pretreatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "from copy import copy\n",
    "import collections\n",
    "import scipy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import model_selection\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading the dataset\n",
    "Some of the data \"downloaded_cleansed_B\" is produced out of the \"downloaded_cleansed_A\". The difference is:\n",
    "- \"downloaded_cleansed_A\" has three columns that we won't use.\n",
    "- \"downloaded_cleansed_A\" has repeatted tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9665, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>264183816548130816</td>\n",
       "      <td>15140428</td>\n",
       "      <td>positive</td>\n",
       "      <td>Gas by my house hit $3.39!!!! I'm going to Cha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>263405084770172928</td>\n",
       "      <td>591166521</td>\n",
       "      <td>negative</td>\n",
       "      <td>Not Available</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>262163168678248449</td>\n",
       "      <td>35266263</td>\n",
       "      <td>negative</td>\n",
       "      <td>Not Available</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>264249301910310912</td>\n",
       "      <td>18516728</td>\n",
       "      <td>negative</td>\n",
       "      <td>Iranian general says Israel's Iron Dome can't ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>262682041215234048</td>\n",
       "      <td>254373818</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Not Available</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    0          1         2  \\\n",
       "0  264183816548130816   15140428  positive   \n",
       "1  263405084770172928  591166521  negative   \n",
       "2  262163168678248449   35266263  negative   \n",
       "3  264249301910310912   18516728  negative   \n",
       "4  262682041215234048  254373818   neutral   \n",
       "\n",
       "                                                   3  \n",
       "0  Gas by my house hit $3.39!!!! I'm going to Cha...  \n",
       "1                                      Not Available  \n",
       "2                                      Not Available  \n",
       "3  Iranian general says Israel's Iron Dome can't ...  \n",
       "4                                      Not Available  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./data/train/downloaded_cleansed_B.tsv', sep= '\\t', header=None)\n",
    "print (df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that some tweets are \"Not Available\". We will reject them because it will not help in the analysis of feelings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Supprimer tous les tweets \"NOT AVAILABLE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>264183816548130816</td>\n",
       "      <td>15140428</td>\n",
       "      <td>positive</td>\n",
       "      <td>Gas by my house hit $3.39!!!! I'm going to Cha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>264249301910310912</td>\n",
       "      <td>18516728</td>\n",
       "      <td>negative</td>\n",
       "      <td>Iranian general says Israel's Iron Dome can't ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>264105751826538497</td>\n",
       "      <td>147088367</td>\n",
       "      <td>positive</td>\n",
       "      <td>with J Davlar 11th. Main rivals are team Polan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>264094586689953794</td>\n",
       "      <td>332474633</td>\n",
       "      <td>negative</td>\n",
       "      <td>Talking about ACT's &amp;amp;&amp;amp; SAT's, deciding...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>254941790757601280</td>\n",
       "      <td>557103111</td>\n",
       "      <td>negative</td>\n",
       "      <td>They may have a SuperBowl in Dallas, but Dalla...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    0          1         2  \\\n",
       "0  264183816548130816   15140428  positive   \n",
       "3  264249301910310912   18516728  negative   \n",
       "6  264105751826538497  147088367  positive   \n",
       "7  264094586689953794  332474633  negative   \n",
       "9  254941790757601280  557103111  negative   \n",
       "\n",
       "                                                   3  \n",
       "0  Gas by my house hit $3.39!!!! I'm going to Cha...  \n",
       "3  Iranian general says Israel's Iron Dome can't ...  \n",
       "6  with J Davlar 11th. Main rivals are team Polan...  \n",
       "7  Talking about ACT's &amp;&amp; SAT's, deciding...  \n",
       "9  They may have a SuperBowl in Dallas, but Dalla...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[df[3] != \"Not Available\"]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7205, 4)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color='blue'>Training tweets are too limited: just 7205 tweets ...</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "raw_tweets = list(df[3])\n",
    "labels = df[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gas by my house hit $3.39!!!! I'm going to Chapel Hill on Sat. :)\n",
      "\n",
      "\n",
      "Iranian general says Israel's Iron Dome can't deal with their missiles (keep talking like that and we may end up finding out)\n",
      "\n",
      "\n",
      "with J Davlar 11th. Main rivals are team Poland. Hopefully we an make it a successful end to a tough week of training tomorrow.\n",
      "\n",
      "\n",
      "Talking about ACT's &amp;&amp; SAT's, deciding where I want to go to college, applying to colleges and everything about college stresses me out.\n",
      "\n",
      "\n",
      "They may have a SuperBowl in Dallas, but Dallas ain't winning a SuperBowl. Not with that quarterback and owner. @S4NYC @RasmussenPoll\n",
      "\n",
      "\n",
      "Im bringing the monster load of candy tomorrow, I just hope it doesn't get all squiched\n",
      "\n",
      "\n",
      "Apple software, retail chiefs out in overhaul: SAN FRANCISCO Apple Inc CEO Tim Cook on Monday replaced the heads... http://t.co/X49ZEOsG\n",
      "\n",
      "\n",
      "@oluoch @victor_otti @kunjand I just watched it! Sridevi's comeback.... U remember her from the 90s?? Sun mornings on NTA ;)\n",
      "\n",
      "\n",
      "#Livewire Nadal confirmed for Mexican Open in February: Rafael Nadal is set to play at the Me... http://t.co/zgUXpcnC #LiveWireAthletics\n",
      "\n",
      "\n",
      "@MsSheLahY I didnt want to just pop up... but yep we have chapel hill next wednesday you should come.. and shes great ill tell her you asked\n",
      "\n",
      "\n",
      "@Alyoup005 @addicted2haley hmmmm  November is an odd release date if true but if it becomes big enough maybe she could sing it at Grammys\n",
      "\n",
      "\n",
      "#Iran US delisting MKO from global terrorists list in line with Iran campaign: Tehran, Oct 30, IRNA -- Secretary... http://t.co/9wWvxEbf\n",
      "\n",
      "\n",
      "Good Morning Becky ! Thursday is going to be Fantastic ! @SwedenG @DJ4JG @Grdina @Paverlayer @FSBull @RevkahJC @DicksTrash @borderfox116\n",
      "\n",
      "\n",
      "Expect light-moderate rains over E. Visayas; Cebu, Bohol, Samar &amp; Leyte have 30-70% chance of rains tonight! Expect fair weather tomorrow!:)\n",
      "\n",
      "\n",
      "One ticket left for the @49ers game tomorrow!  Don't miss the rematch of the NFC Championship game against the NY Giants!  Hit me up!\n",
      "\n",
      "\n",
      "AFC away fans on Saturday. All this stuff about the 'she said no' chant. It's bollocks. When he has the ball, just turn your back on him.\n",
      "\n",
      "\n",
      "Why is it so hard to find the @TVGuideMagazine these days? Went to 3 stores for the Castle cover issue. NONE. Will search again tomorrow...\n",
      "\n",
      "\n",
      "Game 1 of the NLCS and a rematch of the NFC Championship game tomorrow. SF's gonna be cuuuuhraaaaaaaazeeeee\n",
      "\n",
      "\n",
      "@TrevorJavier the heat game may cost alot more...and plus I would rather see Austin Rivers play\n",
      "\n",
      "\n",
      "Never start working on your dreams and goals tomorrow......tomorrow never comes....if it means anything to U, ACT NOW! #getafterit\n",
      "\n",
      "\n",
      "@TheFFAddict I had Vick and Flacco, needed an upgrade.  Vick may get benched, Jennings a back up again soon. I thought it was a win for me.\n",
      "\n",
      "\n",
      "Looks like Andy the Android may have had a little too much fun yesterday. http://t.co/7ZDEfzEC\n",
      "\n",
      "\n",
      "@APGPhoto oooh nice .. Tis tempting to go up the lakes with my Nikon ...  Hmmmm I may do that ..\n",
      "\n",
      "\n",
      "BLACK FRIDAY Huge Saving Aerial View of a City, Paris Las Vegas, the Las Vegas Strip, Las Vegas,... http://t.co/DCTgeSED\n",
      "\n",
      "\n",
      "@MelmurMel @PBandJenelley_1 @vl_delp_ham_ Jenelle lies,1st she said she was alone &amp;the hosp.now she's saying how weird it was for Keiffer's\n",
      "\n",
      "\n",
      "@MyBeautyisBrown LMFAO his big ass get on my nerves, you going to class tomorrow?\n",
      "\n",
      "\n",
      "Mohamed Morsi, Egypt's Muslim Brotherhood president, instructed the Supreme Council of the Armed Forces Thursday... http://t.co/NnBeUvSt\n",
      "\n",
      "\n",
      "C'mon Avila! You just got tagged out by a guy who looks like the kid Bill Murray was researching in The Royal Tenenbaums! #Tigers\n",
      "\n",
      "\n",
      "@thehuwdavies you think the Boro will beat Swansea? I'm not so sure, December/January is when we implode\n",
      "\n",
      "\n",
      "At the first Grammy Awards, held on 4 May 1959, Domenico Modugno beat out Frank Sinatra and Peggy Lee for the Record of the Year,with Volare\n",
      "\n",
      "\n",
      "@JennetteMcHevan I have studied all day but tomorrow I'm going out with friends! :D Omg Jennette did?!!!! I'm gonna look! &lt;3\n",
      "\n",
      "\n",
      "Good morning Thursday. \"Life is fragile. We're not guaranteed a tomorrow so give it everything you've got.\" - Tim Cook [Do it for Jobs!]\n",
      "\n",
      "\n",
      "#Twitition Mcfly come back to Argentina but this time we want to come to mar del plata!!! http://t.co/DlXY0LCg\n",
      "\n",
      "\n",
      "@Astrochologist anything. I wondered how the aspects btwn my sun/moon faired with my rising. I also have Venus in Sag...yea lol Thank u :)\n",
      "\n",
      "\n",
      "My teachers call themselves givng us candy....wasn't even the GOOD stuff. I might go to Walmart or CVS tomorrow/\n",
      "\n",
      "\n",
      "#Broncos Peyton Manning named AFC Offensive Player of the month. It's his 5th such honor, second to Tom Brady's 6, tied w/ TD.\n",
      "\n",
      "\n",
      "@TooZany is bringing out Kendrick Lamar the 6th of December!?! Get your tickets now!\n",
      "\n",
      "\n",
      "Andre's Wigan Warning - #COYS Official Site Wigan might currently occupy 15th place in the Premier League,... http://t.co/3mo7WIWd\n",
      "\n",
      "\n",
      "@juice005 strange enough, I'm going to see Noel Gallagher in concert tomorrow  night. #alldayb1tch\n",
      "\n",
      "\n",
      "How are they going to act in new york with the subways out? They better clutch it out like Eli Manning 4th quarter\n",
      "\n",
      "\n",
      "Homegrown talent missing on Signing Day: Throughout most of the day on Wednesday, the video scoreboard ... http://t.co/6dZ2hQkD #Raleigh\n",
      "\n",
      "\n",
      "taylor swift is coming with ed sheeran june 29th? most perf news i've heard all night.\n",
      "\n",
      "\n",
      "The great Noel Gallagher is about to hit the stage in St. Paul. Plenty of room here so we're 4th row center. Plenty of room. Pretty fired up\n",
      "\n",
      "\n",
      "@lala_cox There are going to be plenty of ups &amp; downs for Boro,Cardiff,Leicester etc until May.I hope we are still in with a shout for promo\n",
      "\n",
      "\n",
      "@joeeewilliams that sucks, amazon might have some left .. hate that i won't have it until 11am on Tuesday though\n",
      "\n",
      "\n",
      "Playing as the Browns HC K. Fri in #Madden13, XFacalac lost to the Ravens 21-28 on All-Madden #ConnectedCareers\n",
      "\n",
      "\n",
      "@rosanne_89 Nooooooooo rosanne! They've just restocked them and me going overseas Sunday means I can't afford them :'(\n",
      "\n",
      "\n",
      "The pain is far deeper than a Billy cundiff missed field goal. Gotta wake up and forget about it tomorrow. #Orioles #stayhungry\n",
      "\n",
      "\n",
      "@grandvincenzo on November 16 be ready to head to the movies. Breaking Dawn: Part 2. (;\n",
      "\n",
      "\n",
      "after seeing @Iam_FaithReid's tweet about watching The Vow, I think I may need to watch it.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start = 0\n",
    "end = 50\n",
    "for tweet in raw_tweets[start:end]:\n",
    "    print (tweet)\n",
    "    print ('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## <font color='red'>Pre-train the tweets</font>\n",
    "https://nlp.stanford.edu/IR-book/html/htmledition/determining-the-vocabulary-of-terms-1.html\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TT = TweetTokenizer()\n",
    "\n",
    "def emoticondictionary(filename):\n",
    "    \"\"\"\n",
    "    Reads the emoticon file and represents it as dictionary where the emoticon is the key, \n",
    "    and its indication as a value\n",
    "    \"\"\"\n",
    "    emo_scores = {'Positive': 'positive', 'Extremely-Positive': 'positive', \n",
    "                  'Negative': 'negative','Extremely-Negative': 'negative',\n",
    "                  'Neutral': 'neutral'}\n",
    "    emo_score_list = {}\n",
    "    fi = open(filename,\"r\")\n",
    "    l = fi.readline()\n",
    "    while l:\n",
    "        #replace the \"Non-break space\" with the ordinary space \" \"\n",
    "        l = l.replace(\"\\xa0\",\" \")\n",
    "        li = l.split(\" \")\n",
    "        l2 = li[:-1] #removes the polarity of the emoticon ('negative', 'positive')\n",
    "        l2.append(li[len(li) - 1].split(\"\\t\")[0]) #gets the last emoticon attached to the polarity by '\\t'\n",
    "        sentiment=li[len(li) - 1].split(\"\\t\")[1][:-1] #gets only the polarity, and removes '\\n'\n",
    "        score=emo_scores[sentiment]\n",
    "        l2.append(score)\n",
    "        for i in range(0,len(l2)-1):\n",
    "            emo_score_list[l2[i]]=l2[len(l2)-1]\n",
    "        l=fi.readline()\n",
    "    return emo_score_list\n",
    "\n",
    "emoticon_dict = emoticondictionary('./resources/emoticon.txt')\n",
    "\n",
    "\n",
    "# substititue emoticon with its associated sentiment\n",
    "def subsEmoticon(tweet,d):\n",
    "    l = TT.tokenize(tweet)\n",
    "    tweet = [d[i] if i in d.keys() else i for i in l]\n",
    "    return tweet\n",
    "\n",
    "raw_tweets = [subsEmoticon(tweet, emoticon_dict) for tweet in raw_tweets]\n",
    "# print(\":D X3 :|\")\n",
    "# subsEmoticon(\":D X3 :|\", dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Gas', 'by', 'my', 'house', 'hit', '$', '3.39', '!', '!', '!', \"I'm\", 'going', 'to', 'Chapel', 'Hill', 'on', 'Sat', '.', 'positive']\n",
      "\n",
      "\n",
      "['Iranian', 'general', 'says', \"Israel's\", 'Iron', 'Dome', \"can't\", 'deal', 'with', 'their', 'missiles', '(', 'keep', 'talking', 'like', 'that', 'and', 'we', 'may', 'end', 'up', 'finding', 'out', ')']\n",
      "\n",
      "\n",
      "['with', 'J', 'Davlar', '11th', '.', 'Main', 'rivals', 'are', 'team', 'Poland', '.', 'Hopefully', 'we', 'an', 'make', 'it', 'a', 'successful', 'end', 'to', 'a', 'tough', 'week', 'of', 'training', 'tomorrow', '.']\n",
      "\n",
      "\n",
      "['Talking', 'about', \"ACT's\", '&', '&', \"SAT's\", ',', 'deciding', 'where', 'I', 'want', 'to', 'go', 'to', 'college', ',', 'applying', 'to', 'colleges', 'and', 'everything', 'about', 'college', 'stresses', 'me', 'out', '.']\n",
      "\n",
      "\n",
      "['They', 'may', 'have', 'a', 'SuperBowl', 'in', 'Dallas', ',', 'but', 'Dallas', \"ain't\", 'winning', 'a', 'SuperBowl', '.', 'Not', 'with', 'that', 'quarterback', 'and', 'owner', '.', '@S4NYC', '@RasmussenPoll']\n",
      "\n",
      "\n",
      "['Im', 'bringing', 'the', 'monster', 'load', 'of', 'candy', 'tomorrow', ',', 'I', 'just', 'hope', 'it', \"doesn't\", 'get', 'all', 'squiched']\n",
      "\n",
      "\n",
      "['Apple', 'software', ',', 'retail', 'chiefs', 'out', 'in', 'overhaul', ':', 'SAN', 'FRANCISCO', 'Apple', 'Inc', 'CEO', 'Tim', 'Cook', 'on', 'Monday', 'replaced', 'the', 'heads', '...', 'http://t.co/X49ZEOsG']\n",
      "\n",
      "\n",
      "['@oluoch', '@victor_otti', '@kunjand', 'I', 'just', 'watched', 'it', '!', \"Sridevi's\", 'comeback', '...', 'U', 'remember', 'her', 'from', 'the', '90s', '?', '?', 'Sun', 'mornings', 'on', 'NTA', 'positive']\n",
      "\n",
      "\n",
      "['#Livewire', 'Nadal', 'confirmed', 'for', 'Mexican', 'Open', 'in', 'February', ':', 'Rafael', 'Nadal', 'is', 'set', 'to', 'play', 'at', 'the', 'Me', '...', 'http://t.co/zgUXpcnC', '#LiveWireAthletics']\n",
      "\n",
      "\n",
      "['@MsSheLahY', 'I', 'didnt', 'want', 'to', 'just', 'pop', 'up', '...', 'but', 'yep', 'we', 'have', 'chapel', 'hill', 'next', 'wednesday', 'you', 'should', 'come', '..', 'and', 'shes', 'great', 'ill', 'tell', 'her', 'you', 'asked']\n",
      "\n",
      "\n",
      "['@Alyoup005', '@addicted2haley', 'hmmmm', 'November', 'is', 'an', 'odd', 'release', 'date', 'if', 'true', 'but', 'if', 'it', 'becomes', 'big', 'enough', 'maybe', 'she', 'could', 'sing', 'it', 'at', 'Grammys']\n",
      "\n",
      "\n",
      "['#Iran', 'US', 'delisting', 'MKO', 'from', 'global', 'terrorists', 'list', 'in', 'line', 'with', 'Iran', 'campaign', ':', 'Tehran', ',', 'Oct', '30', ',', 'IRNA', '-', '-', 'Secretary', '...', 'http://t.co/9wWvxEbf']\n",
      "\n",
      "\n",
      "['Good', 'Morning', 'Becky', '!', 'Thursday', 'is', 'going', 'to', 'be', 'Fantastic', '!', '@SwedenG', '@DJ4JG', '@Grdina', '@Paverlayer', '@FSBull', '@RevkahJC', '@DicksTrash', '@borderfox116']\n",
      "\n",
      "\n",
      "['Expect', 'light-moderate', 'rains', 'over', 'E', '.', 'Visayas', ';', 'Cebu', ',', 'Bohol', ',', 'Samar', '&', 'Leyte', 'have', '30-70', '%', 'chance', 'of', 'rains', 'tonight', '!', 'Expect', 'fair', 'weather', 'tomorrow', '!', 'positive']\n",
      "\n",
      "\n",
      "['One', 'ticket', 'left', 'for', 'the', '@49ers', 'game', 'tomorrow', '!', \"Don't\", 'miss', 'the', 'rematch', 'of', 'the', 'NFC', 'Championship', 'game', 'against', 'the', 'NY', 'Giants', '!', 'Hit', 'me', 'up', '!']\n",
      "\n",
      "\n",
      "['AFC', 'away', 'fans', 'on', 'Saturday', '.', 'All', 'this', 'stuff', 'about', 'the', \"'\", 'she', 'said', 'no', \"'\", 'chant', '.', \"It's\", 'bollocks', '.', 'When', 'he', 'has', 'the', 'ball', ',', 'just', 'turn', 'your', 'back', 'on', 'him', '.']\n",
      "\n",
      "\n",
      "['Why', 'is', 'it', 'so', 'hard', 'to', 'find', 'the', '@TVGuideMagazine', 'these', 'days', '?', 'Went', 'to', '3', 'stores', 'for', 'the', 'Castle', 'cover', 'issue', '.', 'NONE', '.', 'Will', 'search', 'again', 'tomorrow', '...']\n",
      "\n",
      "\n",
      "['Game', '1', 'of', 'the', 'NLCS', 'and', 'a', 'rematch', 'of', 'the', 'NFC', 'Championship', 'game', 'tomorrow', '.', \"SF's\", 'gonna', 'be', 'cuuuuhraaaaaaaazeeeee']\n",
      "\n",
      "\n",
      "['@TrevorJavier', 'the', 'heat', 'game', 'may', 'cost', 'alot', 'more', '...', 'and', 'plus', 'I', 'would', 'rather', 'see', 'Austin', 'Rivers', 'play']\n",
      "\n",
      "\n",
      "['Never', 'start', 'working', 'on', 'your', 'dreams', 'and', 'goals', 'tomorrow', '...', 'tomorrow', 'never', 'comes', '...', 'if', 'it', 'means', 'anything', 'to', 'U', ',', 'ACT', 'NOW', '!', '#getafterit']\n",
      "\n",
      "\n",
      "['@TheFFAddict', 'I', 'had', 'Vick', 'and', 'Flacco', ',', 'needed', 'an', 'upgrade', '.', 'Vick', 'may', 'get', 'benched', ',', 'Jennings', 'a', 'back', 'up', 'again', 'soon', '.', 'I', 'thought', 'it', 'was', 'a', 'win', 'for', 'me', '.']\n",
      "\n",
      "\n",
      "['Looks', 'like', 'Andy', 'the', 'Android', 'may', 'have', 'had', 'a', 'little', 'too', 'much', 'fun', 'yesterday', '.', 'http://t.co/7ZDEfzEC']\n",
      "\n",
      "\n",
      "['@APGPhoto', 'oooh', 'nice', '..', 'Tis', 'tempting', 'to', 'go', 'up', 'the', 'lakes', 'with', 'my', 'Nikon', '...', 'Hmmmm', 'I', 'may', 'do', 'that', '..']\n",
      "\n",
      "\n",
      "['BLACK', 'FRIDAY', 'Huge', 'Saving', 'Aerial', 'View', 'of', 'a', 'City', ',', 'Paris', 'Las', 'Vegas', ',', 'the', 'Las', 'Vegas', 'Strip', ',', 'Las', 'Vegas', ',', '...', 'http://t.co/DCTgeSED']\n",
      "\n",
      "\n",
      "['@MelmurMel', '@PBandJenelley_1', '@vl_delp_ham_', 'Jenelle', 'lies', ',', '1st', 'she', 'said', 'she', 'was', 'alone', '&', 'the', 'hosp.now', \"she's\", 'saying', 'how', 'weird', 'it', 'was', 'for', \"Keiffer's\"]\n",
      "\n",
      "\n",
      "['@MyBeautyisBrown', 'LMFAO', 'his', 'big', 'ass', 'get', 'on', 'my', 'nerves', ',', 'you', 'going', 'to', 'class', 'tomorrow', '?']\n",
      "\n",
      "\n",
      "['Mohamed', 'Morsi', ',', \"Egypt's\", 'Muslim', 'Brotherhood', 'president', ',', 'instructed', 'the', 'Supreme', 'Council', 'of', 'the', 'Armed', 'Forces', 'Thursday', '...', 'http://t.co/NnBeUvSt']\n",
      "\n",
      "\n",
      "[\"C'mon\", 'Avila', '!', 'You', 'just', 'got', 'tagged', 'out', 'by', 'a', 'guy', 'who', 'looks', 'like', 'the', 'kid', 'Bill', 'Murray', 'was', 'researching', 'in', 'The', 'Royal', 'Tenenbaums', '!', '#Tigers']\n",
      "\n",
      "\n",
      "['@thehuwdavies', 'you', 'think', 'the', 'Boro', 'will', 'beat', 'Swansea', '?', \"I'm\", 'not', 'so', 'sure', ',', 'December', '/', 'January', 'is', 'when', 'we', 'implode']\n",
      "\n",
      "\n",
      "['At', 'the', 'first', 'Grammy', 'Awards', ',', 'held', 'on', '4', 'May', '1959', ',', 'Domenico', 'Modugno', 'beat', 'out', 'Frank', 'Sinatra', 'and', 'Peggy', 'Lee', 'for', 'the', 'Record', 'of', 'the', 'Year', ',', 'with', 'Volare']\n",
      "\n",
      "\n",
      "['@JennetteMcHevan', 'I', 'have', 'studied', 'all', 'day', 'but', 'tomorrow', \"I'm\", 'going', 'out', 'with', 'friends', '!', 'positive', 'Omg', 'Jennette', 'did', '?', '!', '!', '!', \"I'm\", 'gonna', 'look', '!', 'positive']\n",
      "\n",
      "\n",
      "['Good', 'morning', 'Thursday', '.', '\"', 'Life', 'is', 'fragile', '.', \"We're\", 'not', 'guaranteed', 'a', 'tomorrow', 'so', 'give', 'it', 'everything', \"you've\", 'got', '.', '\"', '-', 'Tim', 'Cook', '[', 'Do', 'it', 'for', 'Jobs', '!', ']']\n",
      "\n",
      "\n",
      "['#Twitition', 'Mcfly', 'come', 'back', 'to', 'Argentina', 'but', 'this', 'time', 'we', 'want', 'to', 'come', 'to', 'mar', 'del', 'plata', '!', '!', '!', 'http://t.co/DlXY0LCg']\n",
      "\n",
      "\n",
      "['@Astrochologist', 'anything', '.', 'I', 'wondered', 'how', 'the', 'aspects', 'btwn', 'my', 'sun', '/', 'moon', 'faired', 'with', 'my', 'rising', '.', 'I', 'also', 'have', 'Venus', 'in', 'Sag', '...', 'yea', 'lol', 'Thank', 'u', 'positive']\n",
      "\n",
      "\n",
      "['My', 'teachers', 'call', 'themselves', 'givng', 'us', 'candy', '...', \"wasn't\", 'even', 'the', 'GOOD', 'stuff', '.', 'I', 'might', 'go', 'to', 'Walmart', 'or', 'CVS', 'tomorrow', '/']\n",
      "\n",
      "\n",
      "['#Broncos', 'Peyton', 'Manning', 'named', 'AFC', 'Offensive', 'Player', 'of', 'the', 'month', '.', \"It's\", 'his', '5th', 'such', 'honor', ',', 'second', 'to', 'Tom', \"Brady's\", '6', ',', 'tied', 'w', '/', 'TD', '.']\n",
      "\n",
      "\n",
      "['@TooZany', 'is', 'bringing', 'out', 'Kendrick', 'Lamar', 'the', '6th', 'of', 'December', '!', '?', '!', 'Get', 'your', 'tickets', 'now', '!']\n",
      "\n",
      "\n",
      "[\"Andre's\", 'Wigan', 'Warning', '-', '#COYS', 'Official', 'Site', 'Wigan', 'might', 'currently', 'occupy', '15th', 'place', 'in', 'the', 'Premier', 'League', ',', '...', 'http://t.co/3mo7WIWd']\n",
      "\n",
      "\n",
      "['@juice005', 'strange', 'enough', ',', \"I'm\", 'going', 'to', 'see', 'Noel', 'Gallagher', 'in', 'concert', 'tomorrow', 'night', '.', '#alldayb1tch']\n",
      "\n",
      "\n",
      "['How', 'are', 'they', 'going', 'to', 'act', 'in', 'new', 'york', 'with', 'the', 'subways', 'out', '?', 'They', 'better', 'clutch', 'it', 'out', 'like', 'Eli', 'Manning', '4th', 'quarter']\n",
      "\n",
      "\n",
      "['Homegrown', 'talent', 'missing', 'on', 'Signing', 'Day', ':', 'Throughout', 'most', 'of', 'the', 'day', 'on', 'Wednesday', ',', 'the', 'video', 'scoreboard', '...', 'http://t.co/6dZ2hQkD', '#Raleigh']\n",
      "\n",
      "\n",
      "['taylor', 'swift', 'is', 'coming', 'with', 'ed', 'sheeran', 'june', '29th', '?', 'most', 'perf', 'news', \"i've\", 'heard', 'all', 'night', '.']\n",
      "\n",
      "\n",
      "['The', 'great', 'Noel', 'Gallagher', 'is', 'about', 'to', 'hit', 'the', 'stage', 'in', 'St', '.', 'Paul', '.', 'Plenty', 'of', 'room', 'here', 'so', \"we're\", '4th', 'row', 'center', '.', 'Plenty', 'of', 'room', '.', 'Pretty', 'fired', 'up']\n",
      "\n",
      "\n",
      "['@lala_cox', 'There', 'are', 'going', 'to', 'be', 'plenty', 'of', 'ups', '&', 'downs', 'for', 'Boro', ',', 'Cardiff', ',', 'Leicester', 'etc', 'until', 'May', '.', 'I', 'hope', 'we', 'are', 'still', 'in', 'with', 'a', 'shout', 'for', 'promo']\n",
      "\n",
      "\n",
      "['@joeeewilliams', 'that', 'sucks', ',', 'amazon', 'might', 'have', 'some', 'left', '..', 'hate', 'that', 'i', \"won't\", 'have', 'it', 'until', '11am', 'on', 'Tuesday', 'though']\n",
      "\n",
      "\n",
      "['Playing', 'as', 'the', 'Browns', 'HC', 'K', '.', 'Fri', 'in', '#Madden13', ',', 'XFacalac', 'lost', 'to', 'the', 'Ravens', '21-28', 'on', 'All-Madden', '#ConnectedCareers']\n",
      "\n",
      "\n",
      "['@rosanne_89', 'Nooooooooo', 'rosanne', '!', \"They've\", 'just', 'restocked', 'them', 'and', 'me', 'going', 'overseas', 'Sunday', 'means', 'I', \"can't\", 'afford', 'them', \":'(\"]\n",
      "\n",
      "\n",
      "['The', 'pain', 'is', 'far', 'deeper', 'than', 'a', 'Billy', 'cundiff', 'missed', 'field', 'goal', '.', 'Gotta', 'wake', 'up', 'and', 'forget', 'about', 'it', 'tomorrow', '.', '#Orioles', '#stayhungry']\n",
      "\n",
      "\n",
      "['@grandvincenzo', 'on', 'November', '16', 'be', 'ready', 'to', 'head', 'to', 'the', 'movies', '.', 'Breaking', 'Dawn', ':', 'Part', '2', '.', '(;']\n",
      "\n",
      "\n",
      "['after', 'seeing', '@Iam_FaithReid', \"'\", 's', 'tweet', 'about', 'watching', 'The', 'Vow', ',', 'I', 'think', 'I', 'may', 'need', 'to', 'watch', 'it', '.']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start = 0\n",
    "end = 50\n",
    "for tweet in raw_tweets[start:end]:\n",
    "    print (tweet)\n",
    "    print ('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def correct_case(prev_word, words):\n",
    "    \"\"\"\n",
    "    Gets the proper 'prev_word' case and preserves it with the 'word'.\n",
    "    \"\"\"\n",
    "    def case_of(text):\n",
    "        \"\"\"\n",
    "        Return the case-function appropriate for the given word. \n",
    "        The returned cast is [upper, lower, title, or just str].\n",
    "        \"\"\"\n",
    "        if len(text) == 1:\n",
    "            return str.title if text.isupper() else str.lower\n",
    "        return (str.upper if text.isupper() else\n",
    "                str.lower if text.islower() else\n",
    "                str.title if text.istitle() else\n",
    "                str)\n",
    "    assert type(words) == list\n",
    "    words[0] = case_of(prev_word)(words[0])\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3488 words were replaced\n"
     ]
    }
   ],
   "source": [
    "def loadSlangs(filename):\n",
    "    \"\"\"\n",
    "    This function reads the file that contains the slangs, and put them in a dictionary such that\n",
    "    the key is the \"slang\" and the value is the acronym.\n",
    "    slangs[\"i've\"] = ['i',  'have']\n",
    "    slang['12be'] = ['want', 'to', 'be']\n",
    "    ...\n",
    "    CAUTION: the keys and values are in lower-case\n",
    "    \"\"\"\n",
    "    slangs={}\n",
    "    fi=open(filename,'r')\n",
    "    line=fi.readline()\n",
    "    while line:\n",
    "        l=line.split(r',%,')\n",
    "        if len(l) == 2:\n",
    "            slangs[l[0].lower()]=l[1][:-1].lower().split()\n",
    "        line=fi.readline()\n",
    "    fi.close()\n",
    "    return slangs\n",
    "\n",
    "\n",
    "replaced = 0\n",
    "def replaceSlangs(tweet, slangs):\n",
    "    \"\"\"\n",
    "    This function is used to replace the slang in the original tweets and replace them with the acronym.\n",
    "    And it's also returns the the tweet in lower-case letters\n",
    "    \"\"\"\n",
    "    global replaced\n",
    "    result = []\n",
    "    for w in tweet:\n",
    "        if w.lower() in slangs.keys():\n",
    "            replaced += 1\n",
    "            result.extend(correct_case(w, slangs[w.lower()]))\n",
    "        else:\n",
    "            result.append(w)\n",
    "    return result\n",
    "\n",
    "\n",
    "slangs = loadSlangs('./resources/internetSlangs.txt')\n",
    "raw_tweets = [replaceSlangs(tweet, slangs) for tweet in raw_tweets]\n",
    "print (str(replaced)+\" words were replaced\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Gas', 'by', 'my', 'house', 'hit', '$', '3.39', '!', '!', '!', \"I'm\", 'going', 'to', 'Chapel', 'Hill', 'on', 'Sat', '.', 'positive']\n",
      "\n",
      "\n",
      "['Iranian', 'general', 'says', \"Israel's\", 'Iron', 'Dome', \"can't\", 'deal', 'with', 'their', 'missiles', '(', 'keep', 'talking', 'like', 'that', 'and', 'we', 'may', 'end', 'up', 'finding', 'out', ')']\n",
      "\n",
      "\n",
      "['with', 'J', 'Davlar', '11th', '.', 'Main', 'rivals', 'are', 'team', 'Poland', '.', 'Hopefully', 'we', 'an', 'make', 'it', 'a', 'successful', 'end', 'to', 'a', 'tough', 'week', 'of', 'training', 'tomorrow', '.']\n",
      "\n",
      "\n",
      "['Talking', 'about', \"ACT's\", '&', '&', \"SAT's\", ',', 'deciding', 'where', 'I', 'want', 'to', 'go', 'to', 'college', ',', 'applying', 'to', 'colleges', 'and', 'everything', 'about', 'college', 'stresses', 'me', 'out', '.']\n",
      "\n",
      "\n",
      "['They', 'may', 'have', 'a', 'SuperBowl', 'in', 'Dallas', ',', 'but', 'Dallas', 'am', 'not', 'winning', 'a', 'SuperBowl', '.', 'Not', 'with', 'that', 'quarterback', 'and', 'owner', '.', '@S4NYC', '@RasmussenPoll']\n",
      "\n",
      "\n",
      "['Instant', 'message', 'bringing', 'the', 'monster', 'load', 'of', 'candy', 'tomorrow', ',', 'I', 'just', 'hope', 'it', \"doesn't\", 'get', 'all', 'squiched']\n",
      "\n",
      "\n",
      "['Apple', 'software', ',', 'retail', 'chiefs', 'out', 'in', 'overhaul', ':', 'SAN', 'FRANCISCO', 'Apple', 'Inc', 'CEO', 'Tim', 'Cook', 'on', 'Monday', 'replaced', 'the', 'heads', '...', 'http://t.co/X49ZEOsG']\n",
      "\n",
      "\n",
      "['@oluoch', '@victor_otti', '@kunjand', 'I', 'just', 'watched', 'it', '!', \"Sridevi's\", 'comeback', '...', 'You', 'remember', 'her', 'from', 'the', '90s', '?', '?', 'Sun', 'mornings', 'on', 'NTA', 'positive']\n",
      "\n",
      "\n",
      "['#Livewire', 'Nadal', 'confirmed', 'for', 'Mexican', 'Open', 'in', 'February', ':', 'Rafael', 'Nadal', 'is', 'set', 'to', 'play', 'at', 'the', 'Me', '...', 'http://t.co/zgUXpcnC', '#LiveWireAthletics']\n",
      "\n",
      "\n",
      "['@MsSheLahY', 'I', 'didnt', 'want', 'to', 'just', 'pop', 'up', '...', 'but', 'yep', 'we', 'have', 'chapel', 'hill', 'next', 'wednesday', 'you', 'should', 'come', '..', 'and', 'shes', 'great', 'ill', 'tell', 'her', 'you', 'asked']\n",
      "\n",
      "\n",
      "['@Alyoup005', '@addicted2haley', 'hmmmm', 'November', 'is', 'an', 'odd', 'release', 'date', 'if', 'true', 'but', 'if', 'it', 'becomes', 'big', 'enough', 'maybe', 'she', 'could', 'sing', 'it', 'at', 'Grammys']\n",
      "\n",
      "\n",
      "['#Iran', 'US', 'delisting', 'MKO', 'from', 'global', 'terrorists', 'list', 'in', 'line', 'with', 'Iran', 'campaign', ':', 'Tehran', ',', 'Oct', '30', ',', 'IRNA', '-', '-', 'Secretary', '...', 'http://t.co/9wWvxEbf']\n",
      "\n",
      "\n",
      "['Good', 'Morning', 'Becky', '!', 'Thursday', 'is', 'going', 'to', 'be', 'Fantastic', '!', '@SwedenG', '@DJ4JG', '@Grdina', '@Paverlayer', '@FSBull', '@RevkahJC', '@DicksTrash', '@borderfox116']\n",
      "\n",
      "\n",
      "['Expect', 'light-moderate', 'rains', 'over', 'E', '.', 'Visayas', ';', 'Cebu', ',', 'Bohol', ',', 'Samar', '&', 'Leyte', 'have', '30-70', '%', 'chance', 'of', 'rains', 'tonight', '!', 'Expect', 'fair', 'weather', 'tomorrow', '!', 'positive']\n",
      "\n",
      "\n",
      "['One', 'ticket', 'left', 'for', 'the', '@49ers', 'game', 'tomorrow', '!', \"Don't\", 'miss', 'the', 'rematch', 'of', 'the', 'NO', 'fucking', 'clue', 'Championship', 'game', 'against', 'the', 'NY', 'Giants', '!', 'Hit', 'me', 'up', '!']\n",
      "\n",
      "\n",
      "['AWAY', 'from', 'computer', 'away', 'fans', 'on', 'Saturday', '.', 'All', 'this', 'stuff', 'about', 'the', \"'\", 'she', 'said', 'no', \"'\", 'chant', '.', \"It's\", 'bollocks', '.', 'When', 'he', 'has', 'the', 'ball', ',', 'just', 'turn', 'your', 'back', 'on', 'him', '.']\n",
      "\n",
      "\n",
      "['Why', 'is', 'it', 'so', 'hard', 'to', 'find', 'the', '@TVGuideMagazine', 'these', 'days', '?', 'Went', 'to', '3', 'stores', 'for', 'the', 'Castle', 'cover', 'issue', '.', 'NONE', '.', 'Will', 'search', 'again', 'tomorrow', '...']\n",
      "\n",
      "\n",
      "['Game', '1', 'of', 'the', 'NLCS', 'and', 'a', 'rematch', 'of', 'the', 'NO', 'fucking', 'clue', 'Championship', 'game', 'tomorrow', '.', \"SF's\", 'going', 'to', 'be', 'cuuuuhraaaaaaaazeeeee']\n",
      "\n",
      "\n",
      "['@TrevorJavier', 'the', 'heat', 'game', 'may', 'cost', 'a', 'lot', 'more', '...', 'and', 'plus', 'I', 'would', 'rather', 'see', 'Austin', 'Rivers', 'play']\n",
      "\n",
      "\n",
      "['Never', 'start', 'working', 'on', 'your', 'dreams', 'and', 'goals', 'tomorrow', '...', 'tomorrow', 'never', 'comes', '...', 'if', 'it', 'means', 'anything', 'to', 'You', ',', 'ACT', 'NOW', '!', '#getafterit']\n",
      "\n",
      "\n",
      "['@TheFFAddict', 'I', 'had', 'Vick', 'and', 'Flacco', ',', 'needed', 'an', 'upgrade', '.', 'Vick', 'may', 'get', 'benched', ',', 'Jennings', 'a', 'back', 'up', 'again', 'soon', '.', 'I', 'thought', 'it', 'was', 'a', 'win', 'for', 'me', '.']\n",
      "\n",
      "\n",
      "['Looks', 'like', 'Andy', 'the', 'Android', 'may', 'have', 'had', 'a', 'little', 'too', 'much', 'fun', 'yesterday', '.', 'http://t.co/7ZDEfzEC']\n",
      "\n",
      "\n",
      "['@APGPhoto', 'oooh', 'nice', '..', 'Is', 'tempting', 'to', 'go', 'up', 'the', 'lakes', 'with', 'my', 'Nikon', '...', 'Hmmmm', 'I', 'may', 'do', 'that', '..']\n",
      "\n",
      "\n",
      "['BLACK', 'FRIDAY', 'Huge', 'Saving', 'Aerial', 'View', 'of', 'a', 'City', ',', 'Paris', 'Las', 'Vegas', ',', 'the', 'Las', 'Vegas', 'Strip', ',', 'Las', 'Vegas', ',', '...', 'http://t.co/DCTgeSED']\n",
      "\n",
      "\n",
      "['@MelmurMel', '@PBandJenelley_1', '@vl_delp_ham_', 'Jenelle', 'lies', ',', '1st', 'she', 'said', 'she', 'was', 'alone', '&', 'the', 'hosp.now', \"she's\", 'saying', 'how', 'weird', 'it', 'was', 'for', \"Keiffer's\"]\n",
      "\n",
      "\n",
      "['@MyBeautyisBrown', 'LAUGHING', 'my', 'fucking', 'ass', 'off', 'his', 'big', 'ass', 'get', 'on', 'my', 'nerves', ',', 'you', 'going', 'to', 'class', 'tomorrow', '?']\n",
      "\n",
      "\n",
      "['Mohamed', 'Morsi', ',', \"Egypt's\", 'Muslim', 'Brotherhood', 'president', ',', 'instructed', 'the', 'Supreme', 'Council', 'of', 'the', 'Armed', 'Forces', 'Thursday', '...', 'http://t.co/NnBeUvSt']\n",
      "\n",
      "\n",
      "[\"C'mon\", 'Avila', '!', 'You', 'just', 'got', 'tagged', 'out', 'by', 'a', 'guy', 'who', 'looks', 'like', 'the', 'kid', 'Bill', 'Murray', 'was', 'researching', 'in', 'The', 'Royal', 'Tenenbaums', '!', '#Tigers']\n",
      "\n",
      "\n",
      "['@thehuwdavies', 'you', 'think', 'the', 'Boro', 'will', 'beat', 'Swansea', '?', \"I'm\", 'not', 'so', 'sure', ',', 'December', '/', 'January', 'is', 'when', 'we', 'implode']\n",
      "\n",
      "\n",
      "['At', 'the', 'first', 'Grammy', 'Awards', ',', 'held', 'on', 'for', 'May', '1959', ',', 'Domenico', 'Modugno', 'beat', 'out', 'Frank', 'Sinatra', 'and', 'Peggy', 'Lee', 'for', 'the', 'Record', 'of', 'the', 'Year', ',', 'with', 'Volare']\n",
      "\n",
      "\n",
      "['@JennetteMcHevan', 'I', 'have', 'studied', 'all', 'day', 'but', 'tomorrow', \"I'm\", 'going', 'out', 'with', 'friends', '!', 'positive', 'Oh', 'my', 'god', 'Jennette', 'did', '?', '!', '!', '!', \"I'm\", 'going', 'to', 'look', '!', 'positive']\n",
      "\n",
      "\n",
      "['Good', 'morning', 'Thursday', '.', '\"', 'Life', 'is', 'fragile', '.', \"We're\", 'not', 'guaranteed', 'a', 'tomorrow', 'so', 'give', 'it', 'everything', \"you've\", 'got', '.', '\"', '-', 'Tim', 'Cook', '[', 'Do', 'it', 'for', 'Jobs', '!', ']']\n",
      "\n",
      "\n",
      "['#Twitition', 'Mcfly', 'come', 'back', 'to', 'Argentina', 'but', 'this', 'time', 'we', 'want', 'to', 'come', 'to', 'mar', 'del', 'plata', '!', '!', '!', 'http://t.co/DlXY0LCg']\n",
      "\n",
      "\n",
      "['@Astrochologist', 'anything', '.', 'I', 'wondered', 'how', 'the', 'aspects', 'between', 'my', 'sun', '/', 'moon', 'faired', 'with', 'my', 'rising', '.', 'I', 'also', 'have', 'Venus', 'in', 'Sag', '...', 'yeah', 'laughing', 'out', 'loud', 'Thank', 'you', 'positive']\n",
      "\n",
      "\n",
      "['My', 'teachers', 'call', 'themselves', 'givng', 'us', 'candy', '...', \"wasn't\", 'even', 'the', 'GOOD', 'stuff', '.', 'I', 'might', 'go', 'to', 'Walmart', 'or', 'CVS', 'tomorrow', '/']\n",
      "\n",
      "\n",
      "['#Broncos', 'Peyton', 'Manning', 'named', 'AWAY', 'from', 'computer', 'Offensive', 'Player', 'of', 'the', 'month', '.', \"It's\", 'his', '5th', 'such', 'honor', ',', 'second', 'to', 'Tom', \"Brady's\", '6', ',', 'tied', 'w', '/', 'TD', '.']\n",
      "\n",
      "\n",
      "['@TooZany', 'is', 'bringing', 'out', 'Kendrick', 'Lamar', 'the', '6th', 'of', 'December', '!', '?', '!', 'Get', 'your', 'tickets', 'now', '!']\n",
      "\n",
      "\n",
      "[\"Andre's\", 'Wigan', 'Warning', '-', '#COYS', 'Official', 'Site', 'Wigan', 'might', 'currently', 'occupy', '15th', 'place', 'in', 'the', 'Premier', 'League', ',', '...', 'http://t.co/3mo7WIWd']\n",
      "\n",
      "\n",
      "['@juice005', 'strange', 'enough', ',', \"I'm\", 'going', 'to', 'see', 'Noel', 'Gallagher', 'in', 'concert', 'tomorrow', 'night', '.', '#alldayb1tch']\n",
      "\n",
      "\n",
      "['How', 'are', 'they', 'going', 'to', 'act', 'in', 'new', 'york', 'with', 'the', 'subways', 'out', '?', 'They', 'better', 'clutch', 'it', 'out', 'like', 'Eli', 'Manning', '4th', 'quarter']\n",
      "\n",
      "\n",
      "['Homegrown', 'talent', 'missing', 'on', 'Signing', 'Day', ':', 'Throughout', 'most', 'of', 'the', 'day', 'on', 'Wednesday', ',', 'the', 'video', 'scoreboard', '...', 'http://t.co/6dZ2hQkD', '#Raleigh']\n",
      "\n",
      "\n",
      "['taylor', 'swift', 'is', 'coming', 'with', 'ed', 'sheeran', 'june', '29th', '?', 'most', 'perf', 'news', \"i've\", 'heard', 'all', 'night', '.']\n",
      "\n",
      "\n",
      "['The', 'great', 'Noel', 'Gallagher', 'is', 'about', 'to', 'hit', 'the', 'stage', 'in', 'Stop', 'that', '.', 'Paul', '.', 'Plenty', 'of', 'room', 'here', 'so', \"we're\", '4th', 'row', 'center', '.', 'Plenty', 'of', 'room', '.', 'Pretty', 'fired', 'up']\n",
      "\n",
      "\n",
      "['@lala_cox', 'There', 'are', 'going', 'to', 'be', 'plenty', 'of', 'ups', '&', 'downs', 'for', 'Boro', ',', 'Cardiff', ',', 'Leicester', 'etc', 'until', 'May', '.', 'I', 'hope', 'we', 'are', 'still', 'in', 'with', 'a', 'shout', 'for', 'promo']\n",
      "\n",
      "\n",
      "['@joeeewilliams', 'that', 'sucks', ',', 'amazon', 'might', 'have', 'some', 'left', '..', 'hate', 'that', 'i', \"won't\", 'have', 'it', 'until', '11am', 'on', 'Tuesday', 'though']\n",
      "\n",
      "\n",
      "['Playing', 'as', 'the', 'Browns', 'HOW', 'come', 'Ok', '.', 'Fri', 'in', '#Madden13', ',', 'XFacalac', 'lost', 'to', 'the', 'Ravens', '21-28', 'on', 'All-Madden', '#ConnectedCareers']\n",
      "\n",
      "\n",
      "['@rosanne_89', 'Nooooooooo', 'rosanne', '!', \"They've\", 'just', 'restocked', 'them', 'and', 'me', 'going', 'overseas', 'Sunday', 'means', 'I', \"can't\", 'afford', 'them', \":'(\"]\n",
      "\n",
      "\n",
      "['The', 'pain', 'is', 'far', 'deeper', 'than', 'a', 'Billy', 'cundiff', 'missed', 'field', 'goal', '.', 'Got', 'to', 'wake', 'up', 'and', 'forget', 'about', 'it', 'tomorrow', '.', '#Orioles', '#stayhungry']\n",
      "\n",
      "\n",
      "['@grandvincenzo', 'on', 'November', '16', 'be', 'ready', 'to', 'head', 'to', 'the', 'movies', '.', 'Breaking', 'Dawn', ':', 'Part', 'too', '.', '(;']\n",
      "\n",
      "\n",
      "['after', 'seeing', '@Iam_FaithReid', \"'\", 's', 'tweet', 'about', 'watching', 'The', 'Vow', ',', 'I', 'think', 'I', 'may', 'need', 'to', 'watch', 'it', '.']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start = 0\n",
    "end = 50\n",
    "for tweet in raw_tweets[start:end]:\n",
    "    print (tweet)\n",
    "    print ('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1682 words were replaced\n"
     ]
    }
   ],
   "source": [
    "def load_apostrophe_words(filename):\n",
    "    \"\"\"\n",
    "    This function reads the file that contains all words that have apostrophe, and put them in a dictionary \n",
    "    such that the key is the \"word containing apostrophe\" and the value is the \"the word without apostrophe\".\n",
    "    slangs['i've'] = 'i have'\n",
    "    slang['I'm] = 'I am'\n",
    "    ...\n",
    "    CAUTION: the keys and values are lower-case letters\n",
    "    \"\"\"\n",
    "    apo={}\n",
    "    fi=open(filename,'r')\n",
    "    line=fi.readline()\n",
    "    while line:\n",
    "        l=line.split(r',%,')\n",
    "        if len(l) == 2:\n",
    "            apo[l[0].lower()] = l[1][:-1].lower().split()\n",
    "        line=fi.readline()\n",
    "    fi.close()\n",
    "    return apo\n",
    "\n",
    "replaced = 0\n",
    "def replace_apostrophe(tweet,apos):\n",
    "    global replaced\n",
    "    result = []\n",
    "    for w in tweet:\n",
    "        if w.lower() in apos.keys():\n",
    "            result.extend(correct_case(w, apos[w.lower()]))\n",
    "            replaced += 1\n",
    "        else:\n",
    "            result.append(w)\n",
    "    return result\n",
    "\n",
    "apos = load_apostrophe_words('./resources/apostrophe_words.txt')\n",
    "raw_tweets = [replace_apostrophe(tweet, apos) for tweet in raw_tweets]\n",
    "print (str(replaced)+\" words were replaced\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Gas', 'by', 'my', 'house', 'hit', '$', '3.39', '!', '!', '!', 'i', 'am', 'going', 'to', 'Chapel', 'Hill', 'on', 'Sat', '.', 'positive']\n",
      "\n",
      "\n",
      "['Iranian', 'general', 'says', \"Israel's\", 'Iron', 'Dome', 'can', 'not', 'deal', 'with', 'their', 'missiles', '(', 'keep', 'talking', 'like', 'that', 'and', 'we', 'may', 'end', 'up', 'finding', 'out', ')']\n",
      "\n",
      "\n",
      "['with', 'J', 'Davlar', '11th', '.', 'Main', 'rivals', 'are', 'team', 'Poland', '.', 'Hopefully', 'we', 'an', 'make', 'it', 'a', 'successful', 'end', 'to', 'a', 'tough', 'week', 'of', 'training', 'tomorrow', '.']\n",
      "\n",
      "\n",
      "['Talking', 'about', \"ACT's\", '&', '&', \"SAT's\", ',', 'deciding', 'where', 'I', 'want', 'to', 'go', 'to', 'college', ',', 'applying', 'to', 'colleges', 'and', 'everything', 'about', 'college', 'stresses', 'me', 'out', '.']\n",
      "\n",
      "\n",
      "['They', 'may', 'have', 'a', 'SuperBowl', 'in', 'Dallas', ',', 'but', 'Dallas', 'am', 'not', 'winning', 'a', 'SuperBowl', '.', 'Not', 'with', 'that', 'quarterback', 'and', 'owner', '.', '@S4NYC', '@RasmussenPoll']\n",
      "\n",
      "\n",
      "['Instant', 'message', 'bringing', 'the', 'monster', 'load', 'of', 'candy', 'tomorrow', ',', 'I', 'just', 'hope', 'it', 'does', 'not', 'get', 'all', 'squiched']\n",
      "\n",
      "\n",
      "['Apple', 'software', ',', 'retail', 'chiefs', 'out', 'in', 'overhaul', ':', 'SAN', 'FRANCISCO', 'Apple', 'Inc', 'CEO', 'Tim', 'Cook', 'on', 'Monday', 'replaced', 'the', 'heads', '...', 'http://t.co/X49ZEOsG']\n",
      "\n",
      "\n",
      "['@oluoch', '@victor_otti', '@kunjand', 'I', 'just', 'watched', 'it', '!', \"Sridevi's\", 'comeback', '...', 'You', 'remember', 'her', 'from', 'the', '90s', '?', '?', 'Sun', 'mornings', 'on', 'NTA', 'positive']\n",
      "\n",
      "\n",
      "['#Livewire', 'Nadal', 'confirmed', 'for', 'Mexican', 'Open', 'in', 'February', ':', 'Rafael', 'Nadal', 'is', 'set', 'to', 'play', 'at', 'the', 'Me', '...', 'http://t.co/zgUXpcnC', '#LiveWireAthletics']\n",
      "\n",
      "\n",
      "['@MsSheLahY', 'I', 'didnt', 'want', 'to', 'just', 'pop', 'up', '...', 'but', 'yep', 'we', 'have', 'chapel', 'hill', 'next', 'wednesday', 'you', 'should', 'come', '..', 'and', 'shes', 'great', 'ill', 'tell', 'her', 'you', 'asked']\n",
      "\n",
      "\n",
      "['@Alyoup005', '@addicted2haley', 'hmmmm', 'November', 'is', 'an', 'odd', 'release', 'date', 'if', 'true', 'but', 'if', 'it', 'becomes', 'big', 'enough', 'maybe', 'she', 'could', 'sing', 'it', 'at', 'Grammys']\n",
      "\n",
      "\n",
      "['#Iran', 'US', 'delisting', 'MKO', 'from', 'global', 'terrorists', 'list', 'in', 'line', 'with', 'Iran', 'campaign', ':', 'Tehran', ',', 'Oct', '30', ',', 'IRNA', '-', '-', 'Secretary', '...', 'http://t.co/9wWvxEbf']\n",
      "\n",
      "\n",
      "['Good', 'Morning', 'Becky', '!', 'Thursday', 'is', 'going', 'to', 'be', 'Fantastic', '!', '@SwedenG', '@DJ4JG', '@Grdina', '@Paverlayer', '@FSBull', '@RevkahJC', '@DicksTrash', '@borderfox116']\n",
      "\n",
      "\n",
      "['Expect', 'light-moderate', 'rains', 'over', 'E', '.', 'Visayas', ';', 'Cebu', ',', 'Bohol', ',', 'Samar', '&', 'Leyte', 'have', '30-70', '%', 'chance', 'of', 'rains', 'tonight', '!', 'Expect', 'fair', 'weather', 'tomorrow', '!', 'positive']\n",
      "\n",
      "\n",
      "['One', 'ticket', 'left', 'for', 'the', '@49ers', 'game', 'tomorrow', '!', 'do', 'not', 'miss', 'the', 'rematch', 'of', 'the', 'NO', 'fucking', 'clue', 'Championship', 'game', 'against', 'the', 'NY', 'Giants', '!', 'Hit', 'me', 'up', '!']\n",
      "\n",
      "\n",
      "['AWAY', 'from', 'computer', 'away', 'fans', 'on', 'Saturday', '.', 'All', 'this', 'stuff', 'about', 'the', \"'\", 'she', 'said', 'no', \"'\", 'chant', '.', 'it', 'is', 'bollocks', '.', 'When', 'he', 'has', 'the', 'ball', ',', 'just', 'turn', 'your', 'back', 'on', 'him', '.']\n",
      "\n",
      "\n",
      "['Why', 'is', 'it', 'so', 'hard', 'to', 'find', 'the', '@TVGuideMagazine', 'these', 'days', '?', 'Went', 'to', '3', 'stores', 'for', 'the', 'Castle', 'cover', 'issue', '.', 'NONE', '.', 'Will', 'search', 'again', 'tomorrow', '...']\n",
      "\n",
      "\n",
      "['Game', '1', 'of', 'the', 'NLCS', 'and', 'a', 'rematch', 'of', 'the', 'NO', 'fucking', 'clue', 'Championship', 'game', 'tomorrow', '.', \"SF's\", 'going', 'to', 'be', 'cuuuuhraaaaaaaazeeeee']\n",
      "\n",
      "\n",
      "['@TrevorJavier', 'the', 'heat', 'game', 'may', 'cost', 'a', 'lot', 'more', '...', 'and', 'plus', 'I', 'would', 'rather', 'see', 'Austin', 'Rivers', 'play']\n",
      "\n",
      "\n",
      "['Never', 'start', 'working', 'on', 'your', 'dreams', 'and', 'goals', 'tomorrow', '...', 'tomorrow', 'never', 'comes', '...', 'if', 'it', 'means', 'anything', 'to', 'You', ',', 'ACT', 'NOW', '!', '#getafterit']\n",
      "\n",
      "\n",
      "['@TheFFAddict', 'I', 'had', 'Vick', 'and', 'Flacco', ',', 'needed', 'an', 'upgrade', '.', 'Vick', 'may', 'get', 'benched', ',', 'Jennings', 'a', 'back', 'up', 'again', 'soon', '.', 'I', 'thought', 'it', 'was', 'a', 'win', 'for', 'me', '.']\n",
      "\n",
      "\n",
      "['Looks', 'like', 'Andy', 'the', 'Android', 'may', 'have', 'had', 'a', 'little', 'too', 'much', 'fun', 'yesterday', '.', 'http://t.co/7ZDEfzEC']\n",
      "\n",
      "\n",
      "['@APGPhoto', 'oooh', 'nice', '..', 'Is', 'tempting', 'to', 'go', 'up', 'the', 'lakes', 'with', 'my', 'Nikon', '...', 'Hmmmm', 'I', 'may', 'do', 'that', '..']\n",
      "\n",
      "\n",
      "['BLACK', 'FRIDAY', 'Huge', 'Saving', 'Aerial', 'View', 'of', 'a', 'City', ',', 'Paris', 'Las', 'Vegas', ',', 'the', 'Las', 'Vegas', 'Strip', ',', 'Las', 'Vegas', ',', '...', 'http://t.co/DCTgeSED']\n",
      "\n",
      "\n",
      "['@MelmurMel', '@PBandJenelley_1', '@vl_delp_ham_', 'Jenelle', 'lies', ',', '1st', 'she', 'said', 'she', 'was', 'alone', '&', 'the', 'hosp.now', 'she', 'is', 'saying', 'how', 'weird', 'it', 'was', 'for', \"Keiffer's\"]\n",
      "\n",
      "\n",
      "['@MyBeautyisBrown', 'LAUGHING', 'my', 'fucking', 'ass', 'off', 'his', 'big', 'ass', 'get', 'on', 'my', 'nerves', ',', 'you', 'going', 'to', 'class', 'tomorrow', '?']\n",
      "\n",
      "\n",
      "['Mohamed', 'Morsi', ',', \"Egypt's\", 'Muslim', 'Brotherhood', 'president', ',', 'instructed', 'the', 'Supreme', 'Council', 'of', 'the', 'Armed', 'Forces', 'Thursday', '...', 'http://t.co/NnBeUvSt']\n",
      "\n",
      "\n",
      "[\"C'mon\", 'Avila', '!', 'You', 'just', 'got', 'tagged', 'out', 'by', 'a', 'guy', 'who', 'looks', 'like', 'the', 'kid', 'Bill', 'Murray', 'was', 'researching', 'in', 'The', 'Royal', 'Tenenbaums', '!', '#Tigers']\n",
      "\n",
      "\n",
      "['@thehuwdavies', 'you', 'think', 'the', 'Boro', 'will', 'beat', 'Swansea', '?', 'i', 'am', 'not', 'so', 'sure', ',', 'December', '/', 'January', 'is', 'when', 'we', 'implode']\n",
      "\n",
      "\n",
      "['At', 'the', 'first', 'Grammy', 'Awards', ',', 'held', 'on', 'for', 'May', '1959', ',', 'Domenico', 'Modugno', 'beat', 'out', 'Frank', 'Sinatra', 'and', 'Peggy', 'Lee', 'for', 'the', 'Record', 'of', 'the', 'Year', ',', 'with', 'Volare']\n",
      "\n",
      "\n",
      "['@JennetteMcHevan', 'I', 'have', 'studied', 'all', 'day', 'but', 'tomorrow', 'i', 'am', 'going', 'out', 'with', 'friends', '!', 'positive', 'Oh', 'my', 'god', 'Jennette', 'did', '?', '!', '!', '!', 'i', 'am', 'going', 'to', 'look', '!', 'positive']\n",
      "\n",
      "\n",
      "['Good', 'morning', 'Thursday', '.', '\"', 'Life', 'is', 'fragile', '.', 'we', 'are', 'not', 'guaranteed', 'a', 'tomorrow', 'so', 'give', 'it', 'everything', \"you've\", 'got', '.', '\"', '-', 'Tim', 'Cook', '[', 'Do', 'it', 'for', 'Jobs', '!', ']']\n",
      "\n",
      "\n",
      "['#Twitition', 'Mcfly', 'come', 'back', 'to', 'Argentina', 'but', 'this', 'time', 'we', 'want', 'to', 'come', 'to', 'mar', 'del', 'plata', '!', '!', '!', 'http://t.co/DlXY0LCg']\n",
      "\n",
      "\n",
      "['@Astrochologist', 'anything', '.', 'I', 'wondered', 'how', 'the', 'aspects', 'between', 'my', 'sun', '/', 'moon', 'faired', 'with', 'my', 'rising', '.', 'I', 'also', 'have', 'Venus', 'in', 'Sag', '...', 'yeah', 'laughing', 'out', 'loud', 'Thank', 'you', 'positive']\n",
      "\n",
      "\n",
      "['My', 'teachers', 'call', 'themselves', 'givng', 'us', 'candy', '...', \"wasn't\", 'even', 'the', 'GOOD', 'stuff', '.', 'I', 'might', 'go', 'to', 'Walmart', 'or', 'CVS', 'tomorrow', '/']\n",
      "\n",
      "\n",
      "['#Broncos', 'Peyton', 'Manning', 'named', 'AWAY', 'from', 'computer', 'Offensive', 'Player', 'of', 'the', 'month', '.', 'it', 'is', 'his', '5th', 'such', 'honor', ',', 'second', 'to', 'Tom', \"Brady's\", '6', ',', 'tied', 'w', '/', 'TD', '.']\n",
      "\n",
      "\n",
      "['@TooZany', 'is', 'bringing', 'out', 'Kendrick', 'Lamar', 'the', '6th', 'of', 'December', '!', '?', '!', 'Get', 'your', 'tickets', 'now', '!']\n",
      "\n",
      "\n",
      "[\"Andre's\", 'Wigan', 'Warning', '-', '#COYS', 'Official', 'Site', 'Wigan', 'might', 'currently', 'occupy', '15th', 'place', 'in', 'the', 'Premier', 'League', ',', '...', 'http://t.co/3mo7WIWd']\n",
      "\n",
      "\n",
      "['@juice005', 'strange', 'enough', ',', 'i', 'am', 'going', 'to', 'see', 'Noel', 'Gallagher', 'in', 'concert', 'tomorrow', 'night', '.', '#alldayb1tch']\n",
      "\n",
      "\n",
      "['How', 'are', 'they', 'going', 'to', 'act', 'in', 'new', 'york', 'with', 'the', 'subways', 'out', '?', 'They', 'better', 'clutch', 'it', 'out', 'like', 'Eli', 'Manning', '4th', 'quarter']\n",
      "\n",
      "\n",
      "['Homegrown', 'talent', 'missing', 'on', 'Signing', 'Day', ':', 'Throughout', 'most', 'of', 'the', 'day', 'on', 'Wednesday', ',', 'the', 'video', 'scoreboard', '...', 'http://t.co/6dZ2hQkD', '#Raleigh']\n",
      "\n",
      "\n",
      "['taylor', 'swift', 'is', 'coming', 'with', 'ed', 'sheeran', 'june', '29th', '?', 'most', 'perf', 'news', 'i', 'have', 'heard', 'all', 'night', '.']\n",
      "\n",
      "\n",
      "['The', 'great', 'Noel', 'Gallagher', 'is', 'about', 'to', 'hit', 'the', 'stage', 'in', 'Stop', 'that', '.', 'Paul', '.', 'Plenty', 'of', 'room', 'here', 'so', 'we', 'are', '4th', 'row', 'center', '.', 'Plenty', 'of', 'room', '.', 'Pretty', 'fired', 'up']\n",
      "\n",
      "\n",
      "['@lala_cox', 'There', 'are', 'going', 'to', 'be', 'plenty', 'of', 'ups', '&', 'downs', 'for', 'Boro', ',', 'Cardiff', ',', 'Leicester', 'etc', 'until', 'May', '.', 'I', 'hope', 'we', 'are', 'still', 'in', 'with', 'a', 'shout', 'for', 'promo']\n",
      "\n",
      "\n",
      "['@joeeewilliams', 'that', 'sucks', ',', 'amazon', 'might', 'have', 'some', 'left', '..', 'hate', 'that', 'i', 'will', 'not', 'have', 'it', 'until', '11am', 'on', 'Tuesday', 'though']\n",
      "\n",
      "\n",
      "['Playing', 'as', 'the', 'Browns', 'HOW', 'come', 'Ok', '.', 'Fri', 'in', '#Madden13', ',', 'XFacalac', 'lost', 'to', 'the', 'Ravens', '21-28', 'on', 'All-Madden', '#ConnectedCareers']\n",
      "\n",
      "\n",
      "['@rosanne_89', 'Nooooooooo', 'rosanne', '!', 'they', 'have', 'just', 'restocked', 'them', 'and', 'me', 'going', 'overseas', 'Sunday', 'means', 'I', 'can', 'not', 'afford', 'them', \":'(\"]\n",
      "\n",
      "\n",
      "['The', 'pain', 'is', 'far', 'deeper', 'than', 'a', 'Billy', 'cundiff', 'missed', 'field', 'goal', '.', 'Got', 'to', 'wake', 'up', 'and', 'forget', 'about', 'it', 'tomorrow', '.', '#Orioles', '#stayhungry']\n",
      "\n",
      "\n",
      "['@grandvincenzo', 'on', 'November', '16', 'be', 'ready', 'to', 'head', 'to', 'the', 'movies', '.', 'Breaking', 'Dawn', ':', 'Part', 'too', '.', '(;']\n",
      "\n",
      "\n",
      "['after', 'seeing', '@Iam_FaithReid', \"'\", 's', 'tweet', 'about', 'watching', 'The', 'Vow', ',', 'I', 'think', 'I', 'may', 'need', 'to', 'watch', 'it', '.']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start = 0\n",
    "end = 50\n",
    "for tweet in raw_tweets[start:end]:\n",
    "    print (tweet)\n",
    "    print ('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "negation_words = set(['barely', 'hardly', 'lack', 'never', 'neither', 'no', 'nobody', \\\n",
    "                      'not', 'nothing', 'none', 'nowhere', 'shortage', 'scarcely'])\n",
    "punctuations = [',', '.', ':', ';', '!', '?']\n",
    "\n",
    "def handle_negation(tweet):\n",
    "    output = []\n",
    "    negate = False\n",
    "    for word in tweet:\n",
    "        if word[-1] in punctuations and negate:\n",
    "            negate = False\n",
    "        if negate and not word.lower() in negation_words:\n",
    "            output.append(word+\"_not\")\n",
    "        else:\n",
    "            output.append(word)\n",
    "        if word.lower() in negation_words and not negate:\n",
    "            negate = True\n",
    "        elif word.lower() in negation_words and negate:\n",
    "            negate = False\n",
    "    return output\n",
    "\n",
    "raw_tweets = [handle_negation(tweet) for tweet in raw_tweets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Gas', 'by', 'my', 'house', 'hit', '$', '3.39', '!', '!', '!', 'i', 'am', 'going', 'to', 'Chapel', 'Hill', 'on', 'Sat', '.', 'positive']\n",
      "\n",
      "\n",
      "['Iranian', 'general', 'says', \"Israel's\", 'Iron', 'Dome', 'can', 'not', 'deal_not', 'with_not', 'their_not', 'missiles_not', '(_not', 'keep_not', 'talking_not', 'like_not', 'that_not', 'and_not', 'we_not', 'may_not', 'end_not', 'up_not', 'finding_not', 'out_not', ')_not']\n",
      "\n",
      "\n",
      "['with', 'J', 'Davlar', '11th', '.', 'Main', 'rivals', 'are', 'team', 'Poland', '.', 'Hopefully', 'we', 'an', 'make', 'it', 'a', 'successful', 'end', 'to', 'a', 'tough', 'week', 'of', 'training', 'tomorrow', '.']\n",
      "\n",
      "\n",
      "['Talking', 'about', \"ACT's\", '&', '&', \"SAT's\", ',', 'deciding', 'where', 'I', 'want', 'to', 'go', 'to', 'college', ',', 'applying', 'to', 'colleges', 'and', 'everything', 'about', 'college', 'stresses', 'me', 'out', '.']\n",
      "\n",
      "\n",
      "['They', 'may', 'have', 'a', 'SuperBowl', 'in', 'Dallas', ',', 'but', 'Dallas', 'am', 'not', 'winning_not', 'a_not', 'SuperBowl_not', '.', 'Not', 'with_not', 'that_not', 'quarterback_not', 'and_not', 'owner_not', '.', '@S4NYC', '@RasmussenPoll']\n",
      "\n",
      "\n",
      "['Instant', 'message', 'bringing', 'the', 'monster', 'load', 'of', 'candy', 'tomorrow', ',', 'I', 'just', 'hope', 'it', 'does', 'not', 'get_not', 'all_not', 'squiched_not']\n",
      "\n",
      "\n",
      "['Apple', 'software', ',', 'retail', 'chiefs', 'out', 'in', 'overhaul', ':', 'SAN', 'FRANCISCO', 'Apple', 'Inc', 'CEO', 'Tim', 'Cook', 'on', 'Monday', 'replaced', 'the', 'heads', '...', 'http://t.co/X49ZEOsG']\n",
      "\n",
      "\n",
      "['@oluoch', '@victor_otti', '@kunjand', 'I', 'just', 'watched', 'it', '!', \"Sridevi's\", 'comeback', '...', 'You', 'remember', 'her', 'from', 'the', '90s', '?', '?', 'Sun', 'mornings', 'on', 'NTA', 'positive']\n",
      "\n",
      "\n",
      "['#Livewire', 'Nadal', 'confirmed', 'for', 'Mexican', 'Open', 'in', 'February', ':', 'Rafael', 'Nadal', 'is', 'set', 'to', 'play', 'at', 'the', 'Me', '...', 'http://t.co/zgUXpcnC', '#LiveWireAthletics']\n",
      "\n",
      "\n",
      "['@MsSheLahY', 'I', 'didnt', 'want', 'to', 'just', 'pop', 'up', '...', 'but', 'yep', 'we', 'have', 'chapel', 'hill', 'next', 'wednesday', 'you', 'should', 'come', '..', 'and', 'shes', 'great', 'ill', 'tell', 'her', 'you', 'asked']\n",
      "\n",
      "\n",
      "['@Alyoup005', '@addicted2haley', 'hmmmm', 'November', 'is', 'an', 'odd', 'release', 'date', 'if', 'true', 'but', 'if', 'it', 'becomes', 'big', 'enough', 'maybe', 'she', 'could', 'sing', 'it', 'at', 'Grammys']\n",
      "\n",
      "\n",
      "['#Iran', 'US', 'delisting', 'MKO', 'from', 'global', 'terrorists', 'list', 'in', 'line', 'with', 'Iran', 'campaign', ':', 'Tehran', ',', 'Oct', '30', ',', 'IRNA', '-', '-', 'Secretary', '...', 'http://t.co/9wWvxEbf']\n",
      "\n",
      "\n",
      "['Good', 'Morning', 'Becky', '!', 'Thursday', 'is', 'going', 'to', 'be', 'Fantastic', '!', '@SwedenG', '@DJ4JG', '@Grdina', '@Paverlayer', '@FSBull', '@RevkahJC', '@DicksTrash', '@borderfox116']\n",
      "\n",
      "\n",
      "['Expect', 'light-moderate', 'rains', 'over', 'E', '.', 'Visayas', ';', 'Cebu', ',', 'Bohol', ',', 'Samar', '&', 'Leyte', 'have', '30-70', '%', 'chance', 'of', 'rains', 'tonight', '!', 'Expect', 'fair', 'weather', 'tomorrow', '!', 'positive']\n",
      "\n",
      "\n",
      "['One', 'ticket', 'left', 'for', 'the', '@49ers', 'game', 'tomorrow', '!', 'do', 'not', 'miss_not', 'the_not', 'rematch_not', 'of_not', 'the_not', 'NO', 'fucking', 'clue', 'Championship', 'game', 'against', 'the', 'NY', 'Giants', '!', 'Hit', 'me', 'up', '!']\n",
      "\n",
      "\n",
      "['AWAY', 'from', 'computer', 'away', 'fans', 'on', 'Saturday', '.', 'All', 'this', 'stuff', 'about', 'the', \"'\", 'she', 'said', 'no', \"'_not\", 'chant_not', '.', 'it', 'is', 'bollocks', '.', 'When', 'he', 'has', 'the', 'ball', ',', 'just', 'turn', 'your', 'back', 'on', 'him', '.']\n",
      "\n",
      "\n",
      "['Why', 'is', 'it', 'so', 'hard', 'to', 'find', 'the', '@TVGuideMagazine', 'these', 'days', '?', 'Went', 'to', '3', 'stores', 'for', 'the', 'Castle', 'cover', 'issue', '.', 'NONE', '.', 'Will', 'search', 'again', 'tomorrow', '...']\n",
      "\n",
      "\n",
      "['Game', '1', 'of', 'the', 'NLCS', 'and', 'a', 'rematch', 'of', 'the', 'NO', 'fucking_not', 'clue_not', 'Championship_not', 'game_not', 'tomorrow_not', '.', \"SF's\", 'going', 'to', 'be', 'cuuuuhraaaaaaaazeeeee']\n",
      "\n",
      "\n",
      "['@TrevorJavier', 'the', 'heat', 'game', 'may', 'cost', 'a', 'lot', 'more', '...', 'and', 'plus', 'I', 'would', 'rather', 'see', 'Austin', 'Rivers', 'play']\n",
      "\n",
      "\n",
      "['Never', 'start_not', 'working_not', 'on_not', 'your_not', 'dreams_not', 'and_not', 'goals_not', 'tomorrow_not', '...', 'tomorrow', 'never', 'comes_not', '...', 'if', 'it', 'means', 'anything', 'to', 'You', ',', 'ACT', 'NOW', '!', '#getafterit']\n",
      "\n",
      "\n",
      "['@TheFFAddict', 'I', 'had', 'Vick', 'and', 'Flacco', ',', 'needed', 'an', 'upgrade', '.', 'Vick', 'may', 'get', 'benched', ',', 'Jennings', 'a', 'back', 'up', 'again', 'soon', '.', 'I', 'thought', 'it', 'was', 'a', 'win', 'for', 'me', '.']\n",
      "\n",
      "\n",
      "['Looks', 'like', 'Andy', 'the', 'Android', 'may', 'have', 'had', 'a', 'little', 'too', 'much', 'fun', 'yesterday', '.', 'http://t.co/7ZDEfzEC']\n",
      "\n",
      "\n",
      "['@APGPhoto', 'oooh', 'nice', '..', 'Is', 'tempting', 'to', 'go', 'up', 'the', 'lakes', 'with', 'my', 'Nikon', '...', 'Hmmmm', 'I', 'may', 'do', 'that', '..']\n",
      "\n",
      "\n",
      "['BLACK', 'FRIDAY', 'Huge', 'Saving', 'Aerial', 'View', 'of', 'a', 'City', ',', 'Paris', 'Las', 'Vegas', ',', 'the', 'Las', 'Vegas', 'Strip', ',', 'Las', 'Vegas', ',', '...', 'http://t.co/DCTgeSED']\n",
      "\n",
      "\n",
      "['@MelmurMel', '@PBandJenelley_1', '@vl_delp_ham_', 'Jenelle', 'lies', ',', '1st', 'she', 'said', 'she', 'was', 'alone', '&', 'the', 'hosp.now', 'she', 'is', 'saying', 'how', 'weird', 'it', 'was', 'for', \"Keiffer's\"]\n",
      "\n",
      "\n",
      "['@MyBeautyisBrown', 'LAUGHING', 'my', 'fucking', 'ass', 'off', 'his', 'big', 'ass', 'get', 'on', 'my', 'nerves', ',', 'you', 'going', 'to', 'class', 'tomorrow', '?']\n",
      "\n",
      "\n",
      "['Mohamed', 'Morsi', ',', \"Egypt's\", 'Muslim', 'Brotherhood', 'president', ',', 'instructed', 'the', 'Supreme', 'Council', 'of', 'the', 'Armed', 'Forces', 'Thursday', '...', 'http://t.co/NnBeUvSt']\n",
      "\n",
      "\n",
      "[\"C'mon\", 'Avila', '!', 'You', 'just', 'got', 'tagged', 'out', 'by', 'a', 'guy', 'who', 'looks', 'like', 'the', 'kid', 'Bill', 'Murray', 'was', 'researching', 'in', 'The', 'Royal', 'Tenenbaums', '!', '#Tigers']\n",
      "\n",
      "\n",
      "['@thehuwdavies', 'you', 'think', 'the', 'Boro', 'will', 'beat', 'Swansea', '?', 'i', 'am', 'not', 'so_not', 'sure_not', ',', 'December', '/', 'January', 'is', 'when', 'we', 'implode']\n",
      "\n",
      "\n",
      "['At', 'the', 'first', 'Grammy', 'Awards', ',', 'held', 'on', 'for', 'May', '1959', ',', 'Domenico', 'Modugno', 'beat', 'out', 'Frank', 'Sinatra', 'and', 'Peggy', 'Lee', 'for', 'the', 'Record', 'of', 'the', 'Year', ',', 'with', 'Volare']\n",
      "\n",
      "\n",
      "['@JennetteMcHevan', 'I', 'have', 'studied', 'all', 'day', 'but', 'tomorrow', 'i', 'am', 'going', 'out', 'with', 'friends', '!', 'positive', 'Oh', 'my', 'god', 'Jennette', 'did', '?', '!', '!', '!', 'i', 'am', 'going', 'to', 'look', '!', 'positive']\n",
      "\n",
      "\n",
      "['Good', 'morning', 'Thursday', '.', '\"', 'Life', 'is', 'fragile', '.', 'we', 'are', 'not', 'guaranteed_not', 'a_not', 'tomorrow_not', 'so_not', 'give_not', 'it_not', 'everything_not', \"you've_not\", 'got_not', '.', '\"', '-', 'Tim', 'Cook', '[', 'Do', 'it', 'for', 'Jobs', '!', ']']\n",
      "\n",
      "\n",
      "['#Twitition', 'Mcfly', 'come', 'back', 'to', 'Argentina', 'but', 'this', 'time', 'we', 'want', 'to', 'come', 'to', 'mar', 'del', 'plata', '!', '!', '!', 'http://t.co/DlXY0LCg']\n",
      "\n",
      "\n",
      "['@Astrochologist', 'anything', '.', 'I', 'wondered', 'how', 'the', 'aspects', 'between', 'my', 'sun', '/', 'moon', 'faired', 'with', 'my', 'rising', '.', 'I', 'also', 'have', 'Venus', 'in', 'Sag', '...', 'yeah', 'laughing', 'out', 'loud', 'Thank', 'you', 'positive']\n",
      "\n",
      "\n",
      "['My', 'teachers', 'call', 'themselves', 'givng', 'us', 'candy', '...', \"wasn't\", 'even', 'the', 'GOOD', 'stuff', '.', 'I', 'might', 'go', 'to', 'Walmart', 'or', 'CVS', 'tomorrow', '/']\n",
      "\n",
      "\n",
      "['#Broncos', 'Peyton', 'Manning', 'named', 'AWAY', 'from', 'computer', 'Offensive', 'Player', 'of', 'the', 'month', '.', 'it', 'is', 'his', '5th', 'such', 'honor', ',', 'second', 'to', 'Tom', \"Brady's\", '6', ',', 'tied', 'w', '/', 'TD', '.']\n",
      "\n",
      "\n",
      "['@TooZany', 'is', 'bringing', 'out', 'Kendrick', 'Lamar', 'the', '6th', 'of', 'December', '!', '?', '!', 'Get', 'your', 'tickets', 'now', '!']\n",
      "\n",
      "\n",
      "[\"Andre's\", 'Wigan', 'Warning', '-', '#COYS', 'Official', 'Site', 'Wigan', 'might', 'currently', 'occupy', '15th', 'place', 'in', 'the', 'Premier', 'League', ',', '...', 'http://t.co/3mo7WIWd']\n",
      "\n",
      "\n",
      "['@juice005', 'strange', 'enough', ',', 'i', 'am', 'going', 'to', 'see', 'Noel', 'Gallagher', 'in', 'concert', 'tomorrow', 'night', '.', '#alldayb1tch']\n",
      "\n",
      "\n",
      "['How', 'are', 'they', 'going', 'to', 'act', 'in', 'new', 'york', 'with', 'the', 'subways', 'out', '?', 'They', 'better', 'clutch', 'it', 'out', 'like', 'Eli', 'Manning', '4th', 'quarter']\n",
      "\n",
      "\n",
      "['Homegrown', 'talent', 'missing', 'on', 'Signing', 'Day', ':', 'Throughout', 'most', 'of', 'the', 'day', 'on', 'Wednesday', ',', 'the', 'video', 'scoreboard', '...', 'http://t.co/6dZ2hQkD', '#Raleigh']\n",
      "\n",
      "\n",
      "['taylor', 'swift', 'is', 'coming', 'with', 'ed', 'sheeran', 'june', '29th', '?', 'most', 'perf', 'news', 'i', 'have', 'heard', 'all', 'night', '.']\n",
      "\n",
      "\n",
      "['The', 'great', 'Noel', 'Gallagher', 'is', 'about', 'to', 'hit', 'the', 'stage', 'in', 'Stop', 'that', '.', 'Paul', '.', 'Plenty', 'of', 'room', 'here', 'so', 'we', 'are', '4th', 'row', 'center', '.', 'Plenty', 'of', 'room', '.', 'Pretty', 'fired', 'up']\n",
      "\n",
      "\n",
      "['@lala_cox', 'There', 'are', 'going', 'to', 'be', 'plenty', 'of', 'ups', '&', 'downs', 'for', 'Boro', ',', 'Cardiff', ',', 'Leicester', 'etc', 'until', 'May', '.', 'I', 'hope', 'we', 'are', 'still', 'in', 'with', 'a', 'shout', 'for', 'promo']\n",
      "\n",
      "\n",
      "['@joeeewilliams', 'that', 'sucks', ',', 'amazon', 'might', 'have', 'some', 'left', '..', 'hate', 'that', 'i', 'will', 'not', 'have_not', 'it_not', 'until_not', '11am_not', 'on_not', 'Tuesday_not', 'though_not']\n",
      "\n",
      "\n",
      "['Playing', 'as', 'the', 'Browns', 'HOW', 'come', 'Ok', '.', 'Fri', 'in', '#Madden13', ',', 'XFacalac', 'lost', 'to', 'the', 'Ravens', '21-28', 'on', 'All-Madden', '#ConnectedCareers']\n",
      "\n",
      "\n",
      "['@rosanne_89', 'Nooooooooo', 'rosanne', '!', 'they', 'have', 'just', 'restocked', 'them', 'and', 'me', 'going', 'overseas', 'Sunday', 'means', 'I', 'can', 'not', 'afford_not', 'them_not', \":'(_not\"]\n",
      "\n",
      "\n",
      "['The', 'pain', 'is', 'far', 'deeper', 'than', 'a', 'Billy', 'cundiff', 'missed', 'field', 'goal', '.', 'Got', 'to', 'wake', 'up', 'and', 'forget', 'about', 'it', 'tomorrow', '.', '#Orioles', '#stayhungry']\n",
      "\n",
      "\n",
      "['@grandvincenzo', 'on', 'November', '16', 'be', 'ready', 'to', 'head', 'to', 'the', 'movies', '.', 'Breaking', 'Dawn', ':', 'Part', 'too', '.', '(;']\n",
      "\n",
      "\n",
      "['after', 'seeing', '@Iam_FaithReid', \"'\", 's', 'tweet', 'about', 'watching', 'The', 'Vow', ',', 'I', 'think', 'I', 'may', 'need', 'to', 'watch', 'it', '.']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start = 0\n",
    "end = 50\n",
    "for tweet in raw_tweets[start:end]:\n",
    "    print (tweet)\n",
    "    print ('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7205\n"
     ]
    }
   ],
   "source": [
    "def preprocess(tweet):\n",
    "    tweet = ' '.join(tweet) #change from 'list' to str\n",
    "    # delete symbols and URIs and tags (keep # and _)\n",
    "    tweet =  ' '.join(re.sub(\"(@[A-Za-z0-9_]+)|([^0-9A-Za-z_#' \\t])|(\\w+:\\/\\/\\S+)\", '', tweet).split())\n",
    "    # Convert '@username' to 'at_user'\n",
    "    # tweet = re.sub('@[^\\s]+','at_user',tweet)\n",
    "    # remove hashtags\n",
    "    # tweet = re.sub(r'#\\s', '', tweet)\n",
    "    # remove numbers\n",
    "    tweet = re.sub('[0-9]', '', tweet)\n",
    "    # remove additional spaces\n",
    "    tweet = re.sub('[\\s]+', ' ', tweet)\n",
    "    # replace the occurrence of 2 or more characters in a word, eg. loooong -> loong\n",
    "    tweet = re.sub(r'(.)\\1{2,}', r'\\1\\1', tweet)\n",
    "    return tweet\n",
    "\n",
    "preprocessed_tweets = [preprocess(tweet) for tweet in raw_tweets]\n",
    "print (len(preprocessed_tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gas by my house hit i am going to Chapel Hill on Sat positive\n",
      "\n",
      "\n",
      "Iranian general says Israel's Iron Dome can not deal_not with_not their_not missiles_not _not keep_not talking_not like_not that_not and_not we_not may_not end_not up_not finding_not out_not _not\n",
      "\n",
      "\n",
      "with J Davlar th Main rivals are team Poland Hopefully we an make it a successful end to a tough week of training tomorrow\n",
      "\n",
      "\n",
      "Talking about ACT's SAT's deciding where I want to go to college applying to colleges and everything about college stresses me out\n",
      "\n",
      "\n",
      "They may have a SuperBowl in Dallas but Dallas am not winning_not a_not SuperBowl_not Not with_not that_not quarterback_not and_not owner_not\n",
      "\n",
      "\n",
      "Instant message bringing the monster load of candy tomorrow I just hope it does not get_not all_not squiched_not\n",
      "\n",
      "\n",
      "Apple software retail chiefs out in overhaul SAN FRANCISCO Apple Inc CEO Tim Cook on Monday replaced the heads\n",
      "\n",
      "\n",
      "I just watched it Sridevi's comeback You remember her from the s Sun mornings on NTA positive\n",
      "\n",
      "\n",
      "#Livewire Nadal confirmed for Mexican Open in February Rafael Nadal is set to play at the Me #LiveWireAthletics\n",
      "\n",
      "\n",
      "I didnt want to just pop up but yep we have chapel hill next wednesday you should come and shes great ill tell her you asked\n",
      "\n",
      "\n",
      "hmm November is an odd release date if true but if it becomes big enough maybe she could sing it at Grammys\n",
      "\n",
      "\n",
      "#Iran US delisting MKO from global terrorists list in line with Iran campaign Tehran Oct IRNA Secretary\n",
      "\n",
      "\n",
      "Good Morning Becky Thursday is going to be Fantastic\n",
      "\n",
      "\n",
      "Expect lightmoderate rains over E Visayas Cebu Bohol Samar Leyte have chance of rains tonight Expect fair weather tomorrow positive\n",
      "\n",
      "\n",
      "One ticket left for the game tomorrow do not miss_not the_not rematch_not of_not the_not NO fucking clue Championship game against the NY Giants Hit me up\n",
      "\n",
      "\n",
      "AWAY from computer away fans on Saturday All this stuff about the ' she said no '_not chant_not it is bollocks When he has the ball just turn your back on him\n",
      "\n",
      "\n",
      "Why is it so hard to find the these days Went to stores for the Castle cover issue NONE Will search again tomorrow\n",
      "\n",
      "\n",
      "Game of the NLCS and a rematch of the NO fucking_not clue_not Championship_not game_not tomorrow_not SF's going to be cuuhraazee\n",
      "\n",
      "\n",
      "the heat game may cost a lot more and plus I would rather see Austin Rivers play\n",
      "\n",
      "\n",
      "Never start_not working_not on_not your_not dreams_not and_not goals_not tomorrow_not tomorrow never comes_not if it means anything to You ACT NOW #getafterit\n",
      "\n",
      "\n",
      "I had Vick and Flacco needed an upgrade Vick may get benched Jennings a back up again soon I thought it was a win for me\n",
      "\n",
      "\n",
      "Looks like Andy the Android may have had a little too much fun yesterday\n",
      "\n",
      "\n",
      "ooh nice Is tempting to go up the lakes with my Nikon Hmm I may do that\n",
      "\n",
      "\n",
      "BLACK FRIDAY Huge Saving Aerial View of a City Paris Las Vegas the Las Vegas Strip Las Vegas\n",
      "\n",
      "\n",
      "Jenelle lies st she said she was alone the hospnow she is saying how weird it was for Keiffer's\n",
      "\n",
      "\n",
      "LAUGHING my fucking ass off his big ass get on my nerves you going to class tomorrow\n",
      "\n",
      "\n",
      "Mohamed Morsi Egypt's Muslim Brotherhood president instructed the Supreme Council of the Armed Forces Thursday\n",
      "\n",
      "\n",
      "C'mon Avila You just got tagged out by a guy who looks like the kid Bill Murray was researching in The Royal Tenenbaums #Tigers\n",
      "\n",
      "\n",
      "you think the Boro will beat Swansea i am not so_not sure_not December January is when we implode\n",
      "\n",
      "\n",
      "At the first Grammy Awards held on for May Domenico Modugno beat out Frank Sinatra and Peggy Lee for the Record of the Year with Volare\n",
      "\n",
      "\n",
      "I have studied all day but tomorrow i am going out with friends positive Oh my god Jennette did i am going to look positive\n",
      "\n",
      "\n",
      "Good morning Thursday Life is fragile we are not guaranteed_not a_not tomorrow_not so_not give_not it_not everything_not you've_not got_not Tim Cook Do it for Jobs\n",
      "\n",
      "\n",
      "#Twitition Mcfly come back to Argentina but this time we want to come to mar del plata\n",
      "\n",
      "\n",
      "anything I wondered how the aspects between my sun moon faired with my rising I also have Venus in Sag yeah laughing out loud Thank you positive\n",
      "\n",
      "\n",
      "My teachers call themselves givng us candy wasn't even the GOOD stuff I might go to Walmart or CVS tomorrow\n",
      "\n",
      "\n",
      "#Broncos Peyton Manning named AWAY from computer Offensive Player of the month it is his th such honor second to Tom Brady's tied w TD\n",
      "\n",
      "\n",
      "is bringing out Kendrick Lamar the th of December Get your tickets now\n",
      "\n",
      "\n",
      "Andre's Wigan Warning #COYS Official Site Wigan might currently occupy th place in the Premier League\n",
      "\n",
      "\n",
      "strange enough i am going to see Noel Gallagher in concert tomorrow night #alldaybtch\n",
      "\n",
      "\n",
      "How are they going to act in new york with the subways out They better clutch it out like Eli Manning th quarter\n",
      "\n",
      "\n",
      "Homegrown talent missing on Signing Day Throughout most of the day on Wednesday the video scoreboard #Raleigh\n",
      "\n",
      "\n",
      "taylor swift is coming with ed sheeran june th most perf news i have heard all night\n",
      "\n",
      "\n",
      "The great Noel Gallagher is about to hit the stage in Stop that Paul Plenty of room here so we are th row center Plenty of room Pretty fired up\n",
      "\n",
      "\n",
      "There are going to be plenty of ups downs for Boro Cardiff Leicester etc until May I hope we are still in with a shout for promo\n",
      "\n",
      "\n",
      "that sucks amazon might have some left hate that i will not have_not it_not until_not am_not on_not Tuesday_not though_not\n",
      "\n",
      "\n",
      "Playing as the Browns HOW come Ok Fri in #Madden XFacalac lost to the Ravens on AllMadden #ConnectedCareers\n",
      "\n",
      "\n",
      "Noo rosanne they have just restocked them and me going overseas Sunday means I can not afford_not them_not '_not\n",
      "\n",
      "\n",
      "The pain is far deeper than a Billy cundiff missed field goal Got to wake up and forget about it tomorrow #Orioles #stayhungry\n",
      "\n",
      "\n",
      "on November be ready to head to the movies Breaking Dawn Part too\n",
      "\n",
      "\n",
      "after seeing ' s tweet about watching The Vow I think I may need to watch it\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start = 0\n",
    "end = 50\n",
    "for tweet in preprocessed_tweets[start:end]:\n",
    "    print (tweet)\n",
    "    print ('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Delete stopwords\n",
    "https://nlp.stanford.edu/IR-book/html/htmledition/dropping-common-terms-stop-words-1.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "358\n",
      "\n",
      "Compare tweets before / after\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>3</th>\n",
       "      <th>final_tweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Gas by my house hit $3.39!!!! I'm going to Cha...</td>\n",
       "      <td>Gas house hit going Chapel Hill Sat positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Iranian general says Israel's Iron Dome can't ...</td>\n",
       "      <td>Iranian general says Israel's Iron Dome deal_n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>with J Davlar 11th. Main rivals are team Polan...</td>\n",
       "      <td>J Davlar th Main rivals team Poland Hopefully ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Talking about ACT's &amp;amp;&amp;amp; SAT's, deciding...</td>\n",
       "      <td>Talking ACT's SAT's deciding want go college a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>They may have a SuperBowl in Dallas, but Dalla...</td>\n",
       "      <td>may SuperBowl Dallas Dallas winning_not SuperB...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Im bringing the monster load of candy tomorrow...</td>\n",
       "      <td>Instant message bringing monster load candy to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Apple software, retail chiefs out in overhaul:...</td>\n",
       "      <td>Apple software retail chiefs overhaul SAN FRAN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>@oluoch @victor_otti @kunjand I just watched i...</td>\n",
       "      <td>watched Sridevi's comeback remember Sun mornin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>#Livewire Nadal confirmed for Mexican Open in ...</td>\n",
       "      <td>#Livewire Nadal confirmed Mexican Open Februar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>@MsSheLahY I didnt want to just pop up... but ...</td>\n",
       "      <td>didnt want pop yep chapel hill next wednesday ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    3  \\\n",
       "0   Gas by my house hit $3.39!!!! I'm going to Cha...   \n",
       "3   Iranian general says Israel's Iron Dome can't ...   \n",
       "6   with J Davlar 11th. Main rivals are team Polan...   \n",
       "7   Talking about ACT's &amp;&amp; SAT's, deciding...   \n",
       "9   They may have a SuperBowl in Dallas, but Dalla...   \n",
       "10  Im bringing the monster load of candy tomorrow...   \n",
       "11  Apple software, retail chiefs out in overhaul:...   \n",
       "12  @oluoch @victor_otti @kunjand I just watched i...   \n",
       "14  #Livewire Nadal confirmed for Mexican Open in ...   \n",
       "15  @MsSheLahY I didnt want to just pop up... but ...   \n",
       "\n",
       "                                         final_tweets  \n",
       "0        Gas house hit going Chapel Hill Sat positive  \n",
       "3   Iranian general says Israel's Iron Dome deal_n...  \n",
       "6   J Davlar th Main rivals team Poland Hopefully ...  \n",
       "7   Talking ACT's SAT's deciding want go college a...  \n",
       "9   may SuperBowl Dallas Dallas winning_not SuperB...  \n",
       "10  Instant message bringing monster load candy to...  \n",
       "11  Apple software retail chiefs overhaul SAN FRAN...  \n",
       "12  watched Sridevi's comeback remember Sun mornin...  \n",
       "14  #Livewire Nadal confirmed Mexican Open Februar...  \n",
       "15  didnt want pop yep chapel hill next wednesday ...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend([word+'_not' for word in stop_words]) #negation\n",
    "stop_words = set(stop_words)\n",
    "# stop_words.update('j', 'im')\n",
    "print (len(stop_words))\n",
    "\n",
    "# remove stopwords\n",
    "def rem_stop(tweet):\n",
    "    words = tweet.split()\n",
    "    tweet = ' '.join([word for word in words if word.lower() not in stop_words])\n",
    "    return tweet\n",
    "\n",
    "final_tweets = [rem_stop(tweet) for tweet in preprocessed_tweets]\n",
    "del raw_tweets, preprocessed_tweets\n",
    "\n",
    "print(\"\\nCompare tweets before / after\")\n",
    "df['final_tweets'] = final_tweets\n",
    "df[[3, 'final_tweets']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## <font color='red'>Lexicon Classification</font>\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using MPQA Lexicon\n",
    "\n",
    "These are the very first and last entries of the file 'mpqa.txt'\n",
    "- abandoned priorpolarity=negative\n",
    "- abandonment priorpolarity=negative\n",
    "- abandon priorpolarity=negative\n",
    "- abase priorpolarity=negative\n",
    "- abasement priorpolarity=negative\n",
    "- ...\n",
    "- zealot priorpolarity=negative\n",
    "- zealous priorpolarity=negative\n",
    "- zealously priorpolarity=negative\n",
    "- zenith priorpolarity=positive\n",
    "- zest priorpolarity=positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of MPQA words: 13772\n",
      "['watched', \"Sridevi's\", 'positive', 'remember', 'Sun', 'mornings', 'NTA', 'positive']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['neutral', 'positive', 'negative']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def MPQAdictionary(filename):\n",
    "    \"\"\"\n",
    "    reads mpqa file which contains the polarity of some of the english words. e.g. 'love': 'positive'\n",
    "    \"\"\"\n",
    "    MPQA_scores = {'priorpolarity=positive\\n': 'positive','priorpolarity=negative\\n': 'negative',\n",
    "                  'priorpolarity=neutral\\n': 'neutral', 'priorpolarity=both\\n': 'neutral'}\n",
    "    MPQA_score_list = {}\n",
    "    fi = open(filename,\"r\")\n",
    "    line = fi.readline()\n",
    "    while line: \n",
    "        li = line.split(\" \")\n",
    "        l2 = li[:-1] # the word as a list\n",
    "        sentiment=li[1] #the word's polarity\n",
    "        score=MPQA_scores[sentiment]\n",
    "        l2.append(score)\n",
    "        for i in range(0,len(l2)-1):\n",
    "            MPQA_score_list[l2[i]]=l2[-1]\n",
    "            # negation\n",
    "            if l2[-1] == 'positive':\n",
    "                MPQA_score_list[l2[i]+'_not']='positive' \n",
    "            else:\n",
    "                MPQA_score_list[l2[i]+'_not']='negative' \n",
    "        line=fi.readline()\n",
    "    return MPQA_score_list\n",
    "\n",
    "\n",
    "def subsMPQA(tweet,d):\n",
    "    l = TT.tokenize(tweet)\n",
    "    #print(l)\n",
    "    tweet = [d[i] if i in d.keys() else i for i in l]\n",
    "    return tweet\n",
    "\n",
    "dictionary = MPQAdictionary('./resources/mpqa/mpqa.txt')\n",
    "print (\"Number of MPQA words: %d\" % len(dictionary.keys()))\n",
    "raw_tweets_MPQA = [subsMPQA(tweet, dictionary) for tweet in final_tweets]\n",
    "\n",
    "print (subsMPQA(final_tweets[7], dictionary))\n",
    "# watched sridevis comeback remember sun morning nta positive\n",
    "subsMPQA(\"surprise happy abandoned\", dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Bing Liu Lexicon\n",
    "These are the very first and last entries of the file 'positive-words.txt':\n",
    "- a+\n",
    "- abound\n",
    "- abounds\n",
    "- abundance\n",
    "- abundant\n",
    "- ...\n",
    "- youthful\n",
    "- zeal\n",
    "- zenith\n",
    "- zest\n",
    "- zippy\n",
    "\n",
    "These are the very first and last entries of the file 'negative-words.txt':\n",
    "- 2-faced\n",
    "- 2-faces\n",
    "- abnormal\n",
    "- abolish\n",
    "- abominable\n",
    "- ...\n",
    "- zaps\n",
    "- zealot\n",
    "- zealous\n",
    "- zealously\n",
    "- zombie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of positive words 6789\n",
      "Number of negative words 6789\n"
     ]
    }
   ],
   "source": [
    "ENGLISH_OPINION_LEXICON_LOCATION = os.path.join('resources/opinion-lexicon-English')\n",
    "POS_WORDS_FILE = os.path.join(ENGLISH_OPINION_LEXICON_LOCATION, 'positive-words.txt')\n",
    "NEG_WORDS_FILE = os.path.join(ENGLISH_OPINION_LEXICON_LOCATION, 'negative-words.txt')\n",
    "\n",
    "pos_words = []\n",
    "neg_words = []\n",
    "\n",
    "for pos_word in open(POS_WORDS_FILE, 'r').readlines()[35:]:\n",
    "    pos_words.append(pos_word.rstrip())\n",
    "\n",
    "for neg_word in open(NEG_WORDS_FILE, 'r').readlines()[35:]:\n",
    "    neg_words.append(neg_word.rstrip())\n",
    "\n",
    "#negation\n",
    "expanded_pos = copy(pos_words)\n",
    "expanded_pos.extend([word+\"_not\" for word in neg_words])\n",
    "expanded_neg = copy(neg_words)\n",
    "expanded_neg.extend([word+\"_not\" for word in pos_words])\n",
    "\n",
    "#delete unnecessary objects\n",
    "del pos_words, neg_words\n",
    "del ENGLISH_OPINION_LEXICON_LOCATION, POS_WORDS_FILE, NEG_WORDS_FILE\n",
    "print (\"Number of positive words %d\" % len(expanded_pos))\n",
    "print (\"Number of negative words %d\" % len(expanded_neg))\n",
    "\n",
    "def subsBINGP(tweet, pos_words):\n",
    "    # l = TT.tokenize(tweet)\n",
    "    tweet = ['positive' if i in pos_words else i for i in tweet]\n",
    "    return tweet\n",
    "\n",
    "def subsBINGN(tweet, neg_words):\n",
    "    # l = TT.tokenize(tweet)\n",
    "    tweet = ['negative' if i in neg_words else i for i in tweet]\n",
    "    return tweet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['positive', 'firas', 'positive']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_tweets_bing = [subsBINGP(tweet, expanded_pos) for tweet in raw_tweets_MPQA]\n",
    "raw_tweets_bing = [subsBINGN(tweet, expanded_neg) for tweet in raw_tweets_MPQA]\n",
    "del raw_tweets_MPQA\n",
    "\n",
    "subsBINGP(['enjoy', 'firas', 'extraordinarily'], expanded_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Afinn](https://pypi.python.org/pypi/afinn)\n",
    "\n",
    "These are the very first and last entries of 'afinn.txt':\n",
    "- abandon\t-2\n",
    "- abandoned\t-2\n",
    "- abandons\t-2\n",
    "- abducted\t-2\n",
    "- abduction\t-2\n",
    "- ...\n",
    "- yucky\t-2\n",
    "- yummy\t3\n",
    "- zealot\t-2\n",
    "- zealots\t-2\n",
    "- zealous\t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Afinn entries 4922\n"
     ]
    }
   ],
   "source": [
    "def loadAfinn(filename):\n",
    "    f=open(filename,'r')\n",
    "    afinn={}\n",
    "    line=f.readline()\n",
    "    while line:\n",
    "        if \" \" in line:   #exclude entries like 'cool stuff    3'\n",
    "            pass\n",
    "        else:\n",
    "            l=line[:-1].split('\\t') #line[:-1] removes the '\\r\\n' character\n",
    "            afinn[l[0]]=float(l[1])    # normalization -------> \n",
    "            afinn[l[0]+\"_not\"] = -float(l[1])  # negation\n",
    "        line=f.readline()\n",
    "\n",
    "    return afinn\n",
    "\n",
    "afinn = loadAfinn('./resources/afinn.txt')\n",
    "# print (afinn)\n",
    "print (\"Number of Afinn entries %d\" % len(afinn.keys()))\n",
    "\n",
    "def afinnPolarity(tweet, afinn):\n",
    "    p=0.0\n",
    "    num = 0\n",
    "    for w in tweet:\n",
    "        if w in afinn.keys():\n",
    "            num += 1\n",
    "            p+=afinn[w]\n",
    "    return p, num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SemEval2015 English lexicon \n",
    "\n",
    "These are the very first and last entries of 'SemEval2015-English-Twitter-Lexicon.txt':\n",
    "- 0.984\tloves\n",
    "- 0.984\t#inspirational\n",
    "- 0.969\tamazing\n",
    "- 0.969\t#peaceful\n",
    "- 0.953\t#greatness\n",
    "- ...\n",
    "- -0.969\tabuse\n",
    "- -0.969\t#failure\n",
    "- -0.982\tkill\n",
    "- -0.984\tbitches\n",
    "- -0.984\t#disappointment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of EnglishLexicon entries 3032\n"
     ]
    }
   ],
   "source": [
    "def loadSemEval(filename):\n",
    "    f=open(filename,'r')\n",
    "    lexicon={}\n",
    "    line=f.readline()\n",
    "    while line:\n",
    "        l=line[:-1].split('\\t')\n",
    "        lexicon[l[1]]=float(l[0])\n",
    "        lexicon[l[1]+'_not']=-float(l[0]) # negation\n",
    "        line=f.readline()\n",
    "\n",
    "    return lexicon\n",
    "\n",
    "EnglishLexicon = loadSemEval('./resources/SemEval2015-English-Twitter-Lexicon.txt')\n",
    "# print (EnglishLexicon)\n",
    "print (\"Number of EnglishLexicon entries %d\" % len(EnglishLexicon.keys()))\n",
    "\n",
    "\n",
    "def SemEvalLexiconPolarity(tweet, EnglishLexicon):\n",
    "    p=0.0\n",
    "    num = 0\n",
    "    for w in tweet:\n",
    "        if w in EnglishLexicon.keys():\n",
    "            num +=1\n",
    "            p+=EnglishLexicon[w]\n",
    "    return p, num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SentiWordNet\n",
    "\n",
    "Here is the very first five lines of the csv file 'sentiWordnetBig.csv':\n",
    "\n",
    "|POS|ID|PosSCore|NegScore|SynsetTerms|\n",
    "|-|-------|-----|-----|-------------------|\n",
    "|a|1740|0.125|0|able#1|\n",
    "|a|2098|0|0.75|unable#1|\n",
    "|a|2312|0|0|dorsal#2 abaxial#1|\n",
    "|a|2527|0|0|ventral#2 adaxial#1|\n",
    "|a|2730|0|0|acroscopic#1|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening the SentiWordnet file ...\n",
      "Loading...\n",
      "Number of sentiWordnet entries 294612\n"
     ]
    }
   ],
   "source": [
    "def loadSentiWordnet(filename): \n",
    "    output={}\n",
    "    print (\"Opening the SentiWordnet file ...\")\n",
    "    fi=open(filename,\"r\")\n",
    "    line=fi.readline() # ignore the header\n",
    "    line=fi.readline()\n",
    "    print (\"Loading...\")\n",
    "\n",
    "    while line:\n",
    "        l=line.split('\\t')\n",
    "        try:\n",
    "            sentence=l[4]\n",
    "            new = [word for word in sentence.split() if (word[-2] == \"#\" and word[-1].isdigit())]\n",
    "            pos=abs(float(l[2]))\n",
    "            neg=abs(float(l[3]))\n",
    "            neu=float(pos-neg)\n",
    "        except:\n",
    "            line=fi.readline()\n",
    "            continue\n",
    "\n",
    "        for w in new:\n",
    "            output[(w[:-2])]=neu\n",
    "            output[(w[:-2])+'_not'] = -neu   #negation\n",
    "        line=fi.readline()\n",
    "        \n",
    "    fi.close()\n",
    "    return output\n",
    "\n",
    "\n",
    "sentiWordnet = loadSentiWordnet('./resources/sentiWordnetBig.csv')\n",
    "print (\"Number of sentiWordnet entries %d\" % len(sentiWordnet.keys()))\n",
    "\n",
    "\n",
    "\n",
    "def WordnetPolarity(tweet, sentiWordnet):\n",
    "    p=0.0\n",
    "    num = 0\n",
    "    for w in tweet:\n",
    "        if w in sentiWordnet.keys():\n",
    "            num += 1\n",
    "            p+=sentiWordnet[w]\n",
    "    return p, num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polarity calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7205, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bing_mpqa_score</th>\n",
       "      <th>afinn_score</th>\n",
       "      <th>wordnet_score</th>\n",
       "      <th>sem_eval_score</th>\n",
       "      <th>final_score</th>\n",
       "      <th>final_tweets</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.009805</td>\n",
       "      <td>0.008657</td>\n",
       "      <td>-0.004690</td>\n",
       "      <td>0.009668</td>\n",
       "      <td>0.023439</td>\n",
       "      <td>Gas house hit going Chapel Hill Sat positive</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.019610</td>\n",
       "      <td>0.017313</td>\n",
       "      <td>0.002345</td>\n",
       "      <td>0.017601</td>\n",
       "      <td>0.056869</td>\n",
       "      <td>Iranian general says Israel's Iron Dome deal_n...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.009805</td>\n",
       "      <td>0.008657</td>\n",
       "      <td>0.011725</td>\n",
       "      <td>0.007132</td>\n",
       "      <td>0.037319</td>\n",
       "      <td>J Davlar th Main rivals team Poland Hopefully ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.011725</td>\n",
       "      <td>-0.000162</td>\n",
       "      <td>-0.011888</td>\n",
       "      <td>Talking ACT's SAT's deciding want go college a...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.009805</td>\n",
       "      <td>-0.008657</td>\n",
       "      <td>-0.011725</td>\n",
       "      <td>-0.008562</td>\n",
       "      <td>-0.038749</td>\n",
       "      <td>may SuperBowl Dallas Dallas winning_not SuperB...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.011725</td>\n",
       "      <td>-0.001745</td>\n",
       "      <td>-0.013470</td>\n",
       "      <td>Instant message bringing monster load candy to...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Apple software retail chiefs overhaul SAN FRAN...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.019610</td>\n",
       "      <td>0.017313</td>\n",
       "      <td>0.007035</td>\n",
       "      <td>0.018555</td>\n",
       "      <td>0.062513</td>\n",
       "      <td>watched Sridevi's comeback remember Sun mornin...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002345</td>\n",
       "      <td>0.006817</td>\n",
       "      <td>0.009162</td>\n",
       "      <td>#Livewire Nadal confirmed Mexican Open Februar...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.019610</td>\n",
       "      <td>0.017313</td>\n",
       "      <td>-0.002345</td>\n",
       "      <td>0.022247</td>\n",
       "      <td>0.056825</td>\n",
       "      <td>didnt want pop yep chapel hill next wednesday ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    bing_mpqa_score  afinn_score  wordnet_score  sem_eval_score  final_score  \\\n",
       "0          0.009805     0.008657      -0.004690        0.009668     0.023439   \n",
       "3          0.019610     0.017313       0.002345        0.017601     0.056869   \n",
       "6          0.009805     0.008657       0.011725        0.007132     0.037319   \n",
       "7          0.000000     0.000000      -0.011725       -0.000162    -0.011888   \n",
       "9         -0.009805    -0.008657      -0.011725       -0.008562    -0.038749   \n",
       "10         0.000000     0.000000      -0.011725       -0.001745    -0.013470   \n",
       "11         0.000000     0.000000       0.000000        0.000000     0.000000   \n",
       "12         0.019610     0.017313       0.007035        0.018555     0.062513   \n",
       "14         0.000000     0.000000       0.002345        0.006817     0.009162   \n",
       "15         0.019610     0.017313      -0.002345        0.022247     0.056825   \n",
       "\n",
       "                                         final_tweets         2  \n",
       "0        Gas house hit going Chapel Hill Sat positive  positive  \n",
       "3   Iranian general says Israel's Iron Dome deal_n...  negative  \n",
       "6   J Davlar th Main rivals team Poland Hopefully ...  positive  \n",
       "7   Talking ACT's SAT's deciding want go college a...  negative  \n",
       "9   may SuperBowl Dallas Dallas winning_not SuperB...  negative  \n",
       "10  Instant message bringing monster load candy to...   neutral  \n",
       "11  Apple software retail chiefs overhaul SAN FRAN...   neutral  \n",
       "12  watched Sridevi's comeback remember Sun mornin...  positive  \n",
       "14  #Livewire Nadal confirmed Mexican Open Februar...   neutral  \n",
       "15  didnt want pop yep chapel hill next wednesday ...  positive  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BingMpqaScore = []\n",
    "AfinnScore, AfinnReps = [], []\n",
    "WordnetScore, WordnetReps = [], []\n",
    "SemEvalScore, SemEvalReps = [], []\n",
    "length = len(raw_tweets_bing)\n",
    "\n",
    "for tw in raw_tweets_bing:\n",
    "    Bing_MPQA = 0\n",
    "    for i in tw:\n",
    "        if (i == 'positive'):\n",
    "            Bing_MPQA +=  1\n",
    "        if (i == 'negative'):\n",
    "            Bing_MPQA -= 1\n",
    "    BingMpqaScore.append(Bing_MPQA)\n",
    "    tmp = afinnPolarity(tw, afinn)\n",
    "    AfinnScore.append(tmp[0])\n",
    "    AfinnReps.append(tmp[1])\n",
    "    tmp = WordnetPolarity(tw, sentiWordnet)\n",
    "    WordnetScore.append(tmp[0])\n",
    "    WordnetReps.append(tmp[1])\n",
    "    tmp = SemEvalLexiconPolarity(tw, EnglishLexicon)\n",
    "    SemEvalScore.append(tmp[0])\n",
    "    SemEvalReps.append(tmp[1])\n",
    "\n",
    "    \n",
    "#reshape\n",
    "BingMpqaScore = np.array(BingMpqaScore).reshape(length, 1)\n",
    "AfinnScore = np.array(AfinnScore).reshape(length, 1)\n",
    "AfinnReps = np.array(AfinnReps).reshape(length, 1)\n",
    "WordnetScore = np.array(WordnetScore).reshape(length, 1)\n",
    "WordnetReps = np.array(WordnetReps).reshape(length, 1)\n",
    "SemEvalScore = np.array(SemEvalScore).reshape(length, 1)\n",
    "SemEvalReps = np.array(SemEvalReps).reshape(length, 1)\n",
    "\n",
    "#Normalization\n",
    "BingMpqaScore = BingMpqaScore/np.linalg.norm(BingMpqaScore)\n",
    "AfinnScore = AfinnScore/np.linalg.norm(AfinnScore)\n",
    "AfinnReps = AfinnReps/np.linalg.norm(AfinnReps)\n",
    "WordnetScore = WordnetScore/np.linalg.norm(WordnetScore)\n",
    "WordnetReps = WordnetReps/np.linalg.norm(WordnetReps)\n",
    "SemEvalScore = SemEvalScore/np.linalg.norm(SemEvalScore)\n",
    "SemEvalReps = SemEvalReps/np.linalg.norm(SemEvalReps)\n",
    "\n",
    "\n",
    "#final_score_tweets (my score list)\n",
    "df['bing_mpqa_score'] = BingMpqaScore\n",
    "df['afinn_score'] = AfinnScore\n",
    "df['wordnet_score'] = WordnetScore\n",
    "df['sem_eval_score'] = SemEvalScore\n",
    "total = np.hstack( (BingMpqaScore, AfinnScore, WordnetScore, SemEvalScore) )\n",
    "final = np.sum(total, axis=1).reshape(length, 1)\n",
    "print (total.shape)\n",
    "df['final_score'] = final\n",
    "\n",
    "df[['bing_mpqa_score','afinn_score', 'wordnet_score', 'sem_eval_score','final_score', 'final_tweets' ,2]].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Comparing the scores with the real results, we can conclude that the use of lexicon does not give very good results ... We must add scores for bi-grams.\n",
    "##### You must then combine the lexicon / machine learning!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "del df, raw_tweets_bing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## <font color='red'>Train the model</font>\n",
    "***\n",
    "#### Create a feature vector\n",
    "* See [Bag of Words](https://en.wikipedia.org/wiki/Bag-of-words_model) for more details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7205, 139389)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vectorizer = CountVectorizer(analyzer='word', preprocessor=None, stop_words=None, tokenizer=None,  ngram_range=(1,3))\n",
    "features = count_vectorizer.fit_transform(final_tweets)\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(7205, 5)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#reducing the CountVector\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "svd = TruncatedSVD(n_components=5, n_iter=7, random_state=42)\n",
    "count_features = svd.fit_transform(features)\n",
    "count_features = scipy.sparse.csr_matrix(count_features)\n",
    "print (type(count_features))\n",
    "count_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7205, 139389)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(analyzer='word', preprocessor=None, stop_words=None, tokenizer=None,  ngram_range=(1,3))\n",
    "features = tfidf_vectorizer.fit_transform(final_tweets)\n",
    "del final_tweets\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7205, 4) (7205, 1) (7205, 1) (7205, 1) (7205, 1)\n",
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "(7205, 8)\n"
     ]
    }
   ],
   "source": [
    "print (total.shape, final.shape, AfinnReps.shape, WordnetReps.shape, SemEvalReps.shape)\n",
    "final_total = np.hstack( (total, final, AfinnReps, WordnetReps, SemEvalReps) )\n",
    "final_total = scipy.sparse.csr_matrix(final_total)\n",
    "print (type(final_total))\n",
    "print (final_total.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7205, 139402)\n"
     ]
    }
   ],
   "source": [
    "features = scipy.sparse.hstack([count_features, features, final_total])\n",
    "print (features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# delete unnecessary data objects\n",
    "del total, final, final_total, BingMpqaScore, AfinnScore, WordnetScore, SemEvalScore\n",
    "del AfinnReps, WordnetReps, SemEvalReps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Put labels to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7205,)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapper = {'positive': 1, 'negative': -1, 'neutral': 0}\n",
    "\n",
    "labels = labels.map(mapper)\n",
    "labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import SVM\n",
    "\n",
    "http://scikit-learn.org/stable/modules/svm.html\n",
    "\n",
    "For a mathematical overview,\n",
    "https://docs.opencv.org/2.4/doc/tutorials/ml/introduction_to_svm/introduction_to_svm.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the optimal regulation parameter using handout method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max value of 0.997 occured at C=3.900\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x7f1c122a83c8>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRUFERDJUFEREJENFQUREMlQUREQk4/8D9tmZaA+p59gAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1c130b0390>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Takes around 20-30 minutes without the feature selection part\n",
    "# Takes a couple of miniutes after the feature selection\n",
    "\n",
    "KERNEL = 'linear'\n",
    "C_start = 0.1\n",
    "C_end = 4.0\n",
    "C_inc = 0.20\n",
    "\n",
    "C_values, precision_scores = [], []\n",
    "\n",
    "C_val = C_start\n",
    "best_precision_score = 0\n",
    "while (C_val < C_end):\n",
    "    C_values.append(C_val)\n",
    "    svc_model_loop = SVC(C=C_val, kernel=KERNEL, random_state=42)\n",
    "    svc_model_loop.fit(features, labels.ravel())\n",
    "    svc_predict_loop_test = svc_model_loop.predict(features)\n",
    "    \n",
    "    precision_score = metrics.precision_score(labels, svc_predict_loop_test, average='micro')\n",
    "    precision_scores.append(precision_score)\n",
    "    if (precision_score > best_precision_score):\n",
    "        best_precision_score = precision_score\n",
    "        best_lr_predict_test = svc_predict_loop_test\n",
    "        \n",
    "    C_val = C_val + C_inc\n",
    "\n",
    "best_score_C_val = C_values[precision_scores.index(best_precision_score)]\n",
    "print(\"Max value of {0:.3f} occured at C={1:.3f}\".format(best_precision_score, best_score_C_val))\n",
    "\n",
    "%matplotlib inline \n",
    "plt.plot(C_values, precision_scores, \"-\")\n",
    "plt.xlabel(\"C value\")\n",
    "plt.ylabel(\"precision score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=3.9000000000000012, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape=None, degree=3, gamma='auto', kernel='linear',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# best_score_C_val = 1.300\n",
    "KERNEL = 'linear'\n",
    "classifier = SVC(kernel=KERNEL, C=best_score_C_val)\n",
    "classifier.fit(features, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction on training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9971\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "nb_predict_train = classifier.predict(features)\n",
    "#check accuracy\n",
    "print(\"Accuracy: {:0.4f}\".format(metrics.accuracy_score(labels, nb_predict_train)))\n",
    "del features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2681    1   10]\n",
      " [   2 1015    7]\n",
      " [   1    0 3488]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       1.00      1.00      1.00      2692\n",
      "         -1       1.00      0.99      1.00      1024\n",
      "          0       1.00      1.00      1.00      3489\n",
      "\n",
      "avg / total       1.00      1.00      1.00      7205\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#print confusion matrix\n",
    "print(\"{}\".format(metrics.confusion_matrix(labels, nb_predict_train, labels=[1,-1, 0])))\n",
    "\n",
    "print(\"{}\".format(metrics.classification_report(labels, nb_predict_train, labels=[1, -1, 0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict using the model\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8902, 4)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_df = pd.read_csv('./data/test/actual/test_B_labeled.tsv', sep='\\t', header=None)\n",
    "t_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7584, 4)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_df = t_df[t_df[3] != 'Not Available']\n",
    "t_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAnMAAAHoCAYAAAAi+WkTAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1c121d7080>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# The bar chart for the test data set\n",
    "\n",
    "y = [len(t_df[t_df[2] == i]) for i in ['positive', 'negative', 'neutral']]\n",
    "x = ['positive', 'negative', 'neutral']\n",
    "x_pos = range(len(x))\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.bar(x_pos, y, alpha=0.5)\n",
    "plt.xticks(x_pos, x)\n",
    "plt.ylabel('# Occurences').set_size(15)\n",
    "plt.xlabel('Classes').set_size(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-process tweets from the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "raw_tweets_test = t_df[3]\n",
    "raw_tweets_test = [subsEmoticon(tweet, emoticon_dict) for tweet in raw_tweets_test]\n",
    "raw_tweets_test = [replaceSlangs(tweet, slangs) for tweet in raw_tweets_test]\n",
    "raw_tweets_test = [replace_apostrophe(tweet, apos) for tweet in raw_tweets_test]\n",
    "raw_tweets_test = [handle_negation(tweet) for tweet in raw_tweets_test]\n",
    "preprocessed_tweets_test = [preprocess(tweet) for tweet in raw_tweets_test]\n",
    "final_tweets_test = [rem_stop(tweet) for tweet in preprocessed_tweets_test]\n",
    "t_df[3] = final_tweets_test\n",
    "\n",
    "del raw_tweets_test, preprocessed_tweets_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7584, 4)\n"
     ]
    }
   ],
   "source": [
    "t_raw_tweets_MPQA = [subsMPQA(tweet,dictionary) for tweet in final_tweets_test]\n",
    "t_raw_tweets_bing = [subsBINGP(tweet, expanded_pos) for tweet in t_raw_tweets_MPQA]\n",
    "t_raw_tweets_bing = [subsBINGN(tweet, expanded_neg) for tweet in t_raw_tweets_MPQA]\n",
    "\n",
    "t_BingMpqaScore = []\n",
    "t_AfinnScore, t_AfinnReps = [], []\n",
    "t_WordnetScore, t_WordnetReps = [], []\n",
    "t_SemEvalScore, t_SemEvalReps = [], []\n",
    "t_length = len(t_raw_tweets_bing)\n",
    "\n",
    "for tw in t_raw_tweets_bing:\n",
    "    Bing_MPQA = 0\n",
    "    for i in tw:\n",
    "        if (i == 'positive'):\n",
    "            Bing_MPQA +=  1\n",
    "        if (i == 'negative'):\n",
    "            Bing_MPQA -= 1\n",
    "    t_BingMpqaScore.append(Bing_MPQA)\n",
    "    tmp = afinnPolarity(tw, afinn)\n",
    "    t_AfinnScore.append(tmp[0])\n",
    "    t_AfinnReps.append(tmp[1])\n",
    "    tmp = WordnetPolarity(tw, sentiWordnet)\n",
    "    t_WordnetScore.append(tmp[0])\n",
    "    t_WordnetReps.append(tmp[1])\n",
    "    tmp = SemEvalLexiconPolarity(tw, EnglishLexicon)\n",
    "    t_SemEvalScore.append(tmp[0])\n",
    "    t_SemEvalReps.append(tmp[1])\n",
    "\n",
    "    \n",
    "#reshape\n",
    "t_BingMpqaScore = np.array(t_BingMpqaScore).reshape(t_length, 1)\n",
    "t_AfinnScore = np.array(t_AfinnScore).reshape(t_length, 1)\n",
    "t_AfinnReps = np.array(t_AfinnReps).reshape(t_length, 1)\n",
    "t_WordnetScore = np.array(t_WordnetScore).reshape(t_length, 1)\n",
    "t_WordnetReps = np.array(t_WordnetReps).reshape(t_length, 1)\n",
    "t_SemEvalScore = np.array(t_SemEvalScore).reshape(t_length, 1)\n",
    "t_SemEvalReps = np.array(t_SemEvalReps).reshape(t_length, 1)\n",
    "\n",
    "#Normalization\n",
    "t_BingMpqaScore = t_BingMpqaScore/np.linalg.norm(t_BingMpqaScore)\n",
    "t_AfinnScore = t_AfinnScore/np.linalg.norm(t_AfinnScore)\n",
    "t_AfinnReps = t_AfinnReps/np.linalg.norm(t_AfinnReps)\n",
    "t_WordnetScore = t_WordnetScore/np.linalg.norm(t_WordnetScore)\n",
    "t_WordnetReps = t_WordnetReps/np.linalg.norm(t_WordnetReps)\n",
    "t_SemEvalScore = t_SemEvalScore/np.linalg.norm(t_SemEvalScore)\n",
    "t_SemEvalReps = t_SemEvalReps/np.linalg.norm(t_SemEvalReps)\n",
    "\n",
    "\n",
    "t_total = np.hstack( (t_BingMpqaScore, t_AfinnScore, t_WordnetScore, t_SemEvalScore) )\n",
    "t_final = np.sum(t_total, axis=1).reshape(t_length, 1)\n",
    "print (t_total.shape)\n",
    "\n",
    "# Delete\n",
    "del t_raw_tweets_MPQA, t_raw_tweets_bing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>282031301962395648</td>\n",
       "      <td>T14111200</td>\n",
       "      <td>neutral</td>\n",
       "      <td>dec st know end_not world_not Baby_not Boom_no...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11975</td>\n",
       "      <td>SM112166</td>\n",
       "      <td>negative</td>\n",
       "      <td>Yar quite clever aft many guesses lor got ask ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>136592</td>\n",
       "      <td>LJ112295</td>\n",
       "      <td>negative</td>\n",
       "      <td>Yeah Thin Lizzy HATE informercials</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>253421252956545024</td>\n",
       "      <td>T13114433</td>\n",
       "      <td>neutral</td>\n",
       "      <td>MT #Syria Deir Ezzor Ali Bashar altheeb martyr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>220880422320603137</td>\n",
       "      <td>T14114138</td>\n",
       "      <td>negative</td>\n",
       "      <td>hate life see_not roskilde_not festival_not sa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    0          1         2  \\\n",
       "2  282031301962395648  T14111200   neutral   \n",
       "3               11975   SM112166  negative   \n",
       "4              136592   LJ112295  negative   \n",
       "5  253421252956545024  T13114433   neutral   \n",
       "6  220880422320603137  T14114138  negative   \n",
       "\n",
       "                                                   3  \n",
       "2  dec st know end_not world_not Baby_not Boom_no...  \n",
       "3  Yar quite clever aft many guesses lor got ask ...  \n",
       "4                 Yeah Thin Lizzy HATE informercials  \n",
       "5  MT #Syria Deir Ezzor Ali Bashar altheeb martyr...  \n",
       "6  hate life see_not roskilde_not festival_not sa...  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the features vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "dimension mismatch",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-65-d801c130f719>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtest_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcount_vectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_tweets_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtest_count_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msvd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtest_count_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsr_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_count_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcount_features\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/anwar/anaconda3/lib/python3.6/site-packages/sklearn/decomposition/truncated_svd.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    202\u001b[0m         \"\"\"\n\u001b[1;32m    203\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'csr'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msafe_sparse_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomponents_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minverse_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/anwar/anaconda3/lib/python3.6/site-packages/sklearn/utils/extmath.py\u001b[0m in \u001b[0;36msafe_sparse_dot\u001b[0;34m(a, b, dense_output)\u001b[0m\n\u001b[1;32m    182\u001b[0m     \"\"\"\n\u001b[1;32m    183\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdense_output\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"toarray\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/anwar/anaconda3/lib/python3.6/site-packages/scipy/sparse/base.py\u001b[0m in \u001b[0;36m__mul__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 389\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dimension mismatch'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mul_multivector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: dimension mismatch"
     ]
    }
   ],
   "source": [
    "test_features = count_vectorizer.transform(final_tweets_test)\n",
    "test_count_features = svd.transform(test_features)\n",
    "test_count_features = scipy.sparse.csr_matrix(test_count_features)\n",
    "print (count_features.shape)\n",
    "\n",
    "\n",
    "test_features = tfidf_vectorizer.transform(final_tweets_test)\n",
    "test_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t_final_total = scipy.sparse.csr_matrix(np.hstack( (t_total, t_final, t_AfinnReps, t_WordnetReps, t_SemEvalReps) ))\n",
    "print (t_final_total.shape)\n",
    "test_features = scipy.sparse.hstack([test_count_features, test_features, t_final_total])\n",
    "del t_total, t_final, t_final_total, t_BingMpqaScore, t_AfinnScore, t_WordnetScore, t_SemEvalScore\n",
    "del t_AfinnReps, t_WordnetReps, t_SemEvalReps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get labels from a set of test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "actual_labels = t_df[2]\n",
    "actual_labels = actual_labels.map(mapper)\n",
    "actual_labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict labels using the template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predicted_labels = classifier.predict(test_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the Model\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Accuracy: {:0.2f}%'.format(metrics.accuracy_score(actual_labels, predicted_labels) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy of cross-validation 10 times on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from sklearn import model_selection\n",
    "\n",
    "# scores = model_selection.cross_val_score(classifier, test_features, actual_labels, cv=10, scoring='accuracy')\n",
    "# print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n",
    "# del test_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# draw the classification report\n",
    "print('{}'.format(metrics.classification_report(actual_labels, predicted_labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See [Confusion Matrix](https://fr.wikipedia.org/wiki/Matrice_de_confusion) for more details\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "print('{}\\n'.format(metrics.confusion_matrix(actual_labels, predicted_labels, labels=[1,-1,0])))\n",
    "print(\"\\x1b[31m\\\" macro f1 score \\\"\\x1b[0m\")\n",
    "print('{}\\n'.format(metrics.f1_score(actual_labels, predicted_labels, average='macro')))\n",
    "print(\"\\x1b[31m\\\" micro f1 score \\\"\\x1b[0m\")\n",
    "print('{}\\n'.format(metrics.f1_score(actual_labels, predicted_labels, average='micro')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison with the 5 best teams of subtask B\n",
    "\n",
    "We compare our average f-score with the other teams in the workshop. The results are taken from the attached document:\n",
    "[Final report SemEval 2014 Subtask 9](http://www.aclweb.org/anthology/S14-2009)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Team|Accuracy (Macro Averaged)| Accuracy (Micro Averaged)|\n",
    "|----|-------------------------|--------------------------|\n",
    "|TeamX|65.63%|69.99%|\n",
    "|coooolll|63.23%|70.51%|\n",
    "|RTRGO|63.08%|70.15%|\n",
    "|NRC-Canada|67.62%|71.37%|\n",
    "|TUGAS|63.89%|68.84%|\n",
    "|**_ME_**|_57.48%_|_64.86%_|\n",
    "| | |***classement : 23 / 50***|\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
