{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# <font color='green'>Analyse des sentiments</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='red'>Lecture des données</font>\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importer les bibliothèques\n",
    "\n",
    "Reportez-vous aux pages Web pour les bibliothèques individuelles\n",
    "* [pandas] (http://pandas.pydata.org/), pour charger et gérer les données\n",
    "* [matplotlib] (http://matplotlib.org/), pour la visualisation\n",
    "* [numpy] (http://www.numpy.org/) pour peindre la représentation et la manipulation\n",
    "* [re] (https://docs.python.org/3/library/re.html) pour l'expression régulière\n",
    "* [nltk] (http://www.nltk.org/) pour le prétraitement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "from copy import copy\n",
    "import collections\n",
    "import scipy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import model_selection\n",
    "from sklearn.metrics import make_scorer, roc_auc_score\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lecture de l'ensemble de données\n",
    "Certaines des données \"uploaded_cleansed_B\" sont produites à partir de \"uploaded_cleansed_A\". La différence est:\n",
    "- \"uploaded_cleansed_A\" a trois colonnes que nous n'utiliserons pas.\n",
    "- \"uploaded_cleansed_A\" a des tweets répété."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9665, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>264183816548130816</td>\n",
       "      <td>15140428</td>\n",
       "      <td>positive</td>\n",
       "      <td>Gas by my house hit $3.39!!!! I'm going to Cha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>263405084770172928</td>\n",
       "      <td>591166521</td>\n",
       "      <td>negative</td>\n",
       "      <td>Not Available</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>262163168678248449</td>\n",
       "      <td>35266263</td>\n",
       "      <td>negative</td>\n",
       "      <td>Not Available</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>264249301910310912</td>\n",
       "      <td>18516728</td>\n",
       "      <td>negative</td>\n",
       "      <td>Iranian general says Israel's Iron Dome can't ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>262682041215234048</td>\n",
       "      <td>254373818</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Not Available</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    0          1         2  \\\n",
       "0  264183816548130816   15140428  positive   \n",
       "1  263405084770172928  591166521  negative   \n",
       "2  262163168678248449   35266263  negative   \n",
       "3  264249301910310912   18516728  negative   \n",
       "4  262682041215234048  254373818   neutral   \n",
       "\n",
       "                                                   3  \n",
       "0  Gas by my house hit $3.39!!!! I'm going to Cha...  \n",
       "1                                      Not Available  \n",
       "2                                      Not Available  \n",
       "3  Iranian general says Israel's Iron Dome can't ...  \n",
       "4                                      Not Available  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./data/train/downloaded_cleansed_B.tsv', sep= '\\t', header=None)\n",
    "print (df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notez que certains tweets sont \"Non disponible\". Nous les rejetterons car cela n'aidera pas dans l'analyse des sentiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Supprimer tous les tweets \"NOT AVAILABLE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>264183816548130816</td>\n",
       "      <td>15140428</td>\n",
       "      <td>positive</td>\n",
       "      <td>Gas by my house hit $3.39!!!! I'm going to Cha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>264249301910310912</td>\n",
       "      <td>18516728</td>\n",
       "      <td>negative</td>\n",
       "      <td>Iranian general says Israel's Iron Dome can't ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>264105751826538497</td>\n",
       "      <td>147088367</td>\n",
       "      <td>positive</td>\n",
       "      <td>with J Davlar 11th. Main rivals are team Polan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>264094586689953794</td>\n",
       "      <td>332474633</td>\n",
       "      <td>negative</td>\n",
       "      <td>Talking about ACT's &amp;amp;&amp;amp; SAT's, deciding...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>254941790757601280</td>\n",
       "      <td>557103111</td>\n",
       "      <td>negative</td>\n",
       "      <td>They may have a SuperBowl in Dallas, but Dalla...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    0          1         2  \\\n",
       "0  264183816548130816   15140428  positive   \n",
       "3  264249301910310912   18516728  negative   \n",
       "6  264105751826538497  147088367  positive   \n",
       "7  264094586689953794  332474633  negative   \n",
       "9  254941790757601280  557103111  negative   \n",
       "\n",
       "                                                   3  \n",
       "0  Gas by my house hit $3.39!!!! I'm going to Cha...  \n",
       "3  Iranian general says Israel's Iron Dome can't ...  \n",
       "6  with J Davlar 11th. Main rivals are team Polan...  \n",
       "7  Talking about ACT's &amp;&amp; SAT's, deciding...  \n",
       "9  They may have a SuperBowl in Dallas, but Dalla...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[df[3] != \"Not Available\"]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7205, 4)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dessiner les données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAncAAAHsCAYAAACwg4t/AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3X+0XGV97/H3x0ARRQRLwBiiQRuqQAVLSrFY649S0Ntb\n0PojVgXUihZLFe0PaXsV6qXXu6yltVZqUC7Qqpj6o6CFKkT8gQgYKAIB0bRAIQaIVkRqixC/94+9\nTx2PJ8mZcyaZkyfv11qzZs+zn73nO1nD8Dl772c/qSokSZLUhoeMuwBJkiSNjuFOkiSpIYY7SZKk\nhhjuJEmSGmK4kyRJaojhTpIkqSGGO0mSpIYY7iRJkhpiuJMkSWrIDuMuYJz22GOPWrx48bjLkCRJ\n2qyrr776m1U1f3P9tutwt3jxYlatWjXuMiRJkjYryW3T6edpWUmSpIYY7iRJkhpiuJMkSWqI4U6S\nJKkhhjtJkqSGGO4kSZIaYriTJElqiOFOkiSpIYY7SZKkhhjuJEmSGmK4kyRJaojhTpIkqSGGO0mS\npIYY7iRJkhoy1nCX5KFJrkrylSSrk5zat5+SZG2Sa/vHcwe2OTnJmiQ3JzlioP3gJNf3696VJOP4\nTJIkSeO0w5jf/37gWVV1X5IdgcuSXNSvO72q/mywc5L9gGXA/sBjgEuS7FtVG4AzgFcDVwIXAkcC\nFyFJkrQdGeuRu+rc17/csX/UJjY5Cjivqu6vqluANcAhSRYAu1bVFVVVwLnA0VuydkmSpLlo7Nfc\nJZmX5FrgbuDiqrqyX3VikuuSnJVk975tIXD7wOZ39G0L++XJ7VO93/FJViVZtX79+pF+FkmSpHEb\ne7irqg1VdRCwN91RuAPoTrE+HjgIWAe8c4Tvt7yqllbV0vnz549qt5IkSXPCuK+5+29VdU+SS4Ej\nB6+1S3Im8Mn+5Vpg0cBme/dta/vlye2SJG3S6Rd/bdwlaBt30uH7jruEHzHu0bLzk+zWL+8MHA58\ntb+GbsLzgBv65QuAZUl2SrIPsAS4qqrWAfcmObQfJXsMcP5W+yCSJElzxLiP3C0Azkkyjy5orqiq\nTyb52yQH0Q2uuBV4DUBVrU6yArgReBB4XT9SFuAE4GxgZ7pRso6UlSRJ252xhruqug54yhTtL9/E\nNqcBp03Rvgo4YKQFSpIkbWPGPqBCkiRJo2O4kyRJaojhTpIkqSGGO0mSpIYY7iRJkhpiuJMkSWqI\n4U6SJKkhhjtJkqSGGO4kSZIaYriTJElqiOFOkiSpIYY7SZKkhhjuJEmSGmK4kyRJaojhTpIkqSGG\nO0mSpIYY7iRJkhpiuJMkSWqI4U6SJKkhhjtJkqSGGO4kSZIaYriTJElqiOFOkiSpIYY7SZKkhhju\nJEmSGmK4kyRJaojhTpIkqSGGO0mSpIYY7iRJkhpiuJMkSWqI4U6SJKkhhjtJkqSGGO4kSZIaYriT\nJElqiOFOkiSpIYY7SZKkhhjuJEmSGmK4kyRJaojhTpIkqSGGO0mSpIYY7iRJkhpiuJMkSWqI4U6S\nJKkhhjtJkqSGGO4kSZIaYriTJElqiOFOkiSpIYY7SZKkhhjuJEmSGmK4kyRJashYw12Shya5KslX\nkqxOcmrf/qgkFyf5ev+8+8A2JydZk+TmJEcMtB+c5Pp+3buSZByfSZIkaZzGfeTufuBZVXUgcBBw\nZJJDgTcDK6tqCbCyf02S/YBlwP7AkcB7kszr93UG8GpgSf84cmt+EEmSpLlgrOGuOvf1L3fsHwUc\nBZzTt58DHN0vHwWcV1X3V9UtwBrgkCQLgF2r6oqqKuDcgW0kSZK2G+M+ckeSeUmuBe4GLq6qK4G9\nqmpd3+VOYK9+eSFw+8Dmd/RtC/vlye2SJEnblbGHu6raUFUHAXvTHYU7YNL6ojuaNxJJjk+yKsmq\n9evXj2q3kiRJc8LYw92EqroHuJTuWrm7+lOt9M93993WAosGNtu7b1vbL09un+p9llfV0qpaOn/+\n/NF+CEmSpDEb92jZ+Ul265d3Bg4HvgpcABzbdzsWOL9fvgBYlmSnJPvQDZy4qj+Fe2+SQ/tRsscM\nbCNJkrTd2GHM778AOKcf8foQYEVVfTLJl4AVSV4F3Aa8CKCqVidZAdwIPAi8rqo29Ps6ATgb2Bm4\nqH9IkiRtV8Ya7qrqOuApU7R/C3j2RrY5DThtivZVwAE/voUkSdL2Y85ccydJkqTZM9xJkiQ1xHAn\nSZLUEMOdJElSQwx3kiRJDTHcSZIkNcRwJ0mS1BDDnSRJUkMMd5IkSQ0x3EmSJDXEcCdJktQQw50k\nSVJDDHeSJEkNMdxJkiQ1xHAnSZLUEMOdJElSQwx3kiRJDTHcSZIkNcRwJ0mS1BDDnSRJUkMMd5Ik\nSQ0x3EmSJDXEcCdJktQQw50kSVJDDHeSJEkNMdxJkiQ1xHAnSZLUEMOdJElSQwx3kiRJDTHcSZIk\nNcRwJ0mS1JAdxl2ApG3L6Rd/bdwlaBt30uH7jrsEqWkeuZMkSWqI4U6SJKkhhjtJkqSGGO4kSZIa\nYriTJElqiOFOkiSpIYY7SZKkhhjuJEmSGmK4kyRJaojhTpIkqSGGO0mSpIYY7iRJkhpiuJMkSWqI\n4U6SJKkhhjtJkqSGGO4kSZIaYriTJElqiOFOkiSpIYY7SZKkhow13CVZlOTSJDcmWZ3k9X37KUnW\nJrm2fzx3YJuTk6xJcnOSIwbaD05yfb/uXUkyjs8kSZI0TjuM+f0fBN5UVdckeQRwdZKL+3WnV9Wf\nDXZOsh+wDNgfeAxwSZJ9q2oDcAbwauBK4ELgSOCirfQ5JEmS5oSxHrmrqnVVdU2//F3gJmDhJjY5\nCjivqu6vqluANcAhSRYAu1bVFVVVwLnA0Vu4fEmSpDlnzlxzl2Qx8BS6I28AJya5LslZSXbv2xYC\ntw9sdkfftrBfntw+1fscn2RVklXr168f4SeQJEkavzkR7pLsAnwUeENV3Ut3ivXxwEHAOuCdo3qv\nqlpeVUuraun8+fNHtVtJkqQ5YezhLsmOdMHuA1X1MYCququqNlTVD4AzgUP67muBRQOb7923re2X\nJ7dLkiRtV8Y9WjbA+4GbqurPB9oXDHR7HnBDv3wBsCzJTkn2AZYAV1XVOuDeJIf2+zwGOH+rfAhJ\nkqQ5ZNyjZQ8DXg5cn+Tavu0PgZckOQgo4FbgNQBVtTrJCuBGupG2r+tHygKcAJwN7Ew3StaRspIk\nabsz1nBXVZcBU92P7sJNbHMacNoU7auAA0ZXnSRJ0rZn7NfcSZIkaXQMd5IkSQ0x3EmSJDXEcCdJ\nktQQw50kSVJDDHeSJEkNMdxJkiQ1xHAnSZLUEMOdJElSQwx3kiRJDTHcSZIkNcRwJ0mS1BDDnSRJ\nUkMMd5IkSQ0x3EmSJDXEcCdJktQQw50kSVJDDHeSJEkNMdxJkiQ1ZKhwl2T3JPsl2WlS+yuSnJ/k\ng0kOGW2JkiRJmq4dhuz/p8DLgD0nGpKcCPwFkL7p6CRLq+rG0ZQoSZKk6Rr2tOxhwMqq+s+Btt8F\n1gJPB17Ut71xBLVJkiRpSMMeuVsIrJx4kWQ/YBHwB1V1Wd/2QrqgJ0mSpK1s2CN3OwP/NfD6MKCA\nSwba/oUuBEqSJGkrGzbcrQWeOPD6COBe4CsDbbsDg6dtJUmStJUMe1r2UuDYJL9NdwTv14CPVtUP\nBvo8Abh9RPVJkiRpCMMeufs/wH3AXwLL6QLeKRMrk+wKPA24fET1SZIkaQhDHbmrqluS7A+8oG+6\noKr+baDLTwHvBT44ovokSZI0hGFPy1JVdwLv3si6a4BrZluUJEmSZmbocDchycOBfYFdquoLoytJ\nkiRJMzX03LJJ9k7yUeDbwCq6QRYT656W5MYkzxhdiZIkSZquYeeWXQBcCRwFfBL4Ej+cdox+3Z7A\ni0dVoCRJkqZv2CN3b6ULb4dX1fOBiwdXVtUDwBfobm4sSZKkrWzYcPdcuhGyl26iz78Bj5l5SZIk\nSZqpYcPdXsDXN9PnAeDhMytHkiRJszFsuPt3YNFm+uwL3DmzciRJkjQbw4a7LwK/luTRU61MsgQ4\nkoERtJIkSdp6hg137wAeCnwuyXOAh0F3z7v+9SeAHwDvHGmVkiRJmpZhpx+7MslrgDPoboUy4d7+\n+UHglVW1ekT1SZIkaQgzmX7srCRfAE4ADgV+EvgOcAXw7qq6ebQlSpIkabpmNP1YVX0dOGnEtUiS\nJGmWhp5+TJIkSXPXsNOPvTDJZ5JMeZPiJAuTrEzy/NGUJ0mSpGEMe+TuN4HdquobU62sqrXAI/t+\nkiRJ2sqGDXc/A6zaTJ8vA0+eWTmSJEmajWHD3aOAuzfT51vAHjMrR5IkSbMxbLj7JrBkM32WAPfM\nrBxJkiTNxkynH3viVCuTPAk4CvjCbAuTJEnS8IYNd39Gd2+8y5L8TpJ9+6nH9k3yerpQN6/vJ0mS\npK1s2OnHvpzkBOCvgdP7x6ANwG9V1ZUjqk+SJElDGPomxlV1JnAg8B7gauBf+ue/Bg6sqvdNd19J\nFiW5NMmNSVb3R/9I8qgkFyf5ev+8+8A2JydZk+TmJEcMtB+c5Pp+3buSZNjPJkmStK2b6fRjNwEn\njuD9HwTeVFXXJHkEcHWSi4HjgJVV9fYkbwbeDPxBkv2AZcD+wGOAS5LsW1UbgDOAVwNXAhcCRwIX\njaBGSZKkbcZYpx+rqnVVdU2//F3gJmAh3aCMc/pu5wBH98tHAedV1f1VdQuwBjgkyQJg16q6oqoK\nOHdgG0mSpO3GjI7cJZkH/DSwO90Aih9TVZ8fcp+LgafQHXnbq6rW9avuBPbqlxcCVwxsdkff9kC/\nPLl9qvc5Hjge4LGPfewwJUqSJM15Q4e7JP8LOIlumrFNmTL0bWSfuwAfBd5QVfcOXi5XVZWkhq1z\nY6pqObAcYOnSpSPbryRJ0lwwVLhL8vvAqcB3gL8Fbqe7bm7GkuxIF+w+UFUf65vvSrKgqtb1p1wn\nZsVYCywa2Hzvvm1tvzy5XZIkabsy7JG7V9OFpp+tqvWzffN+ROv7gZuq6s8HVl0AHAu8vX8+f6D9\ng0n+nG5AxRLgqqrakOTeJIfSndY9Bvir2dYnSZK0rRk23C0CzhxFsOsdBrwcuD7JtX3bH9KFuhVJ\nXgXcBrwIoKpWJ1kB3Eh3xPB1/UhZgBOAs4Gd6UbJOlJWkiRtd4YNd3fNYJuNqqrLgI3dj+7ZG9nm\nNOC0KdpXAQeMqjZJkqRt0bC3QlkBHJ5kpy1RjCRJkmZn2HD3VmAd8JEk+2yBeiRJkjQLw55ivQHY\nkW4ww3OTfAe4Z4p+VVVPmG1xkiRJGs6w4e4hdAMZ/m2gbapr5pzXVZIkaQyGCndVtXgL1SFJkqQR\nGOvcspIkSRqtWYW7JLsnWbT5npIkSdoahg53SXZJ8s4kdwLfBG4ZWPfzSS5M8rOjLFKSJEnTM1S4\nS/JI4EvAScA3gJv40cET1wO/CLxkVAVKkiRp+oY9cvdHwP7AcVX1s8DfD66squ8Bn2Mjs0tIkiRp\nyxo23D0f+FRVnbuJPrcBC2dekiRJkmZq2HC3N3DdZvrcBzxyZuVIkiRpNoYNd98F9txMn33oBlpI\nkiRpKxs23H0Z+NUkj5hqZZIFwHOBy2ZbmCRJkoY3bLj7S+AngQuTPGlwRf/674GHAu8aTXmSJEka\nxrDTj30qyanAW4EbgAcAknwT2J3utih/UFWXj7pQSZIkbd7QNzGuqlPpbnVyAfBtYANQwIXAL1fV\nO0ZaoSRJkqZtqCN3E6rqUuDSEdciSZKkWRp2horPJHnblipGkiRJszPsadlDgXlbohBJkiTN3rDh\n7uvAoi1RiCRJkmZv2HD3PuB/JHnslihGkiRJszPsgIpPAIcDX0zyf+luanwn3WjZH1FV/zb78iRJ\nkjSMYcPdv9IFudDd0Hhjagb7liRJ0iwNG8DOZYqjdJIkSZobhp2h4rgtVIckSZJGYOgZKiRJkjR3\nGe4kSZIaMtRp2SRnTbNrVdWrZlCPJEmSZmHYARXHbWb9xEjaAgx3kiRJW9mw4W6fjbTvBvwc8L+A\ny4E3z6YoSZIkzcywo2Vv28iq24CvJPkUcB1wCfD+WdYmSZKkIY10QEVV3U43i8XrR7lfSZIkTc+W\nGC17F7BkC+xXkiRJmzHScJdkHvAs4Duj3K8kSZKmZ9hboTx9E/tZBLwCOAh43yzrkiRJ0gwMO1r2\ns2x6btkAnwd+b6YFSZIkaeaGDXd/wtTh7gfAt4GrquqqWVclSZKkGRn2ViinbKE6JEmSNALOLStJ\nktSQocJdkoOTvCXJXhtZ/+h+/UGjKU+SJEnDGPbI3ZuA3wTu3sj6u+jmlH3jbIqSJEnSzAwb7p4K\nXFpVU46Y7ds/Axw228IkSZI0vGHD3aOBOzbT5xvAgpmVI0mSpNkYNtx9D5i/mT7zgftnVo4kSZJm\nY9hwdy1wVJJdplqZZFfgqL6fJEmStrJhw91yuiNzFyd58uCKJAcCnwb26PtJkiRpKxv2JsYfTvIc\n4Bjgn5PcBawFFgJ70U0/dm5VfWjklUqSJGmzhr6JcVUdB7wWuJFugMXB/fNq4Ph+vSRJksZg2Lll\nAaiq5cDyJA8DdgPuqarvjbQySZIkDW1G4W5CH+gMdZIkSXPEWKcfS3JWkruT3DDQdkqStUmu7R/P\nHVh3cpI1SW5OcsSkuq7v170rSYb5XJIkSa0Y9/RjZwNHTtF+elUd1D8uBEiyH7AM2L/f5j1J5vX9\nzwBeDSzpH1PtU5IkqXljnX6sqj4P/Ps03/so4Lyqur+qbgHWAIckWQDsWlVX9O9/LnD0NPcpSZLU\nlLk6/diJSa7rT9vu3rctBG4f6HNH37ZwUk0T7VNKcnySVUlWrV+/fpZlSpIkzS1zcfqxM4DHAwcB\n64B3zmJfP6aqllfV0qpaOn/+5j6KJEnStmXOTT9WVXdV1Yaq+gFwJnBIv2otsGig695929p+eXK7\nJEnSdmfOTT/WX0M34XnAxEjaC4BlSXZKsg/dwImrqmodcG+SQ/tRsscA58/0/SVJkrZlY51+LMmH\ngGcAeyS5A3gr8Iz+VioF3Aq8pn/v1UlW0M2M8SDwuqra0O/qBLqRtzsDF/UPSZKk7c7QNzGuquOS\nXA6cSHdbkkf3q24A3lVV7xtiXy+Zovn9m+h/GnDaFO2rgAOm+76SJEmtcvoxSZKkhgwd7pL8Et19\n7B7TN30DuAz4/AjrkiRJ0gxMO9z1oe4M4Kcnmvrn6td/Ffit/sbEkiRJGoNphbskvw58qO+/DriU\nH95QeBHdoIgnAZckWVZVHxt9qZIkSdqczYa7JI8BzqEboXoi8L6BUaoTfR5CN6fsXwDnJrmiqr6x\nBeqVJEnSJkznPndvAB4GvLSq3js52AFU1Q+q6kzgpX3f14+2TEmSJE3HdMLdkcCVVfXxzXWsqn8A\nrgSeM9vCJEmSNLzphLvHAZcPsc/LgcUzqkaSJEmzMp1wtyPw/SH2+QAwb2blSJIkaTamE+7WAT8z\nxD73B+6cWTmSJEmajemEu88Dhyd54uY6JnkScATe0FiSJGksphPu3k13avaTSfbbWKc+2H2C7pTs\nX4+mPEmSJA1js/e5q6qrk7wD+D3gmiQfA1byozcx/mXgecBPAO+sqlVbqF5JkiRtwrRmqKiqP0jy\nH8AfA8uAF0/qEmAD8DbglFEWKEmSpOmb9tyyVfUnSc4BXgkcBizoV90JXAacXVW3jL5ESZIkTde0\nwx1AVd0GvHUL1SJJkqRZms6ACkmSJG0jDHeSJEkNMdxJkiQ1xHAnSZLUEMOdJElSQwx3kiRJDTHc\nSZIkNcRwJ0mS1BDDnSRJUkMMd5IkSQ0x3EmSJDXEcCdJktQQw50kSVJDDHeSJEkNMdxJkiQ1xHAn\nSZLUEMOdJElSQwx3kiRJDTHcSZIkNcRwJ0mS1BDDnSRJUkMMd5IkSQ0x3EmSJDXEcCdJktQQw50k\nSVJDDHeSJEkNMdxJkiQ1xHAnSZLUEMOdJElSQwx3kiRJDTHcSZIkNcRwJ0mS1BDDnSRJUkMMd5Ik\nSQ3ZYZxvnuQs4FeBu6vqgL7tUcCHgcXArcCLqurb/bqTgVcBG4DfqapP9e0HA2cDOwMXAq+vqtqa\nn2VjTr/4a+MuQdu4kw7fd9wlSJK2IeM+cnc2cOSktjcDK6tqCbCyf02S/YBlwP79Nu9JMq/f5gzg\n1cCS/jF5n5IkSduFsYa7qvo88O+Tmo8CzumXzwGOHmg/r6rur6pbgDXAIUkWALtW1RX90bpzB7aR\nJEnaroz7yN1U9qqqdf3yncBe/fJC4PaBfnf0bQv75cntU0pyfJJVSVatX79+dFVLkiTNAXMx3P23\n/kjcSK+dq6rlVbW0qpbOnz9/lLuWJEkau7kY7u7qT7XSP9/dt68FFg3027tvW9svT26XJEna7szF\ncHcBcGy/fCxw/kD7siQ7JdmHbuDEVf0p3HuTHJokwDED20iSJG1Xxn0rlA8BzwD2SHIH8Fbg7cCK\nJK8CbgNeBFBVq5OsAG4EHgReV1Ub+l2dwA9vhXJR/5AkSdrujDXcVdVLNrLq2Rvpfxpw2hTtq4AD\nRliaJEnSNmkunpaVJEnSDBnuJEmSGmK4kyRJaojhTpIkqSGGO0mSpIYY7iRJkhpiuJMkSWqI4U6S\nJKkhhjtJkqSGGO4kSZIaYriTJElqiOFOkiSpIYY7SZKkhhjuJEmSGmK4kyRJaojhTpIkqSGGO0mS\npIYY7iRJkhpiuJMkSWqI4U6SJKkhhjtJkqSGGO4kSZIaYriTJElqiOFOkiSpIYY7SZKkhhjuJEmS\nGmK4kyRJaojhTpIkqSGGO0mSpIYY7iRJkhpiuJMkSWqI4U6SJKkhhjtJkqSGGO4kSZIaYriTJElq\niOFOkiSpIYY7SZKkhhjuJEmSGmK4kyRJaojhTpIkqSGGO0mSpIYY7iRJkhpiuJMkSWqI4U6SJKkh\nhjtJkqSGGO4kSZIaYriTJElqiOFOkiSpIYY7SZKkhhjuJEmSGjJnw12SW5Ncn+TaJKv6tkcluTjJ\n1/vn3Qf6n5xkTZKbkxwxvsolSZLGZ86Gu94zq+qgqlrav34zsLKqlgAr+9ck2Q9YBuwPHAm8J8m8\ncRQsSZI0TnM93E12FHBOv3wOcPRA+3lVdX9V3QKsAQ4ZQ32SJEljNZfDXQGXJLk6yfF9215Vta5f\nvhPYq19eCNw+sO0dfduPSXJ8klVJVq1fv35L1C1JkjQ2O4y7gE14WlWtTbIncHGSrw6urKpKUsPu\ntKqWA8sBli5dOvT2kiRJc9mcPXJXVWv757uBj9OdZr0ryQKA/vnuvvtaYNHA5nv3bZIkSduVORnu\nkjw8ySMmloFfAW4ALgCO7bsdC5zfL18ALEuyU5J9gCXAVVu3akmSpPGbq6dl9wI+ngS6Gj9YVf+U\n5MvAiiSvAm4DXgRQVauTrABuBB4EXldVG8ZTuiRJ0vjMyXBXVf8KHDhF+7eAZ29km9OA07ZwaZIk\nSXPanDwtK0mSpJkx3EmSJDXEcCdJktQQw50kSVJDDHeSJEkNMdxJkiQ1xHAnSZLUEMOdJElSQwx3\nkiRJDTHcSZIkNcRwJ0mS1BDDnSRJUkMMd5IkSQ0x3EmSJDXEcCdJktQQw50kSVJDDHeSJEkNMdxJ\nkiQ1xHAnSZLUEMOdJElSQwx3kiRJDTHcSZIkNcRwJ0mS1BDDnSRJUkMMd5IkSQ0x3EmSJDXEcCdJ\nktQQw50kSVJDDHeSJEkNMdxJkiQ1xHAnSZLUEMOdJElSQwx3kiRJDTHcSZIkNcRwJ0mS1BDDnSRJ\nUkMMd5IkSQ0x3EmSJDXEcCdJktQQw50kSVJDDHeSJEkNMdxJkiQ1xHAnSZLUEMOdJElSQwx3kiRJ\nDTHcSZIkNcRwJ0mS1BDDnSRJUkMMd5IkSQ0x3EmSJDXEcCdJktSQpsJdkiOT3JxkTZI3j7seSZKk\nra2ZcJdkHvDXwHOA/YCXJNlvvFVJkiRtXc2EO+AQYE1V/WtVfR84DzhqzDVJkiRtVTuMu4ARWgjc\nPvD6DuDnJ3dKcjxwfP/yviQ3b4XatGl7AN8cdxFz1RvHXYBmwu/0Jvid3ib5nd6Erfidftx0OrUU\n7qalqpYDy8ddh34oyaqqWjruOqRR8Tut1vid3ra0dFp2LbBo4PXefZskSdJ2o6Vw92VgSZJ9kvwE\nsAy4YMw1SZIkbVXNnJatqgeT/DbwKWAecFZVrR5zWZoeT5OrNX6n1Rq/09uQVNW4a5AkSdKItHRa\nVpIkabtnuJMkSWqI4U5zSpLdkpww8PoxST4yzpqk6Ury2iTH9MvHJXnMwLr3OWuOtmVJFif5jRlu\ne9+o69HGec2d5pQki4FPVtUBYy5FmpUknwV+t6pWjbsWaRSSPIPuO/2rU6zboaoe3MS291XVLluy\nPv2QR+40lP4vt5uSnJlkdZJPJ9k5yROS/FOSq5N8IckT+/5PSHJFkuuT/O+Jv96S7JJkZZJr+nUT\nU8W9HXhCkmuTvKN/vxv6ba5Isv9ALZ9NsjTJw5OcleSqJP88sC9p2vrv2leTfKD/jn8kycOSPLv/\nXl3ff8926vu/PcmNSa5L8md92ylJfjfJC4ClwAf67/LOA9/X1yZ5x8D7Hpfk3f3yy/rv8bVJ3tvP\nmS3Nygx+t8/uv8MT208cdXs78Iv99/Ok/rt7QZLPACs38buura2qfPiY9gNYDDwIHNS/XgG8DFgJ\nLOnbfh74TL/8SeAl/fJrgfv65R2AXfvlPYA1QPr93zDp/W7ol08CTu2XFwA398t/CrysX94N+Brw\n8HH/W/nYth79d62Aw/rXZwF/TDet4b5927nAG4CfBG7mh2c/duufT6E7sgHwWWDpwP4/Sxf45tPN\ngz3RfhHwNOBJwCeAHfv29wDHjPvfxce2/5jB7/bZwAsGtp/43X4G3ZmVifbj6Kb6fFT/esrf9cF9\n+Ng6D4/caSZuqapr++Wr6X44fgH4+yTXAu+lC18ATwX+vl/+4MA+AvxpkuuAS+jmBt5rM++7Apj4\na/JFwMS1eL8CvLl/788CDwUeO/SnkuD2qvpiv/x3wLPpvu9f69vOAZ4OfAf4L+D9SZ4PfG+6b1BV\n64F/TXJW7PCQAAAHtElEQVRokp8Engh8sX+vg4Ev99/lZwOPH8FnkmC43+1hXFxV/94vz+R3XVtA\nMzcx1lZ1/8DyBrr/eO+pqoOG2MdL6Y5gHFxVDyS5lS6UbVRVrU3yrSRPBl5MdyQQuh+UX6+qm4d4\nf2kqky9CvofuKN2Pdupumn4IXQB7AfDbwLOGeJ/z6P5A+Srw8aqqJAHOqaqTZ1S5tGnD/G4/SH/Z\nVpKHAD+xif3+x8Dy0L/r2jI8cqdRuBe4JckLAdI5sF93BfDr/fKygW0eCdzd/wA8E3hc3/5d4BGb\neK8PA78PPLKqruvbPgWc2P/PkSRPme0H0nbrsUme2i//BrAKWJzkp/q2lwOfS7IL3XfwQrrLBQ78\n8V1t8rv8ceAo4CV0QQ+6U2QvSLInQJJHJXncRraXZmtTv9u30h1FBvg1YMd+eXO/zxv7XddWZrjT\nqLwUeFWSrwCr6f7HBd31SW/sD9P/FN3pLIAPAEuTXA8cQ3cEg6r6FvDFJDcMXnQ+4CN0IXHFQNvb\n6H58rkuyun8tzcTNwOuS3ATsDpwOvILu1NX1wA+Av6H7H9wn++/1ZcAbp9jX2cDfTAyoGFxRVd8G\nbgIeV1VX9W030l3j9+l+vxczs9Nk0nRt7Hf7TOCX+van8sOjc9cBG5J8JclJU+xvyt91bX3eCkVb\nVJKHAf/Zn3ZaRje4whFUmnPibXgkNcJr7rSlHQy8uz9leg/wyjHXI0lS0zxyJ0mS1BCvuZMkSWqI\n4U6SJKkhhjtJkqSGGO4kbXf6uTYrydnjrkWSRs1wJ6kZSZ6Y5K/6+yR+J8n3k3wjyT8meVWSncZd\noyRtad4KRVITkrwFeCvdH61fopsH9rt00yw9HXgf8FvA0nHVKElbg+FO0jYvyR8CpwK3Ay+sqiun\n6HMk3dR1ktQ0T8tK2qb1M0ucAjwAPHeqYAdQVf8EPGcz+9o3yduTrEqyPsn9SW5LsjzJ3lP0T5Jj\nk1ze9/+vJLcn+VSSF0/q++QkH0pya7/f9UmuSfIXSXac1HeHJCckuSLJvUm+l+Sfk/x2P5H75Dp+\nLcnKJOv6fX8jyeeSnLCZfz5JDfLInaRt3Svo5hY+r6pu2FTHqrp/M/t6PvBa4FLgcuD7wP7AbwL/\nM8nSqlo70P804GTgFrr5jr9DNx/szwEvBD4MXbADrgQKuKDvvyvdfMsn0M0p+0Dfd0fgE8ARdHPd\nfhD4L+CZwF8BPw+8fKKAJMcD7wXu7Lf7JrAn8OT+3+Y9m/nMkhpjuJO0rXta/7xyBPv6W+D0ySEw\nya8AF9GFsN8aWPUaYC1wQFV9b9I2ewy8PBZ4KHB0VZ0/qd/uwOC2f0QX7N4NvKGqNvT95gHLgVcm\n+cjAfl5DF0IPrKq7N1GDpO2Ep2UlbesW9M93zHZHVbV2qqN7VfVpYDVd6JrsAWDDFNt8c4q+/zlF\nv29X1Q8A+lOuJ9IdhTtpItj1/TYAb6I7+vfSSbt5sK9jOjVIapxH7iSplyR0wek44EBgd2DeQJfv\nT9rkA3Rh7MYkK4DPAV+qqu9M6vdh4PXAPyT5CHAJ8MWq+pdJ/fYFHgV8Hfjjrpwf85/AkybV8M6+\nhvP6Gr5YVes3+4ElNSlVNe4aJGnGkqwEngX8ZlW9f5rbLKa77u2cqjpuoP104A3AOuAzdKdcJ462\nHQc8rqoy0H8eXbh7Bd01btAdRbsQeFNVrRno+1S6U67PAnbum28GTq2qD/V9DgMum8ZHuLWq9hnY\n9zF01+79HN0ZmaILeb9XVaumsT9JDTHcSdqmJTkVeAvwoar6jWlus5hJ4S7JnnSh7kbgF6rqu5O2\nuRnYdzDcTVq/J931f8voBlP8C7D/FNfv7QQcDBxJFwx3Aw6vqkuSHABcD3y8qp4/nc8yad+7Ab8A\nPA94JXAP8ESP4knbF6+5k7St+39015v9epL9NtVxMzNUPJ7uN/HTUwS7vfv1G1VVd1fVx6rqRXRH\n/Z4AHDBFv/ur6vKqegvwO33zUf3zV+kC2aGTb48yHVV1T1VdWFWvBs6mO8X79GH3I2nbZriTtE2r\nqlvp7nP3E8A/JplyBookzwH+aRO7urV/flp/unViu12AM5l0jXKSnfrTqJPfZ0e6UAX9KNgkv5Bk\n58l96WbP+O9+VfUg3e1OFgDvmmqbJAsGQ2ySZ2bqi/P2HNy3pO2HAyokbfOq6k+T7EA3/diXk1wO\nrALu44fTjy3p2za2jzv7AQnLgGuTfBp4JHA43X3mrgUOGthkZ+CyJGuAq4Hb6G53cjjdgIcLquqm\nvu/vA89K8gW608H30d0/7znAt+lucTLhbXSDOV5Ld2+9iWv/9uw/w2F01+7d2Pf/OHBfkivoAmqA\nX6S7/u5qusEbkrYjXnMnqRlJnkQ3sOCZwGPpwta36ILZR4C/q6r7NzGg4mF0wenFwN7AerqbDr8F\n+CjwSxPX3PVH6E7q32t/uvD1Xbpr7c4Gzqqq7/d9fwV4Cd0NiBfS/WF9B/Ap4J1VddukzxHgZXSD\nOJ4C7NLXcgvdYI2/rarb+76vpbtFy4HAo+mC6G3Ah4AzJp9iltQ+w50kSVJDvOZOkiSpIYY7SZKk\nhhjuJEmSGmK4kyRJaojhTpIkqSGGO0mSpIYY7iRJkhpiuJMkSWqI4U6SJKkh/x+XYvgU/THuAQAA\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa4c35e5710>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# calculate the total number of occurrences of each class\n",
    "y = [len(df[df[2] == i]) for i in ['negative', 'positive', 'neutral']]\n",
    "# X axis\n",
    "objects = ['negative', 'positive', 'neutral']\n",
    "x_pos = range(len(objects))\n",
    "\n",
    "# Draw Diagram\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.bar(x_pos, y, alpha=0.5)\n",
    "plt.xticks(x_pos, objects)\n",
    "plt.ylabel('Occurences').set_size(20)\n",
    "plt.xlabel('Classes').set_size(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interprétation\n",
    "À partir du graphique ci-dessus, nous pouvons clairement noter que la classe «négative» a le moins d'échantillons dans les données par rapport à «positif» et «neutre». Par conséquent, les données semblent déséquilibrées."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7205,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_tweets = list(df[3])\n",
    "labels = df[2]\n",
    "mapper = {'positive': 1, 'negative': -1, 'neutral': 0}\n",
    "labels = labels.map(mapper)\n",
    "labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment140 Score\n",
    "\n",
    "Avant de faire un pré-traitement sur les tweets, nous allons d'abord utiliser le score du Sentiment140 corpus. Ce corpus a le score des mots les plus courants (formels, informels) utilisés dans twitter. Le score est un nombre compris entre [-4.999: 4.999].\n",
    "\n",
    "Le score sera divisé en trois parties:\n",
    "- unigram score  --> 'unigram140_score'\n",
    "- bigram score   --> 'bigram140_score'\n",
    "- pair score     --> 'pair140_score'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7205"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def Sentiment140_dictionary(filename):\n",
    "    sentiment140 = {}\n",
    "    with open(filename) as fin:\n",
    "        line = fin.readline()[:-1]\n",
    "        while line:\n",
    "            line = line.split('\\t')\n",
    "            sentiment140[line[0]] = float(line[1])\n",
    "            line = fin.readline()[:-1]\n",
    "    return sentiment140\n",
    "\n",
    "\n",
    "def unigram140Polarity(tweet, d):\n",
    "    score=0.0\n",
    "    reps = 0\n",
    "    for w in tweet.split(' '):\n",
    "        if w in d.keys():\n",
    "            reps += 1\n",
    "            score+=d[w]\n",
    "    return score, reps\n",
    "\n",
    "unigram140_d = Sentiment140_dictionary('resources/Sentiment140/unigrams-pmilexicon.txt')\n",
    "unigram140Score, unigram140Reps = [], []\n",
    "for tweet in raw_tweets:\n",
    "    score, reps = unigram140Polarity(tweet.lower(), unigram140_d)\n",
    "    unigram140Score.append(score)\n",
    "    unigram140Reps.append(reps)\n",
    "\n",
    "len(unigram140Score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7205"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def find_bigrams(input_list):\n",
    "    bigram_list = []\n",
    "    for i in range(len(input_list)-1):\n",
    "        bigram_list.append(input_list[i] + \" \" + input_list[i+1])\n",
    "    return bigram_list\n",
    "\n",
    "\n",
    "def bigram140Polarity(tweet, d):\n",
    "    score=0.0\n",
    "    reps = 0\n",
    "    tweet = find_bigrams(tweet.split(' '))\n",
    "    for w in tweet:\n",
    "        if w in d.keys():\n",
    "            reps += 1\n",
    "            score+=d[w]\n",
    "    return score, reps\n",
    "\n",
    "\n",
    "bigram140_d = Sentiment140_dictionary('resources/Sentiment140/bigrams-pmilexicon.txt')\n",
    "bigram140Score, bigram140Reps = [], []\n",
    "for tweet in raw_tweets:\n",
    "    score, reps = bigram140Polarity(tweet.lower(), bigram140_d)\n",
    "    bigram140Score.append(score)\n",
    "    bigram140Reps.append(reps)\n",
    "\n",
    "len(bigram140Score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SemEval2015 English lexicon \n",
    "\n",
    "Ce sont les toutes premières et dernières entrées de 'SemEval2015-English-Twitter-Lexicon.txt':\n",
    "- 0.984\tloves\n",
    "- 0.984\t#inspirational\n",
    "- 0.969\tamazing\n",
    "- 0.969\t#peaceful\n",
    "- 0.953\t#greatness\n",
    "- ...\n",
    "- -0.969\tabuse\n",
    "- -0.969\t#failure\n",
    "- -0.982\tkill\n",
    "- -0.984\tbitches\n",
    "- -0.984\t#disappointment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of EnglishLexicon entries 1516\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7205"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def loadSemEval(filename):\n",
    "    f = open(filename,'r')\n",
    "    lexicon = {}\n",
    "    line = f.readline()\n",
    "    while line:\n",
    "        l = line[:-1].split('\\t')\n",
    "        lexicon[l[1]] = float(l[0])\n",
    "        line = f.readline()\n",
    "    return lexicon\n",
    "\n",
    "\n",
    "def SemEvalLexiconPolarity(tweet, EnglishLexicon):\n",
    "    score=0.0\n",
    "    reps = 0\n",
    "    for w in tweet.split(' '):\n",
    "        if w in EnglishLexicon.keys():\n",
    "            reps += 1\n",
    "            score += EnglishLexicon[w]\n",
    "    return score, reps\n",
    "\n",
    "EnglishLexicon = loadSemEval('./resources/SemEval2015-English-Twitter-Lexicon.txt')\n",
    "SemEvalScore, SemEvalReps = [], []\n",
    "for tweet in raw_tweets:\n",
    "    score, reps = SemEvalLexiconPolarity(tweet.lower(), EnglishLexicon)\n",
    "    SemEvalScore.append(score)\n",
    "    SemEvalReps.append(reps)\n",
    "\n",
    "print (\"Number of EnglishLexicon entries %d\" % len(EnglishLexicon.keys()))\n",
    "len(SemEvalScore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# <font color='red'>Pre-entraîner les tweets</font>\n",
    "https://nlp.stanford.edu/IR-book/html/htmledition/determining-the-vocabulary-of-terms-1.html\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Supprimer les slangs des tweets\n",
    "Par (slangs) argot, nous entendons des mots comme:\n",
    "- i've --> I have\n",
    "- 12be --> want to be\n",
    "- *4u  --> kiss for you"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loadSlangs(filename):\n",
    "    \"\"\"\n",
    "    This function reads the file that contains the slangs, and put them in a dictionary such that\n",
    "    the key is the \"slang\" and the value is the acronym.\n",
    "    slangs['i've'] = 'i have'\n",
    "    slang['12be'] = 'want to be'\n",
    "    ...\n",
    "    CAUTION: the keys and values are lower-case letters\n",
    "    \"\"\"\n",
    "    slangs={}\n",
    "    fi=open(filename,'r')\n",
    "    line=fi.readline()\n",
    "    while line:\n",
    "        l=line.split(r',%,')\n",
    "        if len(l) == 2:\n",
    "            slangs[l[0].lower()]=l[1][:-1].lower()  #HERE\n",
    "        line=fi.readline()\n",
    "    fi.close()\n",
    "    return slangs\n",
    "\n",
    "\n",
    "def replaceSlangs(tweet,slangs):\n",
    "    \"\"\"\n",
    "    This function is used to replace the slang in the original tweets and replace them with the acronym.\n",
    "    And it's also returns the the tweet in lower-case letters\n",
    "    \"\"\"\n",
    "    result=''\n",
    "    tweet = tweet.lower()\n",
    "    words=tweet.split()\n",
    "    for w in words:\n",
    "        if w in slangs.keys():\n",
    "            result=result+slangs[w]+\" \"\n",
    "        else:\n",
    "            result=result+w+\" \"\n",
    "    return result.strip()\n",
    "\n",
    "slangs = loadSlangs('./resources/internetSlangs.txt')\n",
    "raw_tweets = [replaceSlangs(tweet, slangs) for tweet in raw_tweets]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remplacer les mots apostrophe\n",
    "\n",
    "Par cela, nous entendons changer des mots comme 'can't', 'cant' en 'can not'. Ces mots sont dans un fichier appelé 'apostrophe_words.txt' qui existait dans le répertoire 'resources'.\n",
    "\n",
    "Nous devons faire cela pour gérer le problème de la négation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_apostrophe_words(filename):\n",
    "    \"\"\"\n",
    "    This function reads the file that contains all words that have apostrophe, and put them in a dictionary \n",
    "    such that the key is the \"word containing apostrophe\" and the value is the \"the word without apostrophe\".\n",
    "    slangs['i've'] = 'i have'\n",
    "    slang['I'm] = 'I am'\n",
    "    ...\n",
    "    CAUTION: the keys and values are lower-case letters\n",
    "    \"\"\"\n",
    "    apo={}\n",
    "    fi=open(filename,'r')\n",
    "    line=fi.readline()\n",
    "    while line:\n",
    "        l=line.split(r',%,')\n",
    "        if len(l) == 2:\n",
    "            apo[l[0].lower()]=l[1][:-1].lower()\n",
    "        line=fi.readline()\n",
    "    fi.close()\n",
    "    return apo\n",
    "\n",
    "\n",
    "def replace_apostrophe(tweet,apos):\n",
    "    result=''\n",
    "    words=tweet.split()\n",
    "    for w in words:\n",
    "        if w in apos.keys():\n",
    "            result=result+apos[w]+\" \"\n",
    "        else:\n",
    "            result=result+w+\" \"\n",
    "    return result.strip()\n",
    "\n",
    "apos = load_apostrophe_words('./resources/apostrophe_words.txt')\n",
    "raw_tweets = [replace_apostrophe(tweet, apos) for tweet in raw_tweets]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Appliquer des techniques de prétraitement standard\n",
    "\n",
    "* https://nlp.stanford.edu/IR-book/html/htmledition/tokenization-1.html\n",
    "* https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utiliser NRC emoticon lexicon\n",
    "\n",
    "Nous remplacerons l'émoticône par sa signification associée."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TT = TweetTokenizer()\n",
    "\n",
    "def emoticondictionary(filename):\n",
    "    \"\"\"\n",
    "    Reads the emoticon file and represents it as dictionary where the emoticon is the key, \n",
    "    and its indication as a value\n",
    "    \"\"\"\n",
    "    emo_scores = {'Positive': 'positive', 'Extremely-Positive': 'positive', \n",
    "                  'Negative': 'negative','Extremely-Negative': 'negative',\n",
    "                  'Neutral': 'neutral'}\n",
    "    emo_score_list = {}\n",
    "    fi = open(filename,\"r\")\n",
    "    l = fi.readline()\n",
    "    while l:\n",
    "        #replace the \"Non-break space\" with the ordinary space \" \"\n",
    "        l = l.replace(\"\\xa0\",\" \") #HERE\n",
    "        li = l.split(\" \")\n",
    "        l2 = li[:-1] #removes the polarity of the emoticon ('negative', 'positive')\n",
    "        l2.append(li[len(li) - 1].split(\"\\t\")[0]) #gets the last emoticon attached to the polarity by '\\t'\n",
    "        sentiment=li[len(li) - 1].split(\"\\t\")[1][:-1] #gets only the polarity, and removes '\\n'\n",
    "        score=emo_scores[sentiment]\n",
    "        l2.append(score)\n",
    "        for i in range(0,len(l2)-1):\n",
    "            emo_score_list[l2[i]]=l2[len(l2)-1]\n",
    "        l=fi.readline()\n",
    "    return emo_score_list\n",
    "\n",
    "dict = emoticondictionary('./resources/emoticon.txt')\n",
    "\n",
    "\n",
    "# substititue emoticon with its associated sentiment\n",
    "def subsEmoticon(tweet,d):\n",
    "    l = TT.tokenize(tweet)\n",
    "    tweet = [d[i] if i in d.keys() else i for i in l]\n",
    "    return tweet\n",
    "\n",
    "\n",
    "raw_tweets = [subsEmoticon(tweet, dict) for tweet in raw_tweets]\n",
    "# print(\":D X3 :|\")\n",
    "# subsEmoticon(\":D X3 :|\", dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gérer la négation\n",
    "\n",
    "Suite au travail de Pang et al (2002), nous définissons un contexte nié comme un segment d'un tweet qui commence par un mot de négation (par exemple, no, never) et se termine par l'un des signes de ponctuation: ',', ' . ',': ','; ','! ','? '.\n",
    "\n",
    "Après avoir manipulé la négation, un tweet comme  'I don't like vegan food' serait 'I do not like_not vegan_not food_not.'.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "negation_words = set(['barely', 'hardly', 'lack', 'never', 'neither', 'no', 'nobody', \\\n",
    "                      'not', 'nothing', 'none', 'nowhere', 'shortage', 'scarcely', 'few', \\\n",
    "                      'low', 'merely', 'nope', 'seldom', 'rarely', 'without', 'zero'])\n",
    "punctuations = [',', '.', ':', ';', '!', '?']\n",
    "\n",
    "def handle_negation(tweet):\n",
    "    output = []\n",
    "    negate = False\n",
    "    for word in tweet:\n",
    "        if word in punctuations and negate:\n",
    "            negate = False\n",
    "        if negate and not word in negation_words:\n",
    "            output.append(word+\"_not\")\n",
    "        else:\n",
    "            output.append(word)\n",
    "        if word in negation_words and not negate:\n",
    "            negate = True\n",
    "        elif word in negation_words and negate:\n",
    "            negate = False\n",
    "    return output\n",
    "\n",
    "raw_tweets = [handle_negation(tweet) for tweet in raw_tweets]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### lemmatizer les mots \n",
    "La lemmatisation ressemble à la conversion du mot 'networks' en 'network'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i love nlp\n",
      "i love nlp\n"
     ]
    }
   ],
   "source": [
    "# Lemmatize the tweets\n",
    "def lemma(tweet):\n",
    "    mmer = WordNetLemmatizer()\n",
    "    #'tweet' here MUST be list like so ['i', 'love', 'nlp']\n",
    "    if type(tweet) == str:\n",
    "        tweet = TT.tokenize(tweet)\n",
    "    \n",
    "    return ' '.join([mmer.lemmatize(word) for word in tweet])\n",
    "\n",
    "print(lemma(['i', 'love', 'nlp']))\n",
    "print(lemma('i love nlp'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On traite ici différents problèmes:\n",
    "- supprime les caractères de ponctuation comme,. :; etc.\n",
    "- supprime les numéros du tweet.\n",
    "- supprime les espaces supplémentaires dans le tweet.\n",
    "- supprime l'occurrence de deux ou plusieurs caractères dans un mot, par exemple. loooong -> loong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def preprocess(tweet):\n",
    "    # delete symbols and URIs and tags\n",
    "    tweet =  ' '.join(re.sub(\"(@[A-Za-z0-9_]+)|([^0-9A-Za-z_# \\t])|(\\w+:\\/\\/\\S+)\", '', tweet).split()) #here _#\n",
    "    # Convert '@username' to 'at_user'\n",
    "    # tweet = re.sub('@[^\\s]+','at_user',tweet)\n",
    "    # remove hashtags\n",
    "    tweet = re.sub(r'#', '', tweet)\n",
    "    # remove numbers\n",
    "    tweet = re.sub('[0-9]', '', tweet)\n",
    "    # remove additional spaces\n",
    "    tweet = re.sub('[\\s]+', ' ', tweet)\n",
    "    # replace the occurrence of 2 or more characters in a word, eg. loooong -> loong\n",
    "    tweet = re.sub(r'(.)\\1{2,}', r'\\1\\1', tweet)\n",
    "    return tweet\n",
    "\n",
    "lemmatized_tweets = [lemma(tweet) for tweet in raw_tweets]\n",
    "preprocessed_tweets = [preprocess(tweet) for tweet in lemmatized_tweets]\n",
    "del lemmatized_tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Supprimer stopwords\n",
    "https://nlp.stanford.edu/IR-book/html/htmledition/dropping-common-terms-stop-words-1.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "358\n",
      "\n",
      "Compare tweets before / after\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>3</th>\n",
       "      <th>final_tweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Gas by my house hit $3.39!!!! I'm going to Cha...</td>\n",
       "      <td>gas house hit going chapel hill sat positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Iranian general says Israel's Iron Dome can't ...</td>\n",
       "      <td>iranian general say israels iron dome deal_not...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>with J Davlar 11th. Main rivals are team Polan...</td>\n",
       "      <td>j davlar th main rival team poland hopefully m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Talking about ACT's &amp;amp;&amp;amp; SAT's, deciding...</td>\n",
       "      <td>talking acts sats deciding want go college app...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>They may have a SuperBowl in Dallas, but Dalla...</td>\n",
       "      <td>may superbowl dallas dallas winning_not superb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Im bringing the monster load of candy tomorrow...</td>\n",
       "      <td>instant message bringing monster load candy to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Apple software, retail chiefs out in overhaul:...</td>\n",
       "      <td>apple software retail chief overhaul san franc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>@oluoch @victor_otti @kunjand I just watched i...</td>\n",
       "      <td>watched sridevis comeback remember sun morning...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>#Livewire Nadal confirmed for Mexican Open in ...</td>\n",
       "      <td>livewire nadal confirmed mexican open february...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>@MsSheLahY I didnt want to just pop up... but ...</td>\n",
       "      <td>didnt want pop yep chapel hill next wednesday ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    3  \\\n",
       "0   Gas by my house hit $3.39!!!! I'm going to Cha...   \n",
       "3   Iranian general says Israel's Iron Dome can't ...   \n",
       "6   with J Davlar 11th. Main rivals are team Polan...   \n",
       "7   Talking about ACT's &amp;&amp; SAT's, deciding...   \n",
       "9   They may have a SuperBowl in Dallas, but Dalla...   \n",
       "10  Im bringing the monster load of candy tomorrow...   \n",
       "11  Apple software, retail chiefs out in overhaul:...   \n",
       "12  @oluoch @victor_otti @kunjand I just watched i...   \n",
       "14  #Livewire Nadal confirmed for Mexican Open in ...   \n",
       "15  @MsSheLahY I didnt want to just pop up... but ...   \n",
       "\n",
       "                                         final_tweets  \n",
       "0        gas house hit going chapel hill sat positive  \n",
       "3   iranian general say israels iron dome deal_not...  \n",
       "6   j davlar th main rival team poland hopefully m...  \n",
       "7   talking acts sats deciding want go college app...  \n",
       "9   may superbowl dallas dallas winning_not superb...  \n",
       "10  instant message bringing monster load candy to...  \n",
       "11  apple software retail chief overhaul san franc...  \n",
       "12  watched sridevis comeback remember sun morning...  \n",
       "14  livewire nadal confirmed mexican open february...  \n",
       "15  didnt want pop yep chapel hill next wednesday ...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend([word+'_not' for word in stop_words]) #negation\n",
    "stop_words = set(stop_words)\n",
    "print (len(stop_words))\n",
    "\n",
    "# remove stopwords\n",
    "def rem_stop(tweet):\n",
    "    words = tweet.split()\n",
    "    tweet = ' '.join([word for word in words if word not in stop_words])\n",
    "    return tweet\n",
    "\n",
    "final_tweets = [rem_stop(tweet) for tweet in preprocessed_tweets]\n",
    "del raw_tweets, preprocessed_tweets\n",
    "\n",
    "print(\"\\nCompare tweets before / after\")\n",
    "df['final_tweets'] = final_tweets\n",
    "df[[3, 'final_tweets']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# <font color='red'>Création de Features</font>\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utilisation de MPQA Lexicon\n",
    "\n",
    "Ce sont les toutes premières et dernières entrées du fichier 'mpqa.txt'\n",
    "- abandoned priorpolarity=negative\n",
    "- abandonment priorpolarity=negative\n",
    "- abandon priorpolarity=negative\n",
    "- abase priorpolarity=negative\n",
    "- abasement priorpolarity=negative\n",
    "- ...\n",
    "- zealot priorpolarity=negative\n",
    "- zealous priorpolarity=negative\n",
    "- zealously priorpolarity=negative\n",
    "- zenith priorpolarity=positive\n",
    "- zest priorpolarity=positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of MPQA words: 13772\n",
      "['watched', 'sridevis', 'positive', 'remember', 'sun', 'morning', 'nta', 'positive']\n",
      "['neutral', 'positive', 'negative']\n",
      "gas house hit going chapel hill sat positive\n",
      "['gas', 'house', 'hit', 'going', 'chapel', 'hill', 'sat', 'positive']\n"
     ]
    }
   ],
   "source": [
    "def MPQAdictionary(filename):\n",
    "    \"\"\"\n",
    "    reads mpqa file which contains the polarity of some of the english words. e.g. 'love': 'positive'\n",
    "    \"\"\"\n",
    "    MPQA_scores = {'priorpolarity=positive\\n': 'positive','priorpolarity=negative\\n': 'negative',\n",
    "                  'priorpolarity=neutral\\n': 'neutral', 'priorpolarity=both\\n': 'neutral'}\n",
    "    MPQA_score_list = {}\n",
    "    fi = open(filename,\"r\")\n",
    "    line = fi.readline()\n",
    "    while line: \n",
    "        li = line.split(\" \")\n",
    "        l2 = li[:-1] # the word as a list\n",
    "        sentiment=li[1] #the word's polarity\n",
    "        score=MPQA_scores[sentiment]\n",
    "        l2.append(score)\n",
    "        for i in range(0,len(l2)-1):\n",
    "            MPQA_score_list[l2[i]]=l2[-1]\n",
    "            # negation\n",
    "            if l2[-1] == 'positive':\n",
    "                MPQA_score_list[l2[i]+'_not']='positive' \n",
    "            else:\n",
    "                MPQA_score_list[l2[i]+'_not']='negative' \n",
    "        line=fi.readline()\n",
    "    return MPQA_score_list\n",
    "\n",
    "\n",
    "def subsMPQA(tweet,d):\n",
    "    tweet = TT.tokenize(tweet)\n",
    "    return [d[i] if i in d.keys() else i for i in tweet]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dictionary = MPQAdictionary('./resources/mpqa/mpqa.txt')\n",
    "print (\"Number of MPQA words: %d\" % len(dictionary.keys()))\n",
    "raw_tweets_MPQA = [subsMPQA(tweet,dictionary) for tweet in final_tweets]\n",
    "\n",
    "# watched sridevis comeback remember sun morning nta positive\n",
    "print (subsMPQA(final_tweets[7], dictionary))\n",
    "print(subsMPQA(\"surprise happy abandoned\", dictionary))\n",
    "\n",
    "print (final_tweets[0])\n",
    "print (raw_tweets_MPQA[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### WordSat Corpus\n",
    "Ce sont les toutes premières et dernières entrées du fichier 'WordSat_pos.txt':\n",
    "- ABIDE\n",
    "- ABIDED\n",
    "- ABIDES\n",
    "- ABIDING\n",
    "- ABILITY\n",
    "- ...\n",
    "- ZENITHS\n",
    "- ZEST\n",
    "- ZESTFULLY\n",
    "- ZESTFULNESS\n",
    "- ZESTS\n",
    "\n",
    "Ce sont les toutes premières et dernières entrées du fichier 'WordSat_neg.txt':\n",
    "- ABANDON\n",
    "- ABASE\n",
    "- ABASED\n",
    "- ABASES\n",
    "- ABATE\n",
    "- ...\n",
    "- YUKKY\n",
    "- ZEALOT\n",
    "- ZEALOTS\n",
    "- ZEALOUS\n",
    "- ZEALOUSLY\n",
    "- ZILCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of positive words 13841\n",
      "Number of negative words 13841\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['positive', 'firas', 'positive']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ENGLISH_WSD_LOCATION = os.path.join('resources/WSD')\n",
    "POS_WORDS_FILE = os.path.join(ENGLISH_WSD_LOCATION, 'WordSat_pos.txt')\n",
    "NEG_WORDS_FILE = os.path.join(ENGLISH_WSD_LOCATION, 'WordSat_neg.txt')\n",
    "\n",
    "pos_words = []\n",
    "neg_words = []\n",
    "\n",
    "for pos_word in open(POS_WORDS_FILE, 'r').readlines():\n",
    "    pos_word = pos_word.split(' ')[0]\n",
    "    if \"_\" not in pos_word:\n",
    "        pos_words.append(pos_word.lower().strip('*'))\n",
    "\n",
    "for neg_word in open(NEG_WORDS_FILE, 'r').readlines():\n",
    "    neg_word = neg_word.split(' ')[0]\n",
    "    if \"_\" not in neg_word:\n",
    "        neg_words.append(neg_word.lower().strip('*'))\n",
    "\n",
    "#negation\n",
    "expanded_pos = copy(pos_words)\n",
    "expanded_pos.extend([word+\"_not\" for word in neg_words])\n",
    "expanded_neg = copy(neg_words)\n",
    "expanded_neg.extend([word+\"_not\" for word in pos_words])\n",
    "\n",
    "#change its type into a set\n",
    "expanded_pos = set(expanded_pos)\n",
    "expanded_neg = set(expanded_neg)\n",
    "\n",
    "#delete unnecessary objects\n",
    "del pos_words, neg_words\n",
    "del ENGLISH_WSD_LOCATION, POS_WORDS_FILE, NEG_WORDS_FILE\n",
    "print (\"Number of positive words %d\" % len(expanded_pos))\n",
    "print (\"Number of negative words %d\" % len(expanded_neg))\n",
    "\n",
    "def subs_pos(tweet, pos_words):\n",
    "    return ['positive' if i in pos_words else i for i in tweet]\n",
    "\n",
    "def subs_neg(tweet, neg_words):\n",
    "    return ['negative' if i in neg_words else i for i in tweet]\n",
    "\n",
    "raw_tweets_wsd = [subs_pos(tweet, expanded_pos) for tweet in raw_tweets_MPQA]\n",
    "raw_tweets_wsd = [subs_neg(tweet, expanded_neg) for tweet in raw_tweets_wsd]\n",
    "\n",
    "subs_pos(\"enjoy firas extraordinarily\".split(' '), expanded_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utilisation de Bing Liu Lexicon\n",
    "Ce sont les toutes premières et dernières entrées du fichier 'positive-words.txt':\n",
    "- a+\n",
    "- abound\n",
    "- abounds\n",
    "- abundance\n",
    "- abundant\n",
    "- ...\n",
    "- youthful\n",
    "- zeal\n",
    "- zenith\n",
    "- zest\n",
    "- zippy\n",
    "\n",
    "Ce sont les toutes premières et dernières entrées du fichier 'negative-words.txt':\n",
    "- 2-faced\n",
    "- 2-faces\n",
    "- abnormal\n",
    "- abolish\n",
    "- abominable\n",
    "- ...\n",
    "- zaps\n",
    "- zealot\n",
    "- zealous\n",
    "- zealously\n",
    "- zombie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of positive words 14676\n",
      "Number of negative words 14676\n"
     ]
    }
   ],
   "source": [
    "ENGLISH_OPINION_LEXICON_LOCATION = os.path.join('resources/opinion-lexicon-English')\n",
    "POS_WORDS_FILE = os.path.join(ENGLISH_OPINION_LEXICON_LOCATION, 'positive-words.txt')\n",
    "NEG_WORDS_FILE = os.path.join(ENGLISH_OPINION_LEXICON_LOCATION, 'negative-words.txt')\n",
    "\n",
    "for pos_word in open(POS_WORDS_FILE, 'r').readlines()[35:]:\n",
    "    word = pos_word.rstrip()\n",
    "    expanded_pos.add(word)\n",
    "    expanded_neg.add(word+\"_not\")  #negation\n",
    "\n",
    "for neg_word in open(NEG_WORDS_FILE, 'r').readlines()[35:]:\n",
    "    word = pos_word.rstrip()\n",
    "    expanded_neg.add(word)\n",
    "    expanded_pos.add(word+\"_not\")  #negation\n",
    "\n",
    "\n",
    "#delete unnecessary objects\n",
    "del raw_tweets_MPQA\n",
    "del ENGLISH_OPINION_LEXICON_LOCATION, POS_WORDS_FILE, NEG_WORDS_FILE\n",
    "\n",
    "print (\"Number of positive words %d\" % len(expanded_pos))\n",
    "print (\"Number of negative words %d\" % len(expanded_neg))\n",
    "\n",
    "\n",
    "raw_tweets_bing = [subs_pos(tweet, expanded_pos) for tweet in raw_tweets_wsd]\n",
    "raw_tweets_bing = [subs_neg(tweet, expanded_neg) for tweet in raw_tweets_bing]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Afinn](https://pypi.python.org/pypi/afinn)\n",
    "\n",
    "Ce sont les toutes premières et dernières entrées de 'afinn.txt':\n",
    "- abandon\t-2\n",
    "- abandoned\t-2\n",
    "- abandons\t-2\n",
    "- abducted\t-2\n",
    "- abduction\t-2\n",
    "- ...\n",
    "- yucky\t-2\n",
    "- yummy\t3\n",
    "- zealot\t-2\n",
    "- zealots\t-2\n",
    "- zealous\t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Afinn entries 4922\n"
     ]
    }
   ],
   "source": [
    "def loadAfinn(filename):\n",
    "    f=open(filename,'r')\n",
    "    afinn={}\n",
    "    line=f.readline()\n",
    "    while line:\n",
    "        if \" \" in line:   #exclude entries like 'cool stuff    3'\n",
    "            pass\n",
    "        else:\n",
    "            l=line[:-1].split('\\t') #line[:-1] removes the '\\r\\n' character\n",
    "            afinn[l[0]]=float(l[1])\n",
    "            afinn[l[0]+\"_not\"] = -float(l[1])  # negation\n",
    "        line=f.readline()\n",
    "\n",
    "    return afinn\n",
    "\n",
    "afinn = loadAfinn('./resources/afinn.txt')\n",
    "print (\"Number of Afinn entries %d\" % len(afinn.keys()))\n",
    "del raw_tweets_wsd\n",
    "\n",
    "def afinnPolarity(tweet, afinn):\n",
    "    score=0.0\n",
    "    reps = 0\n",
    "    for w in tweet:\n",
    "        if w in afinn.keys():\n",
    "            reps += 1\n",
    "            score += afinn[w]\n",
    "    return score, reps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SentiWordNet\n",
    "\n",
    "Voici les cinq premières lignes du fichier csv 'sentiWordnetBig.csv':\n",
    "\n",
    "|POS|ID|PosSCore|NegScore|SynsetTerms|\n",
    "|-|-------|-----|-----|-------------------|\n",
    "|a|1740|0.125|0|able#1|\n",
    "|a|2098|0|0.75|unable#1|\n",
    "|a|2312|0|0|dorsal#2 abaxial#1|\n",
    "|a|2527|0|0|ventral#2 adaxial#1|\n",
    "|a|2730|0|0|acroscopic#1|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening the SentiWordnet file ...\n",
      "Loading...\n",
      "Number of sentiWordnet entries 294612\n"
     ]
    }
   ],
   "source": [
    "def loadSentiWordnet(filename): \n",
    "    output={}\n",
    "    print (\"Opening the SentiWordnet file ...\")\n",
    "    fi=open(filename,\"r\")\n",
    "    line=fi.readline() # ignore the header\n",
    "    line=fi.readline()\n",
    "    print (\"Loading...\")\n",
    "\n",
    "    while line:\n",
    "        l=line.split('\\t')\n",
    "        try:\n",
    "            sentence=l[4]\n",
    "            new = [word for word in sentence.split() if (word[-2] == \"#\" and word[-1].isdigit())]\n",
    "            pos=abs(float(l[2]))\n",
    "            neg=abs(float(l[3]))\n",
    "            neu=float(pos-neg)\n",
    "        except:\n",
    "            line=fi.readline()\n",
    "            continue\n",
    "\n",
    "        for w in new:\n",
    "            output[(w[:-2])]=neu\n",
    "            output[(w[:-2])+'_not'] = -neu   #negation\n",
    "        line=fi.readline()\n",
    "        \n",
    "    fi.close()\n",
    "    return output\n",
    "\n",
    "\n",
    "sentiWordnet = loadSentiWordnet('./resources/sentiWordnetBig.csv')\n",
    "print (\"Number of sentiWordnet entries %d\" % len(sentiWordnet.keys()))\n",
    "\n",
    "\n",
    "\n",
    "def WordnetPolarity(tweet, sentiWordnet):\n",
    "    score=0.0\n",
    "    reps = 0\n",
    "    for w in tweet:\n",
    "        if w in sentiWordnet.keys():\n",
    "            reps += 1\n",
    "            score+=sentiWordnet[w]\n",
    "    return score, reps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SenticNet API\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from senticnet.senticnet import Senticnet\n",
    "\n",
    "def SenticnetPolarity(tweet):\n",
    "    score=0.0\n",
    "    reps = 0\n",
    "    for w in tweet:\n",
    "        try:\n",
    "            score += float(Senticnet().polarity_intense(w))\n",
    "            reps += 1\n",
    "        except KeyError:\n",
    "            pass\n",
    "    return score, reps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calcul de polarité"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7205, 7)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bing_mpqa_score</th>\n",
       "      <th>afinn_score</th>\n",
       "      <th>wordnet_score</th>\n",
       "      <th>sem_eval_score</th>\n",
       "      <th>Senticnet_score</th>\n",
       "      <th>final_score</th>\n",
       "      <th>final_tweets</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.005781</td>\n",
       "      <td>0.005633</td>\n",
       "      <td>-0.003455</td>\n",
       "      <td>0.008469</td>\n",
       "      <td>0.003540</td>\n",
       "      <td>0.013675</td>\n",
       "      <td>gas house hit going chapel hill sat positive</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.011561</td>\n",
       "      <td>0.011266</td>\n",
       "      <td>0.003455</td>\n",
       "      <td>-0.003356</td>\n",
       "      <td>0.015782</td>\n",
       "      <td>0.027025</td>\n",
       "      <td>iranian general say israels iron dome deal_not...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.005781</td>\n",
       "      <td>0.005633</td>\n",
       "      <td>-0.017275</td>\n",
       "      <td>-0.002014</td>\n",
       "      <td>0.003375</td>\n",
       "      <td>0.014828</td>\n",
       "      <td>j davlar th main rival team poland hopefully m...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.010365</td>\n",
       "      <td>0.004899</td>\n",
       "      <td>-0.016398</td>\n",
       "      <td>-0.064760</td>\n",
       "      <td>talking acts sats deciding want go college app...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.005781</td>\n",
       "      <td>-0.005633</td>\n",
       "      <td>-0.008637</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.011845</td>\n",
       "      <td>-0.041825</td>\n",
       "      <td>may superbowl dallas dallas winning_not superb...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-0.005781</td>\n",
       "      <td>-0.005633</td>\n",
       "      <td>-0.017275</td>\n",
       "      <td>0.016953</td>\n",
       "      <td>0.004123</td>\n",
       "      <td>-0.021357</td>\n",
       "      <td>instant message bringing monster load candy to...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.005182</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.008900</td>\n",
       "      <td>-0.014088</td>\n",
       "      <td>apple software retail chief overhaul san franc...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.011561</td>\n",
       "      <td>0.011266</td>\n",
       "      <td>0.005182</td>\n",
       "      <td>0.006484</td>\n",
       "      <td>0.014082</td>\n",
       "      <td>0.059524</td>\n",
       "      <td>watched sridevis comeback remember sun morning...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.011561</td>\n",
       "      <td>0.011266</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009598</td>\n",
       "      <td>0.011402</td>\n",
       "      <td>0.028304</td>\n",
       "      <td>livewire nadal confirmed mexican open february...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.011561</td>\n",
       "      <td>0.011266</td>\n",
       "      <td>-0.001727</td>\n",
       "      <td>0.018295</td>\n",
       "      <td>-0.002170</td>\n",
       "      <td>0.075563</td>\n",
       "      <td>didnt want pop yep chapel hill next wednesday ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    bing_mpqa_score  afinn_score  wordnet_score  sem_eval_score  \\\n",
       "0          0.005781     0.005633      -0.003455        0.008469   \n",
       "3          0.011561     0.011266       0.003455       -0.003356   \n",
       "6          0.005781     0.005633      -0.017275       -0.002014   \n",
       "7          0.000000     0.000000      -0.010365        0.004899   \n",
       "9         -0.005781    -0.005633      -0.008637        0.000000   \n",
       "10        -0.005781    -0.005633      -0.017275        0.016953   \n",
       "11         0.000000     0.000000      -0.005182        0.000000   \n",
       "12         0.011561     0.011266       0.005182        0.006484   \n",
       "14         0.011561     0.011266       0.000000        0.009598   \n",
       "15         0.011561     0.011266      -0.001727        0.018295   \n",
       "\n",
       "    Senticnet_score  final_score  \\\n",
       "0          0.003540     0.013675   \n",
       "3          0.015782     0.027025   \n",
       "6          0.003375     0.014828   \n",
       "7         -0.016398    -0.064760   \n",
       "9         -0.011845    -0.041825   \n",
       "10         0.004123    -0.021357   \n",
       "11        -0.008900    -0.014088   \n",
       "12         0.014082     0.059524   \n",
       "14         0.011402     0.028304   \n",
       "15        -0.002170     0.075563   \n",
       "\n",
       "                                         final_tweets         2  \n",
       "0        gas house hit going chapel hill sat positive  positive  \n",
       "3   iranian general say israels iron dome deal_not...  negative  \n",
       "6   j davlar th main rival team poland hopefully m...  positive  \n",
       "7   talking acts sats deciding want go college app...  negative  \n",
       "9   may superbowl dallas dallas winning_not superb...  negative  \n",
       "10  instant message bringing monster load candy to...   neutral  \n",
       "11  apple software retail chief overhaul san franc...   neutral  \n",
       "12  watched sridevis comeback remember sun morning...  positive  \n",
       "14  livewire nadal confirmed mexican open february...   neutral  \n",
       "15  didnt want pop yep chapel hill next wednesday ...  positive  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BingMpqaScore = []\n",
    "AfinnScore, AfinnReps = [], []\n",
    "WordnetScore, WordnetReps = [], []\n",
    "SenticnetScore, SenticnetReps = [], []\n",
    "length = len(raw_tweets_bing)\n",
    "\n",
    "for tw in raw_tweets_bing:\n",
    "    Bing_MPQA = 0\n",
    "    for word in tw:\n",
    "        if (word == 'positive'):\n",
    "            Bing_MPQA +=  1\n",
    "        if (word == 'negative'):\n",
    "            Bing_MPQA -= 1\n",
    "    BingMpqaScore.append(Bing_MPQA)\n",
    "    tmp = afinnPolarity(tw, afinn)\n",
    "    AfinnScore.append(tmp[0])\n",
    "    AfinnReps.append(tmp[1])\n",
    "    tmp = WordnetPolarity(tw, sentiWordnet)\n",
    "    WordnetScore.append(tmp[0])\n",
    "    WordnetReps.append(tmp[1])\n",
    "    tmp = SenticnetPolarity(tw)\n",
    "    SenticnetScore.append(tmp[0])\n",
    "    SenticnetReps.append(tmp[1])\n",
    "\n",
    "    \n",
    "#reshape\n",
    "BingMpqaScore = np.array(BingMpqaScore).reshape(length, 1)\n",
    "AfinnScore = np.array(AfinnScore).reshape(length, 1)\n",
    "AfinnReps = np.array(AfinnReps).reshape(length, 1)\n",
    "WordnetScore = np.array(WordnetScore).reshape(length, 1)\n",
    "WordnetReps = np.array(WordnetReps).reshape(length, 1)\n",
    "SemEvalScore = np.array(SemEvalScore).reshape(length, 1)\n",
    "SemEvalReps = np.array(SemEvalReps).reshape(length, 1)\n",
    "SenticnetScore = np.array(SenticnetScore).reshape(length, 1)\n",
    "SenticnetReps = np.array(SenticnetReps).reshape(length, 1)\n",
    "unigram140Score = np.array(unigram140Score).reshape(length, 1)\n",
    "unigram140Reps = np.array(unigram140Reps).reshape(length, 1)\n",
    "bigram140Score = np.array(bigram140Score).reshape(length, 1)\n",
    "bigram140Reps = np.array(bigram140Reps).reshape(length, 1)\n",
    "\n",
    "#Normalization\n",
    "BingMpqaScore = BingMpqaScore/np.linalg.norm(BingMpqaScore)\n",
    "AfinnScore = AfinnScore/np.linalg.norm(AfinnScore)\n",
    "AfinnReps = AfinnReps/np.linalg.norm(AfinnReps)\n",
    "WordnetScore = WordnetScore/np.linalg.norm(WordnetScore)\n",
    "WordnetReps = WordnetReps/np.linalg.norm(WordnetReps)\n",
    "SemEvalScore = SemEvalScore/np.linalg.norm(SemEvalScore)\n",
    "SemEvalReps = SemEvalReps/np.linalg.norm(SemEvalReps)\n",
    "SenticnetScore = SenticnetScore/np.linalg.norm(SenticnetScore)\n",
    "SenticnetReps = SenticnetReps/np.linalg.norm(SenticnetReps)\n",
    "unigram140Score = unigram140Score/np.linalg.norm(unigram140Score)\n",
    "unigram140Reps = unigram140Reps/np.linalg.norm(unigram140Reps)\n",
    "bigram140Score = bigram140Score/np.linalg.norm(bigram140Score)\n",
    "bigram140Reps = bigram140Reps/np.linalg.norm(bigram140Reps)\n",
    "\n",
    "\n",
    "\n",
    "#final_score_tweets (my score list)\n",
    "df['bing_mpqa_score'] = BingMpqaScore\n",
    "df['afinn_score'] = AfinnScore\n",
    "df['wordnet_score'] = WordnetScore\n",
    "df['sem_eval_score'] = SemEvalScore\n",
    "df['Senticnet_score'] = SenticnetScore\n",
    "all_scores = np.hstack( (BingMpqaScore, AfinnScore, WordnetScore, SemEvalScore, SenticnetScore, \\\n",
    "                                                                         unigram140Score, bigram140Score) )\n",
    "sum_score = np.sum(all_scores, axis=1).reshape(length, 1)\n",
    "print (all_scores.shape)\n",
    "df['final_score'] = sum_score\n",
    "\n",
    "df[['bing_mpqa_score','afinn_score', 'wordnet_score', 'sem_eval_score', 'Senticnet_score', \\\n",
    "                                'final_score', 'final_tweets' ,2]].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construire un modèle NN avec des cellules LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from keras.models import Model, Sequential\n",
    "# from keras.layers import Dense, Input, Dropout, LSTM, Activation\n",
    "# from keras.preprocessing import sequence\n",
    "# from keras.initializers import glorot_uniform\n",
    "# from keras.layers.embeddings import Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tout d'abord, nous allons lire le pre-trained  [Global Vectors for Word Representation Embedding](https://nlp.stanford.edu/projects/glove/) réalisés par le personnel de stanford Jeffrey Pennington, Richard Socher, Christopher D. Manning. Plus précisément, le 'glove.6B.50d' qui contient 400 000 mots chaque mot a un vecteur 50 dimensions. Le fichier glove.6B.50d.txt se trouve dans le répertoire 'resources /'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_glove_vecs(glove_file):\n",
    "    with open(glove_file, 'r') as f:\n",
    "        words = set()\n",
    "        word_to_vec_map = {}\n",
    "        for line in f:\n",
    "            line = line.strip().split()\n",
    "            curr_word = line[0]\n",
    "            words.add(curr_word)\n",
    "            word_to_vec_map[curr_word] = np.array(line[1:], dtype=np.float64)\n",
    "        \n",
    "        i = 1\n",
    "        words_to_index = {}\n",
    "        index_to_words = {}\n",
    "        for w in sorted(words):\n",
    "            words_to_index[w] = i\n",
    "            index_to_words[i] = w\n",
    "            i = i + 1\n",
    "    return words_to_index, index_to_words, word_to_vec_map\n",
    "\n",
    "\n",
    "word_to_index, index_to_word, word_to_vec_map = read_glove_vecs('resources/glove.6B.50d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400000, 400000, 400000, 50)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_to_index.keys()), len(index_to_word.keys()), len(word_to_vec_map.keys()), len(word_to_vec_map['love'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# #max_len represents the maximum tweet length in our corpus\n",
    "# max_len = max([len(tweet) for tweet in raw_tweets_bing])\n",
    "# max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def pretrained_embedding_layer():\n",
    "#     \"\"\"\n",
    "#     Creates a Keras Embedding() layer and loads in pre-trained GloVe 50-dimensional vectors.\n",
    "#     And it returns the pretrained embedding_layer layer Keras instance\n",
    "#     \"\"\"\n",
    "    \n",
    "#     vocab_len = len(word_to_index) + 1          # adding 1 to fit Keras embedding (requirement) (= 400,001)\n",
    "#     emb_dim = 50                                # define dimensionality of your GloVe word vectors (= 50)\n",
    "    \n",
    "#     # Initialize the embedding matrix as a numpy array of zeros of shape\n",
    "#     emb_matrix = np.zeros((vocab_len, emb_dim))\n",
    "    \n",
    "#     # Set each row \"index\" of the embedding matrix to be the word vector representation\n",
    "#     # of the \"index\"th word of the vocabulary\n",
    "#     for word, index in word_to_index.items():\n",
    "#         emb_matrix[index, :] = word_to_vec_map[word]\n",
    "\n",
    "#     # Define Keras embedding layer with the correct output/input sizes, make it non-trainable.\n",
    "#     embedding_layer = Embedding(input_dim=vocab_len, output_dim=emb_dim, trainable=False)\n",
    "\n",
    "#     # Build the embedding layer, it is required before setting the weights of the embedding layer.\n",
    "#     # Do not modify the \"None\".\n",
    "#     embedding_layer.build((None,))\n",
    "    \n",
    "#     # Set the weights of the embedding layer to the embedding matrix. Your layer is now pretrained.\n",
    "#     embedding_layer.set_weights([emb_matrix])\n",
    "    \n",
    "#     return embedding_layer\n",
    "\n",
    "# embedding_layer = pretrained_embedding_layer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def create_model():\n",
    "#     \"\"\"\n",
    "#     Creating the model's graph.\n",
    "    \n",
    "#     Arguments:\n",
    "#     input_shape -- shape of the input, usually (max_len,)\n",
    "#     word_to_vec_map -- dictionary mapping every word in a vocabulary into its 50-dimensional vector representation\n",
    "#     word_to_index -- dictionary mapping from words to their indices in the vocabulary (400,001 words)\n",
    "\n",
    "#     Returns:\n",
    "#     model -- a model instance in Keras\n",
    "#     \"\"\"\n",
    "#     input_shape = (max_len,)\n",
    "#     # Define sentence_indices as the input of the graph,\n",
    "#     sentence_indices = Input(shape=input_shape, dtype='int32')\n",
    "    \n",
    "#     # Create the embedding layer pretrained with GloVe Vectors\n",
    "#     embedding_layer = pretrained_embedding_layer()\n",
    "    \n",
    "#     # Propagate sentence_indices through your embedding layer, you get back the embeddings\n",
    "#     embeddings = embedding_layer(sentence_indices)\n",
    "    \n",
    "#     # Using an LSTM layer with 128-dimensional hidden state (returned output is a batch)\n",
    "#     X = LSTM(128, return_sequences=True)(embeddings)\n",
    "#     # Add dropout with a probability of 0.5\n",
    "#     X = Dropout(0.5)(X)\n",
    "#     # Using another LSTM layer with 128-dimensional hidden state (returned output is a single hidden state)\n",
    "#     X = LSTM(128)(X)\n",
    "#     # Add dropout with a probability of 0.5\n",
    "#     X = Dropout(0.5)(X)\n",
    "#     # Adding a Dense layer with softmax activation to get back a batch of 3-dimensional vectors.\n",
    "#     X = Dense(3, activation='softmax')(X)\n",
    "#     # Add a softmax function as activation\n",
    "#     X = Activation('softmax')(X)\n",
    "    \n",
    "#     # Create Model instance which converts sentence_indices into X.\n",
    "#     model = Model(inputs=sentence_indices, outputs=X)\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def create_sequential_model():\n",
    "#     input_shape = (max_len,)\n",
    "#     sentence_indices = Input(shape=input_shape, dtype='int32')\n",
    "#     embedding_layer = pretrained_embedding_layer()\n",
    "#     embeddings = embedding_layer(sentence_indices)\n",
    "#     # Using an LSTM layer with 128-dimensional hidden state (returned output is a batch)\n",
    "#     X = LSTM(32, recurrent_dropout=0.2, dropout=0.2)(embeddings)\n",
    "#     # Add dropout with a probability of 0.5\n",
    "#     X = Dense(32,activation='relu')(X)\n",
    "#     # Using another LSTM layer with 128-dimensional hidden state (returned output is a single hidden state)\n",
    "#     X = Dropout(0.2)(X)\n",
    "#     # Add dropout with a probability of 0.5\n",
    "#     X = Dropout(0.5)(X)\n",
    "#     # Adding a Dense layer with softmax activation to get back a batch of 3-dimensional vectors.\n",
    "#     X = Dense(3, activation='softmax')(X)\n",
    "    \n",
    "#     model = Sequential(inputs=sentence_indices, outputs=X)\n",
    "#     return model\n",
    "\n",
    "# model = create_model(len(x_train[0]),len(y_train[0]))\n",
    "\n",
    "# print ('Fitting model...')\n",
    "# estimator = model.fit(x_train, y_train, batch_size=64, epochs=20, validation_split = 0.1, verbose = 1)\n",
    "# print(\"Training accuracy: %.2f%% / Validation accuracy: %.2f%%\" % (estimator.history['acc'][-1], estimator.history['val_acc'][-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# model = create_model()\n",
    "# # model = create_sequential_model()\n",
    "# model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def sentences_to_indices(training_tweets):\n",
    "#     \"\"\"\n",
    "#     Converts an array of sentences (strings) into an array of indices corresponding to words in the sentences.\n",
    "#     The output shape should be such that it can be given to `Embedding()`. And it returns array of indices \n",
    "#     corresponding to words in the sentences.\n",
    "#     \"\"\"\n",
    "    \n",
    "#     m = len(training_tweets)                      # number of training examples (should be 7205)\n",
    "        \n",
    "#     # Initialize 'indices' as a numpy matrix of zeros\n",
    "#     indices = np.zeros((m, max_len))\n",
    "        \n",
    "#     for i in range(m):\n",
    "#         for j, w in enumerate(training_tweets[i]):\n",
    "#             # Set the (i,j)th entry of 'indices' to the index of the correct word.\n",
    "#             try:\n",
    "#                 indices[i, j] = word_to_index[w]\n",
    "#             # If the word 'w' doesn't exist in our dictionary, set its index into 0\n",
    "#             except KeyError:\n",
    "#                 indices[i, j] = 0\n",
    "    \n",
    "#     return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def convert_to_one_hot(Y, C):\n",
    "#     Y = np.eye(C)[Y.values.reshape(-1)]\n",
    "#     return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# X_train_indices = sentences_to_indices(raw_tweets_bing)\n",
    "# Y_train_oh = convert_to_one_hot(labels, C = 3)\n",
    "# print (X_train_indices.shape)\n",
    "# print (Y_train_oh.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#execution en 60 minutes\n",
    "# model.fit(X_train_indices, Y_train_oh, epochs = 20, batch_size = 32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def sentence_to_avg(train_indices, weights):\n",
    "#     \"\"\"\n",
    "#     This function returns the average vector encoding information about the sentence as \n",
    "#     numpy-array of shape (50,)\n",
    "#     \"\"\"\n",
    "#     # Initializing the average word vector.\n",
    "#     length = len(train_indices)\n",
    "#     avg = np.zeros((length, 50))\n",
    "    \n",
    "#     # Averaging the word vectors.\n",
    "#     for i in range(length):\n",
    "#         for idx in train_indices[i]:\n",
    "#             avg[i, :] += weights[int(idx), :]\n",
    "#         avg[i, :] /= len(train_indices[i])\n",
    "    \n",
    "#     return avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7205, 50)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# word_embedding_features = sentence_to_avg(X_train_indices, model.get_weights()[0])\n",
    "# word_embedding_features.shape\n",
    "\n",
    "### DELETE THE FOLLOWING LINES IF YOU WANT TO CREATE AN NN MODEL\n",
    "avg = np.zeros((len(raw_tweets_bing), 50))\n",
    "\n",
    "for i, tweet in enumerate(raw_tweets_bing):\n",
    "    # Averaging the word vectors.\n",
    "    for word in tweet:\n",
    "        try:\n",
    "            avg[i, :] += word_to_vec_map[word]\n",
    "        except KeyError:\n",
    "            pass\n",
    "    avg[i, :] /= len(tweet)\n",
    "    \n",
    "word_embedding_features = avg\n",
    "word_embedding_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_embedding_features = np.array(word_embedding_features).reshape(length, 50)\n",
    "word_embedding_features = word_embedding_features/np.linalg.norm(word_embedding_features)\n",
    "print(word_embedding_features.shape)\n",
    "word_embedding_features = scipy.sparse.csr_matrix(word_embedding_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "del df, raw_tweets_bing\n",
    "# del X_train_indices, Y_train_oh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## <font color='red'>Entraîner le modèle</font>\n",
    "***\n",
    "#### Créer le feature vector\n",
    "* See [Bag of Words](https://en.wikipedia.org/wiki/Bag-of-words_model) for more details (Supprimer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7205, 139521)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vectorizer = CountVectorizer(analyzer='word', preprocessor=None, stop_words=None, tokenizer=None, ngram_range=(1,3))\n",
    "features = count_vectorizer.fit_transform(final_tweets)\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(7205, 5)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#reducing the CountVector\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "svd = TruncatedSVD(n_components=5, n_iter=7, random_state=42)\n",
    "count_features = svd.fit_transform(features)\n",
    "count_features = scipy.sparse.csr_matrix(count_features)\n",
    "print (type(count_features))\n",
    "count_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7205, 139521)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(analyzer='word', preprocessor=None, stop_words=None, tokenizer=None, ngram_range=(1,3))\n",
    "tfidf_features = tfidf_vectorizer.fit_transform(final_tweets)\n",
    "del final_tweets\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7205, 7) (7205, 1) (7205, 1) (7205, 1) (7205, 1)\n",
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "(7205, 14)\n"
     ]
    }
   ],
   "source": [
    "print (all_scores.shape, sum_score.shape, AfinnReps.shape, WordnetReps.shape, SemEvalReps.shape)\n",
    "final_total = scipy.sparse.csr_matrix(np.hstack( (all_scores, sum_score, AfinnReps, \\\n",
    "                                        SenticnetReps, WordnetReps, SemEvalReps, unigram140Reps, bigram140Reps) ))\n",
    "print (type(final_total))\n",
    "print (final_total.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7205, 139590)\n"
     ]
    }
   ],
   "source": [
    "features = scipy.sparse.hstack([word_embedding_features, count_features, tfidf_features, final_total])\n",
    "print (features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# supprimer les objets inutiles\n",
    "del word_embedding_features\n",
    "del all_scores, sum_score, BingMpqaScore, AfinnScore, WordnetScore, SemEvalScore, SenticnetScore, \\\n",
    "    unigram140Score, bigram140Score\n",
    "del AfinnReps, WordnetReps, SemEvalReps, unigram140Reps, bigram140Reps\n",
    "del count_features, tfidf_features, final_total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importer SVM\n",
    "\n",
    "http://scikit-learn.org/stable/modules/svm.html\n",
    "\n",
    "Pour un aperçu mathématique,\n",
    "https://docs.opencv.org/2.4/doc/tutorials/ml/introduction_to_svm/introduction_to_svm.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RandomizedSearchCV pour avoir l'hyper-param optimal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anwar/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/anwar/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/anwar/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/anwar/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.597321544509\n",
      "20\n",
      "{'C': 2.0682278445000044, 'gamma': 0.44497903297078989}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x7fa4ba861c88>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAEKCAYAAAARnO4WAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nA/CKLRJ+Q4GOAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa4ba859b70>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Takes around 30 minutes\n",
    "rand_list = {\"C\": scipy.stats.uniform(0, 5),\n",
    "             \"gamma\": scipy.stats.uniform(0.1, 1)}\n",
    "\n",
    "rand_search = RandomizedSearchCV(SVC(kernel='linear'), param_distributions = rand_list,\\\n",
    "                                 n_iter = 20, n_jobs = 4, scoring = 'f1_macro')\n",
    "rand_search.fit(features, labels)\n",
    "\n",
    "print(rand_search.best_score_)\n",
    "print (len(rand_search.cv_results_['param_C'].data))\n",
    "print (rand_search.best_params_)\n",
    "\n",
    "rand_search.cv_results_['param_gamma'].data\n",
    "plt.plot(sorted(rand_search.cv_results_['mean_test_score']), rand_search.cv_results_['param_C'].data, \"-\")\n",
    "plt.xlabel(\"Mean Test Score\")\n",
    "plt.ylabel(\"C value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=2.0682278445000044, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape=None, degree=3, gamma=0.44497903297078989,\n",
       "  kernel='linear', max_iter=-1, probability=False, random_state=None,\n",
       "  shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "KERNEL = 'linear'\n",
    "classifier = SVC(kernel=KERNEL, C=rand_search.best_params_['C'], gamma=rand_search.best_params_['gamma'])\n",
    "classifier.fit(features, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prédiction sur les données d'apprentissage "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9950\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "nb_predict_train = classifier.predict(features)\n",
    "#check accuracy\n",
    "print(\"Accuracy: {:0.4f}\".format(metrics.accuracy_score(labels, nb_predict_train)))\n",
    "del features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2674    1   17]\n",
      " [   2 1008   14]\n",
      " [   2    0 3487]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       1.00      0.99      1.00      2692\n",
      "         -1       1.00      0.98      0.99      1024\n",
      "          0       0.99      1.00      1.00      3489\n",
      "\n",
      "avg / total       1.00      1.00      1.00      7205\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#print confusion matrix\n",
    "print(\"{}\".format(metrics.confusion_matrix(labels, nb_predict_train, \n",
    "                                           labels=[1,-1, 0])))\n",
    "\n",
    "print(\"{}\".format(metrics.classification_report(labels, nb_predict_train, \n",
    "                                                labels=[1, -1, 0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prédire en utilisant le modèle\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importer test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8902, 4)\n"
     ]
    }
   ],
   "source": [
    "t_df = pd.read_csv('./data/test/actual/test_B_labeled.tsv', sep='\\t', header=None)\n",
    "print(t_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7584, 4)\n",
      "(7584,)\n"
     ]
    }
   ],
   "source": [
    "t_df = t_df[t_df[3] != 'Not Available']\n",
    "actual_labels = t_df[2]\n",
    "actual_labels = actual_labels.map(mapper)\n",
    "print(t_df.shape)\n",
    "print(actual_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAnMAAAHoCAYAAAAi+WkTAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa4a37d68d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y = [len(t_df[t_df[2] == i]) for i in ['positive', 'negative', 'neutral']]\n",
    "x = ['positive', 'negative', 'neutral']\n",
    "x_pos = range(len(x))\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.bar(x_pos, y, alpha=0.5)\n",
    "plt.xticks(x_pos, x)\n",
    "plt.ylabel('# Occurences').set_size(15)\n",
    "plt.xlabel('Classes').set_size(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pré-traiter les tweets de l'ensemble de données de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_tweets_test = t_df[3]\n",
    "\n",
    "t_unigram140Score, t_unigram140Reps = [], []\n",
    "for tweet in raw_tweets_test:\n",
    "    score, reps = unigram140Polarity(tweet.lower(), unigram140_d)\n",
    "    t_unigram140Score.append(score)\n",
    "    t_unigram140Reps.append(reps)\n",
    "\n",
    "t_bigram140Score, t_bigram140Reps = [], []\n",
    "for tweet in raw_tweets_test:\n",
    "    score, reps = bigram140Polarity(tweet.lower(), bigram140_d)\n",
    "    t_bigram140Score.append(score)\n",
    "    t_bigram140Reps.append(reps)\n",
    "\n",
    "t_SemEvalScore, t_SemEvalReps = [], []\n",
    "for tweet in raw_tweets_test:\n",
    "    score, reps = SemEvalLexiconPolarity(tweet.lower(), EnglishLexicon)\n",
    "    t_SemEvalScore.append(score)\n",
    "    t_SemEvalReps.append(reps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_tweets_test = [replaceSlangs(tweet, slangs) for tweet in raw_tweets_test]\n",
    "raw_tweets_test = [replace_apostrophe(tweet, apos) for tweet in raw_tweets_test]\n",
    "raw_tweets_test = [subsEmoticon(tweet, dict) for tweet in raw_tweets_test]\n",
    "raw_tweets_test = [handle_negation(tweet) for tweet in raw_tweets_test] #negation\n",
    "lemmatized_tweets_test = [lemma(tweet) for tweet in raw_tweets_test]\n",
    "preprocessed_tweets_test = [preprocess(tweet) for tweet in lemmatized_tweets_test]\n",
    "final_tweets_test = [rem_stop(tweet) for tweet in preprocessed_tweets_test]\n",
    "t_df[3] = final_tweets_test\n",
    "\n",
    "del raw_tweets_test, lemmatized_tweets_test, preprocessed_tweets_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done scoring test set ..\n",
      "Done reshaping test set ..\n",
      "Done normalizing test set ..\n",
      "(7584, 7)\n"
     ]
    }
   ],
   "source": [
    "t_raw_tweets_MPQA = [subsMPQA(tweet,dictionary) for tweet in final_tweets_test]\n",
    "t_raw_tweets_wsd = [subs_pos(tweet, expanded_pos) for tweet in t_raw_tweets_MPQA]\n",
    "t_raw_tweets_wsd = [subs_neg(tweet, expanded_neg) for tweet in t_raw_tweets_wsd]\n",
    "t_raw_tweets_bing = [subs_pos(tweet, expanded_pos) for tweet in t_raw_tweets_wsd]\n",
    "t_raw_tweets_bing = [subs_neg(tweet, expanded_neg) for tweet in t_raw_tweets_bing]\n",
    "\n",
    "t_BingMpqaScore = []\n",
    "t_AfinnScore, t_AfinnReps = [], []\n",
    "t_WordnetScore, t_WordnetReps = [], []\n",
    "t_SenticnetScore, t_SenticnetReps = [], []\n",
    "t_length = len(t_raw_tweets_bing)\n",
    "\n",
    "for tw in t_raw_tweets_bing:\n",
    "    Bing_MPQA = 0\n",
    "    for i in tw:\n",
    "        if (i == 'positive'):\n",
    "            Bing_MPQA +=  1\n",
    "        if (i == 'negative'):\n",
    "            Bing_MPQA -= 1\n",
    "    t_BingMpqaScore.append(Bing_MPQA)\n",
    "    tmp = afinnPolarity(tw, afinn)\n",
    "    t_AfinnScore.append(tmp[0])\n",
    "    t_AfinnReps.append(tmp[1])\n",
    "    tmp = WordnetPolarity(tw, sentiWordnet)\n",
    "    t_WordnetScore.append(tmp[0])\n",
    "    t_WordnetReps.append(tmp[1])\n",
    "    tmp = SenticnetPolarity(tw)\n",
    "    t_SenticnetScore.append(tmp[0])\n",
    "    t_SenticnetReps.append(tmp[1])\n",
    "print(\"Done scoring test set ..\")\n",
    "    \n",
    "#reshape\n",
    "t_BingMpqaScore = np.array(t_BingMpqaScore).reshape(t_length, 1)\n",
    "t_AfinnScore = np.array(t_AfinnScore).reshape(t_length, 1)\n",
    "t_AfinnReps = np.array(t_AfinnReps).reshape(t_length, 1)\n",
    "t_WordnetScore = np.array(t_WordnetScore).reshape(t_length, 1)\n",
    "t_WordnetReps = np.array(t_WordnetReps).reshape(t_length, 1)\n",
    "t_SemEvalScore = np.array(t_SemEvalScore).reshape(t_length, 1)\n",
    "t_SemEvalReps = np.array(t_SemEvalReps).reshape(t_length, 1)\n",
    "t_SenticnetScore = np.array(t_SenticnetScore).reshape(t_length, 1)\n",
    "t_SenticnetReps = np.array(t_SenticnetReps).reshape(t_length, 1)\n",
    "t_unigram140Score = np.array(t_unigram140Score).reshape(t_length, 1)\n",
    "t_unigram140Reps = np.array(t_unigram140Reps).reshape(t_length, 1)\n",
    "t_bigram140Score = np.array(t_bigram140Score).reshape(t_length, 1)\n",
    "t_bigram140Reps = np.array(t_bigram140Reps).reshape(t_length, 1)\n",
    "print(\"Done reshaping test set ..\")\n",
    "\n",
    "#Normalization\n",
    "t_BingMpqaScore = t_BingMpqaScore/np.linalg.norm(t_BingMpqaScore)\n",
    "t_AfinnScore = t_AfinnScore/np.linalg.norm(t_AfinnScore)\n",
    "t_AfinnReps = t_AfinnReps/np.linalg.norm(t_AfinnReps)\n",
    "t_WordnetScore = t_WordnetScore/np.linalg.norm(t_WordnetScore)\n",
    "t_WordnetReps = t_WordnetReps/np.linalg.norm(t_WordnetReps)\n",
    "t_SemEvalScore = t_SemEvalScore/np.linalg.norm(t_SemEvalScore)\n",
    "t_SemEvalReps = t_SemEvalReps/np.linalg.norm(t_SemEvalReps)\n",
    "t_SenticnetScore = t_SenticnetScore/np.linalg.norm(t_SenticnetScore)\n",
    "t_SenticnetReps = t_SenticnetReps/np.linalg.norm(t_SenticnetReps)\n",
    "t_unigram140Score = t_unigram140Score/np.linalg.norm(t_unigram140Score)\n",
    "t_unigram140Reps = t_unigram140Reps/np.linalg.norm(t_unigram140Reps)\n",
    "t_bigram140Score = t_bigram140Score/np.linalg.norm(t_bigram140Score)\n",
    "t_bigram140Reps = t_bigram140Reps/np.linalg.norm(t_bigram140Reps)\n",
    "print(\"Done normalizing test set ..\")\n",
    "\n",
    "t_all_scores = np.hstack( (t_BingMpqaScore, t_AfinnScore, t_WordnetScore, t_SemEvalScore, t_SenticnetScore, \\\n",
    "                                               t_unigram140Score, t_bigram140Score) )\n",
    "t_sum_score = np.sum(t_all_scores, axis=1).reshape(t_length, 1)\n",
    "print (t_all_scores.shape)\n",
    "\n",
    "# Delete\n",
    "del t_raw_tweets_MPQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>282031301962395648</td>\n",
       "      <td>T14111200</td>\n",
       "      <td>neutral</td>\n",
       "      <td>dec st know end_not world_not baby_not boom_no...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11975</td>\n",
       "      <td>SM112166</td>\n",
       "      <td>negative</td>\n",
       "      <td>yar quite clever aft many guess lor got ask br...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>136592</td>\n",
       "      <td>LJ112295</td>\n",
       "      <td>negative</td>\n",
       "      <td>yeah thin lizzy hate informercial</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>253421252956545024</td>\n",
       "      <td>T13114433</td>\n",
       "      <td>neutral</td>\n",
       "      <td>mt syria deir ezzor ali bashar altheeb wa mart...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>220880422320603137</td>\n",
       "      <td>T14114138</td>\n",
       "      <td>negative</td>\n",
       "      <td>hate life see_not roskilde_not festival_not sa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    0          1         2  \\\n",
       "2  282031301962395648  T14111200   neutral   \n",
       "3               11975   SM112166  negative   \n",
       "4              136592   LJ112295  negative   \n",
       "5  253421252956545024  T13114433   neutral   \n",
       "6  220880422320603137  T14114138  negative   \n",
       "\n",
       "                                                   3  \n",
       "2  dec st know end_not world_not baby_not boom_no...  \n",
       "3  yar quite clever aft many guess lor got ask br...  \n",
       "4                  yeah thin lizzy hate informercial  \n",
       "5  mt syria deir ezzor ali bashar altheeb wa mart...  \n",
       "6  hate life see_not roskilde_not festival_not sa...  "
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# max_len = max([len(tweet) for tweet in t_raw_tweets_bing])\n",
    "# X_test_indices = sentences_to_indices(t_raw_tweets_bing)\n",
    "# Y_test_oh = convert_to_one_hot(actual_labels, C = 3)\n",
    "# print (X_test_indices.shape)\n",
    "# print (Y_test_oh.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anwar/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:15: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7584, 50)\n"
     ]
    }
   ],
   "source": [
    "# t_word_embedding_features = sentence_to_avg(X_test_indices, model.get_weights()[0])\n",
    "# t_word_embedding_features = scipy.sparse.csr_matrix(t_word_embedding_features)\n",
    "# t_word_embedding_features.shape\n",
    "\n",
    "### DELETE THE FOLLOWING LINES IF YOU WANT TO CREATE AN NN MODEL\n",
    "avg = np.zeros((t_length, 50))\n",
    "\n",
    "for i, tweet in enumerate(t_raw_tweets_bing):\n",
    "    # Averaging the word vectors.\n",
    "    for word in tweet:\n",
    "        try:\n",
    "            avg[i, :] += word_to_vec_map[word]\n",
    "        except KeyError:\n",
    "            pass\n",
    "    avg[i, :] /= len(tweet)\n",
    "\n",
    "t_word_embedding_features = avg\n",
    "t_word_embedding_features = np.array(t_word_embedding_features).reshape(t_length, 50)\n",
    "# t_word_embedding_features = t_word_embedding_features/np.linalg.norm(t_word_embedding_features)\n",
    "t_word_embedding_features = scipy.sparse.csr_matrix(t_word_embedding_features)\n",
    "print(t_word_embedding_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.any(t_word_embedding_features[:, 0] == value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(<1x50 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 50 stored elements in Compressed Sparse Row format>, dtype=object)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(t_word_embedding_features[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Créer le features vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7584, 5)\n",
      "(7584, 139521)\n"
     ]
    }
   ],
   "source": [
    "test_features = count_vectorizer.transform(final_tweets_test)\n",
    "test_count_features = svd.transform(test_features)\n",
    "test_count_features = scipy.sparse.csr_matrix(test_count_features)\n",
    "print (test_count_features.shape)\n",
    "\n",
    "\n",
    "test_tfidf_features = tfidf_vectorizer.transform(final_tweets_test)\n",
    "test_tfidf_features = scipy.sparse.csr_matrix(test_tfidf_features)\n",
    "print (test_tfidf_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7584, 14)\n",
      "(7584, 139590)\n"
     ]
    }
   ],
   "source": [
    "t_final_total = scipy.sparse.csr_matrix(np.hstack( (t_all_scores, t_sum_score, t_AfinnReps, t_WordnetReps, \\\n",
    "                                            t_SemEvalReps, t_SenticnetReps, t_unigram140Reps, t_bigram140Reps) ))\n",
    "print (t_final_total.shape)\n",
    "test_features = scipy.sparse.hstack([t_word_embedding_features, test_count_features, test_tfidf_features, t_final_total])\n",
    "print (test_features.shape)\n",
    "\n",
    "del t_raw_tweets_bing\n",
    "del t_word_embedding_features\n",
    "del t_all_scores, t_sum_score, t_BingMpqaScore, t_AfinnScore, t_WordnetScore, t_SemEvalScore, \\\n",
    "    t_SenticnetScore, t_unigram140Score, t_bigram140Score\n",
    "del t_AfinnReps, t_WordnetReps, t_SemEvalReps, t_unigram140Reps, t_bigram140Reps\n",
    "del test_count_features, test_tfidf_features, t_final_total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prédire les étiquettes en utilisant le modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-106-51bd5394a857>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpredicted_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/anwar/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    571\u001b[0m             \u001b[0mClass\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m         \"\"\"\n\u001b[0;32m--> 573\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseSVC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    574\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/anwar/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    308\u001b[0m         \u001b[0my_pred\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m         \"\"\"\n\u001b[0;32m--> 310\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_for_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    311\u001b[0m         \u001b[0mpredict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sparse_predict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sparse\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dense_predict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/anwar/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py\u001b[0m in \u001b[0;36m_validate_for_predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    457\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'support_'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 459\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'csr'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"C\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    460\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sparse\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misspmatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsr_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/anwar/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    378\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m         array = _ensure_sparse_format(array, accept_sparse, dtype, copy,\n\u001b[0;32m--> 380\u001b[0;31m                                       force_all_finite)\n\u001b[0m\u001b[1;32m    381\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m         \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/anwar/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_ensure_sparse_format\u001b[0;34m(spmatrix, accept_sparse, dtype, copy, force_all_finite)\u001b[0m\n\u001b[1;32m    266\u001b[0m                           % spmatrix.format)\n\u001b[1;32m    267\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 268\u001b[0;31m             \u001b[0m_assert_all_finite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspmatrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mspmatrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/anwar/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X)\u001b[0m\n\u001b[1;32m     56\u001b[0m             and not np.isfinite(X).all()):\n\u001b[1;32m     57\u001b[0m         raise ValueError(\"Input contains NaN, infinity\"\n\u001b[0;32m---> 58\u001b[0;31m                          \" or a value too large for %r.\" % X.dtype)\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "predicted_labels = classifier.predict(test_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Évaluer le modèle\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Accuracy: {:0.2f}%'.format(metrics.accuracy_score(actual_labels, predicted_labels) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Rapport de classification\n",
    "print('{}'.format(metrics.classification_report(actual_labels, predicted_labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voir [Confusion Matrix](https://fr.wikipedia.org/wiki/Matrice_de_confusion) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "print('{}\\n'.format(metrics.confusion_matrix(actual_labels, predicted_labels, labels=[1,-1,0])))\n",
    "print(\"\\x1b[31m\\\" macro f1 score \\\"\\x1b[0m\")\n",
    "print('{}\\n'.format(metrics.f1_score(actual_labels, predicted_labels, average='macro')))\n",
    "print(\"\\x1b[31m\\\" micro f1 score \\\"\\x1b[0m\")\n",
    "print('{}\\n'.format(metrics.f1_score(actual_labels, predicted_labels, average='micro')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparaison avec les 5 meilleures équipes du subtask B\n",
    "\n",
    "Nous comparons notre score avec les autres équipes de l'atelier. Les résultats sont tirés du document joint:\n",
    "[Final report SemEval 2014 Subtask 9](http://www.aclweb.org/anthology/S14-2009)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Team|Accuracy (Macro Averaged)| Accuracy (Micro Averaged)|\n",
    "|----|-------------------------|--------------------------|\n",
    "|TeamX|65.63%|69.99%|\n",
    "|coooolll|63.23%|70.51%|\n",
    "|RTRGO|63.08%|70.15%|\n",
    "|NRC-Canada|67.62%|71.37%|\n",
    "|TUGAS|63.89%|68.84%|\n",
    "|**_ME_**|_63.67%_|_67.44%_|\n",
    "| | |***classement : 10 / 50***|\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# <font color='red'> Analyse des StopList du rapport: \n",
    "[On Stopwords, Filtering and Data Sparsity for Sentiment Analysis of Twitter](http://www.lrec-conf.org/proceedings/lrec2014/pdf/292_Paper.pdf)</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Methode|Accuracy (Macro Averaged)| Accuracy (Micro Averaged)|\n",
    "|----|-------------------------|--------------------------|\n",
    "|Baseline (with stopwords)|63.56%|65.75%|\n",
    "|classical|65.14%|68.01%|\n",
    "|TF-High|58.78%|61.23%|\n",
    "|TF1|63.69%|66.20%|\n",
    "|IDF|52.39%|56.77%|\n",
    "|TRBS|55.86%|58.35%|\n",
    "|MI|60.99%|61.90%|\n",
    "\n",
    "**NOTE**:\n",
    "- Je pense que la raison derrière l'incapacité de ces méthodes à améliorer les performances est que la plupart de nos features dépendent des mots (CountVector, tfidfVector).\n",
    "- La méthode 'TF1' n'était pas très bonne car nous utilisons tfidf vectorizer qui dépend principalement des mots peu fréquents que nous avons supprimés."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
